<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>TensorFlow 入门之 MNIST 手写体数字识别问题 - When Art Meets Tech</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="When Art Meets Tech"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="When Art Meets Tech"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="本篇我们将针对实际中的图像识别问题，使用深度学习入门级 MNIST 手写体数字识别数据集，并且结合上一篇博文中给出的更合理的神经网络设计与优化方法，给出一个使用 TF 实现 MNIST 手写体数字识别神经网络模型优化的最佳实践。"><meta property="og:type" content="blog"><meta property="og:title" content="TensorFlow 入门之 MNIST 手写体数字识别问题"><meta property="og:url" content="https://www.orangeshare.cn/2018/04/05/tensorflow-ru-men-zhi-mnist-shou-xie-ti-shu-zi-shi-bie-wen-ti/"><meta property="og:site_name" content="When Art Meets Tech"><meta property="og:description" content="本篇我们将针对实际中的图像识别问题，使用深度学习入门级 MNIST 手写体数字识别数据集，并且结合上一篇博文中给出的更合理的神经网络设计与优化方法，给出一个使用 TF 实现 MNIST 手写体数字识别神经网络模型优化的最佳实践。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://s2.loli.net/2023/05/18/FwYeMytjCgr4IPG.png"><meta property="og:image" content="https://s2.loli.net/2023/05/26/b9fkesTp3aimohL.png"><meta property="og:image" content="https://s2.loli.net/2023/05/30/Uqku7wKWnoASOmc.png"><meta property="og:image" content="https://s2.loli.net/2023/05/30/hWBMrdlwe19E4qy.png"><meta property="og:image" content="https://s2.loli.net/2023/05/30/VjX5LKm7cFqwaBQ.png"><meta property="og:image" content="https://s2.loli.net/2023/05/30/H61iqQxElZLOGe4.png"><meta property="og:image" content="https://s2.loli.net/2023/05/31/MkgaebKwiFlsORJ.png"><meta property="og:image" content="https://s2.loli.net/2023/05/31/GmCeI4qvkBy2a3b.png"><meta property="og:image" content="https://s2.loli.net/2023/05/31/u2Dj8cPCdVGzEMn.png"><meta property="og:image" content="https://s2.loli.net/2023/05/31/VorUCn3NpdkItSX.png"><meta property="article:published_time" content="2018-04-05T03:14:46.000Z"><meta property="article:modified_time" content="2023-06-07T04:15:10.516Z"><meta property="article:author" content="Waldeinsamkeit"><meta property="article:tag" content="TensorFlow"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://s2.loli.net/2023/05/18/FwYeMytjCgr4IPG.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.orangeshare.cn/2018/04/05/tensorflow-ru-men-zhi-mnist-shou-xie-ti-shu-zi-shi-bie-wen-ti/"},"headline":"When Art Meets Tech","image":["https://s2.loli.net/2023/05/18/FwYeMytjCgr4IPG.png","https://s2.loli.net/2023/05/26/b9fkesTp3aimohL.png","https://s2.loli.net/2023/05/30/Uqku7wKWnoASOmc.png","https://s2.loli.net/2023/05/30/hWBMrdlwe19E4qy.png","https://s2.loli.net/2023/05/30/VjX5LKm7cFqwaBQ.png","https://s2.loli.net/2023/05/30/H61iqQxElZLOGe4.png","https://s2.loli.net/2023/05/31/MkgaebKwiFlsORJ.png","https://s2.loli.net/2023/05/31/GmCeI4qvkBy2a3b.png","https://s2.loli.net/2023/05/31/u2Dj8cPCdVGzEMn.png","https://s2.loli.net/2023/05/31/VorUCn3NpdkItSX.png"],"datePublished":"2018-04-05T03:14:46.000Z","dateModified":"2023-06-07T04:15:10.516Z","author":{"@type":"Person","name":"Waldeinsamkeit"},"description":"本篇我们将针对实际中的图像识别问题，使用深度学习入门级 MNIST 手写体数字识别数据集，并且结合上一篇博文中给出的更合理的神经网络设计与优化方法，给出一个使用 TF 实现 MNIST 手写体数字识别神经网络模型优化的最佳实践。"}</script><link rel="canonical" href="https://www.orangeshare.cn/2018/04/05/tensorflow-ru-men-zhi-mnist-shou-xie-ti-shu-zi-shi-bie-wen-ti/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"></head>    <body class="is-3-column">    <nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="When Art Meets Tech" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Hexo Search" href="https://hexo.io/zh-cn/"><i class="fab fa-hotjar"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>TensorFlow 入门之 MNIST 手写体数字识别问题</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2018-04-05</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2023-06-07</time></span><span class="level-item"><a class="link-muted" href="/categories/DeepLearning/">DeepLearning</a></span><span class="level-item">an hour read (About 9084 words)</span></div></div><div class="content"><p>本篇我们将针对实际中的图像识别问题，使用深度学习入门级 MNIST 手写体数字识别数据集，并且结合上一篇博文中给出的更合理的神经网络设计与优化方法，给出一个使用 TF 实现 MNIST 手写体数字识别神经网络模型优化的最佳实践。</p>
<a id="more"></a>

<p>配置过程中参考了网络上很多的相关博文，也遇到过很多坑，为了感谢配置过程中各位大佬的帮助以及方便本人下次配置或者升级，整理以作此文。</p>
<p><font color="green">更多 TensorFlow 框架学习相关内容，请关注博主相关博文系列 ↓↓↓↓↓</font></p>
<p>之一 &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2018/04/01/yi-wen-xiang-jie-quan-ping-tai-tensorflow-shen-du-xue-xi-kuang-jia-zai-xian-da-jian-cpu-gpu-zhi-chi/">一文详解全平台 TensorFlow 深度学习框架在线搭建 (CPU&amp;GPU 支持)</a></p>
<p>之二 &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2018/04/02/tensorflow-gpu-zhi-chi-ubuntu16-04-nvidia-gtx-cuda-cudnn/">TensorFlow GPU 支持: Ubuntu16.04 + Nvidia GTX + CUDA + CUDNN</a></p>
<p>之三 &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2018/04/03/tensorflow-ru-men-zhi-tf-ji-ben-gong-zuo-yuan-li/">TensorFlow 入门之 TF 基本工作原理</a></p>
<p>之四 &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2018/04/04/tensorflow-ru-men-zhi-shen-du-xue-xi-he-shen-ceng-shen-jing-wang-luo/">TensorFlow 入门之深度学习和深层神经网络</a></p>
<p>之五 &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2018/04/05/tensorflow-ru-men-zhi-mnist-shou-xie-ti-shu-zi-shi-bie-wen-ti/">TensorFlow 入门之 MNIST 手写体数字识别问题</a></p>
<p>之六 &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/">TensorFlow 入门之图像识别和卷积神经网络（CNN）</a></p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<hr>
<h2 id="提纲"><a href="#提纲" class="headerlink" title="提纲"></a>提纲</h2><p>本篇，我们总共安排了三个章节来进行学习：</p>
<p><strong>[1] &gt;&gt;&gt;&gt;</strong> 简单介绍 MNIST 手写体数字图像识别数据集，并给出 TensorFlow 程序如何处理 MNIST 数据集中数据的说明；</p>
<p><strong>[2] &gt;&gt;&gt;&gt;</strong> 基于 MNIST 手写体数字图像识别问题，TF 实现上一篇博文中给出的更合理的神经网络设计与优化方法，以从实际问题角度展示不同优化方法带来的模型性能提升；</p>
<p><strong>[3] &gt;&gt;&gt;&gt;</strong> 通过 TF 变量命名空间，来解决 TensorFlow 变量重用的问题，最终给出 MNIST 识别问题的完整样例。</p>
<hr>
<h2 id="初识-MNIST-数据集"><a href="#初识-MNIST-数据集" class="headerlink" title="初识 MNIST 数据集"></a>初识 MNIST 数据集</h2><p>MNIST（Mixed National Institute of Standards and Technology Database）是一个非常有名的手写体数字图像识别数据集（NIST 数据集的一个子集），也是一个入门级的计算机视觉数据集（很多资料会将其作为深度学习入门样例）。就好比编程入门有 <code>Hello World</code>，机器学习入门有 <strong>MNIST</strong>。</p>
<p>MNIST 数据集中包含各种手写的数字图片：</p>
<div align=center><img src="https://s2.loli.net/2023/05/18/FwYeMytjCgr4IPG.png"></div>

<p>MNIST 官方数据集可以分成两部分：</p>
<ul>
<li><code>60000</code> 行的训练数据集（mnist.train）</li>
<li><code>10000</code> 行的测试数据集（mnist.test）</li>
</ul>
<p>其中，每一行 MNIST 数据单元（数据对象）由两部分组成：一张包含手写数字的图片，和手写数字图片所对应的标签。</p>
<hr>
<h3 id="MNIST-数据单元"><a href="#MNIST-数据单元" class="headerlink" title="MNIST 数据单元"></a>MNIST 数据单元</h3><p><font color="yellow">手写数字图像</font> &gt;&gt;&gt;&gt; 每一张图片都代表了一个手写的 <code>0~9</code> 中数字的灰度图（单通道图像），图片大小为 <code>28 px × 28px</code>。</p>
<p>我们可以用一个像素矩阵来表示手写数字 <code>1</code> 的图片：</p>
<div align=center><img src="https://s2.loli.net/2023/05/26/b9fkesTp3aimohL.png"></div>

<p>关于图像的像素矩阵表示方法，可参考文档【 &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2018/02/01/the-pixel-matrix-representation-of-image/">The Pixel Matrix Representation Of Image</a> &lt;&lt;&lt;&lt;】。</p>
<p><font color="yellow">图像标签</font> &gt;&gt;&gt;&gt; 每一个手写体数字图片，都对应 <code>0~9</code> 中的任意一个数字。</p>
<p>虽然 MNIST 数据集中只提供了训练数据（训练集）和测试数据（测试集），但是为了验证模型训练时的效果，使用时一般会从训练数据集中划分出一部分数据作为验证数据（集验证集）。</p>
<hr>
<h3 id="TensorFlow-Support"><a href="#TensorFlow-Support" class="headerlink" title="TensorFlow Support"></a>TensorFlow Support</h3><p>为了在 TensorFlow 中使用方便，TF 对 MNIST 数据集进行了内部封装，提供了一个示例模块来处理 MNIST 数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这个模块会自动下载并转换 MNIST 数据格式，将其从原始的数据包中解析成训练和测试神经网络时可以使用的数据格式。</span></span><br><span class="line">tensorflow.examples.tutorials.mnist</span><br></pre></td></tr></table></figure>

<p>TF MNIST 数据集使用样例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入用于下载和读取 MNIST 数据集的模块:</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定 MNIST 数据集的下载和读取的路径：</span></span><br><span class="line">MNIST_data_Path = <span class="string">&quot;./MNIST_data/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取 MNIST 数据集对象</span></span><br><span class="line">mnist = input_data.read_data_sets(MNIST_data_Path, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print mnist.train dataSet size :</span></span><br><span class="line">print(<span class="string">&quot;Training data size : &quot;</span>, mnist.train.num_examples)</span><br><span class="line"><span class="comment"># print mnist.validation dataSet size :</span></span><br><span class="line">print(<span class="string">&quot;Validating data size : &quot;</span>, mnist.validation.num_examples)</span><br><span class="line"><span class="comment"># print mnist.test dataSet size :</span></span><br><span class="line">print(<span class="string">&quot;Testing data size : &quot;</span>, mnist.test.num_examples)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print mnist.train.images[0] / mnist.train.labels[0] Format</span></span><br><span class="line">print(<span class="string">&quot;Example training data（image）: &quot;</span>, <span class="string">&quot;\n&quot;</span>, mnist.train.images[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">&quot;Example training data lable : &quot;</span>, mnist.train.labels[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<blockquote>
<p>可能由于网络原因导致 MNIST 数据集下载失败，你可以参考 【 &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/DataSet/MNIST-Introduce/">MNIST Introduction</a> &lt;&lt;&lt;&lt;】进行手动下载！！！</p>
</blockquote>
<p>下载成功后，样例程序输出结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Successfully downloaded train-images-idx3-ubyte.gz <span class="number">9912422</span> <span class="built_in">bytes</span>.</span><br><span class="line">Extracting ./MNIST_data/train-images-idx3-ubyte.gz</span><br><span class="line">Successfully downloaded train-labels-idx1-ubyte.gz <span class="number">28881</span> <span class="built_in">bytes</span>.</span><br><span class="line">Extracting ./MNIST_data/train-labels-idx1-ubyte.gz</span><br><span class="line">Successfully downloaded t10k-images-idx3-ubyte.gz <span class="number">1648877</span> <span class="built_in">bytes</span>.</span><br><span class="line">Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz</span><br><span class="line">Successfully downloaded t10k-labels-idx1-ubyte.gz <span class="number">4542</span> <span class="built_in">bytes</span>.</span><br><span class="line">Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz</span><br><span class="line">Training data size :  <span class="number">55000</span></span><br><span class="line">Validating data size :  <span class="number">5000</span></span><br><span class="line">Testing data size :  <span class="number">10000</span></span><br><span class="line">Example training data :  </span><br><span class="line"> [<span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">1.</span>         .................</span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> </span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.34901962</span> <span class="number">0.9843138</span>  <span class="number">0.9450981</span></span><br><span class="line"> <span class="number">0.3372549</span>  <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.01960784</span> <span class="number">0.8078432</span>  <span class="number">0.96470594</span> <span class="number">0.6156863</span>  <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">1.</span>         ......................</span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.01568628</span> <span class="number">0.45882356</span></span><br><span class="line"> <span class="number">0.27058825</span> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span></span><br><span class="line"> <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>         <span class="number">0.</span>        ]</span><br><span class="line">Example training data lable :  [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br></pre></td></tr></table></figure>

<p>👇👇👇 <strong>MNIST 数据集划分</strong> 👇👇👇</p>
<p>可以看出，<code>input_data.read_data_sets</code> 函数生成的数据集对象会自动将 MNIST 数据集划分为 <code>train</code> &amp;&amp; <code>validation</code> &amp;&amp; <code>test</code> 三个数据集。</p>
<p>其中，<code>train</code> 数据集中包含 <code>55000</code> 张训练图片，<code>validation</code> 数据集中包含 <code>5000</code> 张验证图片，它们共同构成了 MNIST 自身提供的训练数据集。<code>test</code> 数据集中包含了 <code>10000</code> 张测试图片，这些图片都来自于 MNIST 提供的测试数据集。</p>
<p>| ================================================== <strong>Split Line</strong> =============================================== |</p>
<p>👇👇👇 <strong>图像输入和标签处理</strong> 👇👇👇</p>
<p>FCNN 神经网络结构的输入是一个特征向量，所以这里需要将一张二维图像的像素矩阵扁平化处理为一维数组，方便 TF 将图片的像素矩阵提供给神经网络的输入层。</p>
<p>故，TF 封装模块处理后的每张手写数字图片都是一个长度为 <code>784</code> 的一维数组，这个数组中的元素对应了图片像素矩阵中的任意像素值（<code>28 * 28 = 784</code>）。为了方便计算，像素矩阵中像素的灰度值被归一化到 <code>[0, 1]</code>，它代表了颜色的深浅。其中 0 表示白色背景（background），1 表示黑色前景（foreground）。</p>
<p>并且，对手写数字图片所对应的标签，进行了 <code>one-hot</code> 编码处理，方便神经网络的分类任务。one-hot 标签数组是一个 <code>10</code> 维（长度为 10）的向量，每一个维度都对应了 <code>0~9</code> 中数字中的一个。形如：<code>[0,1,0,0,0,0,0,0,0,0]</code> &lt;&lt;&lt;&lt; 数字 1。</p>
<p>| ================================================== <strong>Split Line</strong> =============================================== |</p>
<p>👇👇👇 <strong>Mini Batch 支持</strong> 👇👇👇</p>
<p>为了方便使用小批量样本梯度下降（MGD），<code>input_data.read_data_sets</code> 函数生成的数据集对象还提供了 <strong>mnist.train.next_batch</strong> 方法，可以快速从所有的训练数据中读取一小部分数据作为一个训练 batch。</p>
<p>以下代码显示如何使用这个功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取 BATCH SIZE 为 100 大小的训练数据：</span></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;X Shape: &#x27;</span>, xs.shape)  <span class="comment"># X Shape:  (100, 784)</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Y Shape: &#x27;</span>, ys.shape)  <span class="comment"># Y Shape:  (100, 10)</span></span><br></pre></td></tr></table></figure>

<p>关于 MNIST 数据集更加详细的说明以及使用，请参见文档【 &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2018/03/01/mnist-introduction/">MNIST Introduction</a> &lt;&lt;&lt;&lt;】。</p>
<hr>
<h2 id="MNIST-FCNN-模型训练以及不同优化效果对比"><a href="#MNIST-FCNN-模型训练以及不同优化效果对比" class="headerlink" title="MNIST FCNN 模型训练以及不同优化效果对比"></a>MNIST FCNN 模型训练以及不同优化效果对比</h2><p>这一章节，首先基于 MNIST 手写体数字图像识别问题给出一个 TF 实现，这个程序整合了上一篇博文中介绍的所有优化方法。</p>
<p>接着，介绍验证/测试数据集在神经网络训练过程中的作用。通过实验数据来证明，神经网络在验证数据集上的表现可以近似地作为评价不同神经网络模型效果的标准或者作为迭代轮数的依据。</p>
<p>最后，通过模型在测试集上的表现对比上一篇博文中提到的神经网络结构设计和参数优化的不同方法，从实际问题中展示不同优化方法所带来的性能提升。</p>
<hr>
<h3 id="TF-实现-MNIST-图像识别问题"><a href="#TF-实现-MNIST-图像识别问题" class="headerlink" title="TF 实现 MNIST 图像识别问题"></a>TF 实现 MNIST 图像识别问题</h3><p>这个程序整合了上一篇博文中介绍的所有优化方法：</p>
<p>在神经网络结构设计上采用全连接结构，引入隐藏层、激活函数、偏置项；在训练神经网络上，引入设置指数衰减学习率、正则化以及滑动平均模型。</p>
<p>训练好的神经网络模型在 MNIST 测试数据集上可以达到 <code>98.4%</code> 左右的准确率。</p>
<p>完整的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Hello TensorFlow : MNIST 手写体数字图片识别</span></span><br><span class="line"><span class="string">  Neural Network Structure : Full Connection</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"><span class="comment"># import os</span></span><br><span class="line"><span class="comment"># os.environ[&#x27;TF_CPP_MIN_LOG_LEVEL&#x27;] = &#x27;2&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  <span class="comment"># 忽略 TensorFlow 警告信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入用于下载和读取 MNIST 数据集的模块</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###################### Part 1 : Macros Variables Define ######################</span></span><br><span class="line"><span class="comment">## 1. MNIST 数据集相关的常数 ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像像素（28px * 28px 像素矩阵转化），输入层的节点数；</span></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line"><span class="comment"># 输出节点数，等于类别数目。分别对应 `0~9` 10 个数字类别；</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 2. 配置神经网络中的参数 ##</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Layes1: 隐藏层节点数，这里设置只有一层隐藏层（hidden layers）的网络结构</span></span><br><span class="line">LAYER1_NODE = <span class="number">200</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置滑动平均衰减率</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置描述模型复杂度（结构风险）的正则化项在损失函数中的系数(lambda)</span></span><br><span class="line">REGULARIZATION_RATE = <span class="number">0.0001</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置初始学习率</span></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.8</span></span><br><span class="line"><span class="comment"># 设置学习率的衰减率</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.96</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练轮数</span></span><br><span class="line">TRAINING_STEPS = <span class="number">30000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个训练 batch 块中的训练数据个数。</span></span><br><span class="line"><span class="comment"># batch 数值越小，训练过程越接近随机梯度下降；数值越大，训练越接近梯度下降。</span></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###################### Part 2 : Referenced Function Define #####################</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;### [1] get_time_dif：时间函数</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    用于获取程序中某些功能模块所使用的时间（Seconds）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_time_dif</span>(<span class="params">start_time</span>):</span></span><br><span class="line">    end_time = time.time()</span><br><span class="line">    time_diff = end_time - start_time</span><br><span class="line">    <span class="keyword">return</span> timedelta(seconds=time_diff)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; ### [2] inference : 通过给定的 FCNN 输入和所有参数，计算 FCNN FP 的结果 ## </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">	定义了一个带 ReLU（非线性） 激活函数的三层全连接神经网络，实现了多层网络结构以及去线性化。同时，支持：传入</span></span><br><span class="line"><span class="string">用于计算参数滑动平均值的类，方便在测试时使用滑动平均模型。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span>(<span class="params">input_tensor, avg_class, weights1, biases1, weights2, biases2</span>):</span></span><br><span class="line">    <span class="comment"># 当没有提供滑动平均类时，直接使用参数当前的取值</span></span><br><span class="line">    <span class="keyword">if</span> avg_class == <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 计算隐藏层的 FP 结果，激活函数为 ReLU:</span></span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1, weights2) + biases2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; ### [3] train : 模型训练 ## </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    FCNN 模型训练需要的前向传播算法、反向传播算法，以及迭代训练的实现。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">mnist</span>):</span></span><br><span class="line"></span><br><span class="line">    x = tf.placeholder(dtype=tf.float32, shape=(<span class="literal">None</span>, INPUT_NODE), name=<span class="string">&quot;input_x&quot;</span>)</span><br><span class="line">    y_ = tf.placeholder(dtype=tf.float32, shape=(<span class="literal">None</span>, OUTPUT_NODE), name=<span class="string">&quot;input_y&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 隐藏层参数(Second layer)：</span></span><br><span class="line">    weights1 = tf.Variable(tf.random_normal([INPUT_NODE, LAYER1_NODE], stddev=<span class="number">0.1</span>, dtype=tf.float32))</span><br><span class="line">    biases1 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[LAYER1_NODE]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数(Third layer)：</span></span><br><span class="line">    weights2 = tf.Variable(tf.random_normal([LAYER1_NODE, OUTPUT_NODE], stddev=<span class="number">0.1</span>, dtype=tf.float32))</span><br><span class="line">    biases2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[OUTPUT_NODE]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### 1. 前向传播（FP） ####</span></span><br><span class="line">    <span class="comment"># 计算当前参数下 FCNN FP 的结果，模型训练时前向传播计算的 avg_class = None，所以不会使用参数的滑动平均值进行计算</span></span><br><span class="line">    y = inference(x, <span class="literal">None</span>, weights1, biases1, weights2, biases2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义训练时的全局步数，该变量无需计算滑动平均值。所以一般需要指定全局步数变量为不可训练变量(trainable=False)</span></span><br><span class="line">    global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment">#### =============== ####</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### 2. 滑动平均设置 ####</span></span><br><span class="line">    <span class="comment"># avg_class = variable_averages</span></span><br><span class="line">    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)  <span class="comment">#  初始化滑动平均对象</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算所有可训练神经网络参数变量的滑动平均值（即添加/更新参数变量的影子变量）：</span></span><br><span class="line">    variables_averages_op = variable_averages.apply(tf.trainable_variables())</span><br><span class="line">    <span class="comment"># tf.trainable_variables 可返回 GraphKeys.TRAINABLE_VARIABLES 集合中所有元素（即未指定 trainable=False 的变量）</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算使用了滑动平均之后的 FP 结果。滑动平均之后不会改变变量本身取值，会维护一个 shadow_variable 来记录其滑动平均值</span></span><br><span class="line">    movingAvg_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)</span><br><span class="line">    <span class="comment">#### =============== ####</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### 3. Loss Function ####</span></span><br><span class="line">    <span class="comment"># 定义交叉熵损失函数（经验风险）:该函数第二个参数需要提供的是正确标签的数字，tf.argmax(y_, 1)可以获取 y_ 对应的类别标签（原型: tf.argmax(array, axis)）</span></span><br><span class="line">    <span class="comment"># 注意，这里的 logits 参数值只能是不使用滑动平均的变量，不能是影子变量：</span></span><br><span class="line">    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 计算当前 batch 中所有样例的交叉熵平均值</span></span><br><span class="line">    cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义 L2 正则化损失函数（结构风险）：</span></span><br><span class="line">    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line">    <span class="comment"># 计算模型的 L2 正则化损失（一般只计算权重的正则化损失，而不使用偏置项）:</span></span><br><span class="line">    parm_regularization = regularizer(weights1) + regularizer(weights2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算最终 Loss Function：经验风险和结构分析</span></span><br><span class="line">    loss = cross_entropy_mean + parm_regularization</span><br><span class="line">    <span class="comment">#### =============== ####</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">#### 4. 反向传播（BP） ####</span></span><br><span class="line">    <span class="comment"># 设置指数衰减的学习率:随着迭代进行，更新变量的学习率在这个基础上递减</span></span><br><span class="line">    learning_rate = tf.train.exponential_decay(learning_rate = LEARNING_RATE_BASE,  <span class="comment"># 初始学习率</span></span><br><span class="line">                                               global_step = global_step,  <span class="comment"># 训练时的全局步数</span></span><br><span class="line">                                               decay_steps = mnist.train.num_examples / BATCH_SIZE,  <span class="comment"># 遍历一次所有训练数据需要的迭代次数</span></span><br><span class="line">                                               decay_rate = LEARNING_RATE_DECAY  <span class="comment"># 衰减系数</span></span><br><span class="line">                                              )</span><br><span class="line">    <span class="comment"># 优化器:</span></span><br><span class="line">    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绑定参数滑动平均计算和参数梯度更新计算</span></span><br><span class="line">    <span class="comment"># 每迭代一次数据就需要通过 BP 来更新 FCNN 中参数，又要更新每一个参数的滑动平均值（影子变量）。</span></span><br><span class="line">    <span class="comment"># TF 中提供了两种机制：tf.control_dependencie 和 tf.group：</span></span><br><span class="line">    <span class="comment"># # train_op = tf.group(train_step, variables_averages_op) # 等价于 ==</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([train_step, variables_averages_op]):</span><br><span class="line">        train_op = tf.no_op(name=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    <span class="comment">#### =============== ####</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## 5. Model Evaluation ##</span></span><br><span class="line">    <span class="comment"># 检验使用了滑动平均模型的 FCNN 模型的效果（滑动平均或不使用滑动平均）</span></span><br><span class="line">    <span class="comment"># correct_prediction = tf.equal(tf.argmax(movingAvg_y, 1), tf.argmax(y_, 1))</span></span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># tf.cast(x, dtype) 会将 x 数据格式转化成 dtype 数据格式。这里，将 bool 型数值转为 float 后，x 会变为 0/1 序列。</span></span><br><span class="line">    <span class="comment"># 计算模型准确率</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">############################  Begin To Training  ###########################</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义会话配置:</span></span><br><span class="line">    config = tf.ConfigProto(<span class="comment"># device_count=&#123;&quot;CPU&quot;: 4&#125;, # limit to num_cpu_cor CPU usage</span></span><br><span class="line">                            <span class="comment"># inter_op_parallelism_threads = 1,  # config parallelism</span></span><br><span class="line">                            <span class="comment"># intra_op_parallelism_threads = 1,  # config parallelism</span></span><br><span class="line">                            allow_soft_placement=<span class="literal">True</span>,</span><br><span class="line">                            log_device_placement=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.Session(config=config) <span class="keyword">as</span> sess:</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化所有变量</span></span><br><span class="line">        init_op = tf.global_variables_initializer().run()</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">&#x27;Training and evaluating...&#x27;</span>)</span><br><span class="line">        <span class="comment"># 准备验证数据集，一般用于在训练过程中大致判断停止以及模型训练效果:</span></span><br><span class="line">        validation_feed = &#123;x: mnist.validation.images, y_: mnist.validation.labels&#125;</span><br><span class="line">        <span class="comment"># 准备测试数据集，用于评价模型优劣的标准</span></span><br><span class="line">        test_feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 可视化准备</span></span><br><span class="line">        epoch = <span class="number">0</span></span><br><span class="line">        NUM_EPOCH = <span class="built_in">int</span>(TRAINING_STEPS / <span class="number">1000</span>)</span><br><span class="line">        fig_loss = np.zeros([NUM_EPOCH])</span><br><span class="line">        fig_acc_val = np.zeros([NUM_EPOCH])</span><br><span class="line">        fig_acc_test = np.zeros([NUM_EPOCH])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 迭代训练神经网络</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(TRAINING_STEPS):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 产生这一轮使用的一个 batch 的训练数据，并进行训练</span></span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 每 1000 次输出一次在验证数据上的测试结果</span></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># print(&#x27;Epoch:&#x27;, epoch + 1)</span></span><br><span class="line">                </span><br><span class="line">                loss_train, acc_train = sess.run([loss, accuracy], feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line">                loss_val, acc_val = sess.run([loss, accuracy], feed_dict=validation_feed)  <span class="comment"># todo</span></span><br><span class="line">                time_dif = get_time_dif(start_time)</span><br><span class="line">                msg = <span class="string">&#x27;Iter: &#123;0:&gt;6&#125;, Train Loss: &#123;1:&gt;6.2&#125;, Train Acc: &#123;2:&gt;7.2%&#125;,&#x27;</span> \</span><br><span class="line">                        + <span class="string">&#x27; Val Loss: &#123;3:&gt;6.2&#125;, Val Acc: &#123;4:&gt;7.2%&#125;, Time: &#123;5&#125;&#x27;</span></span><br><span class="line">                print(msg.<span class="built_in">format</span>(step, loss_train, acc_train, loss_val, acc_val, time_dif))</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 可视化准备</span></span><br><span class="line">                acc_test = sess.run(accuracy, feed_dict=test_feed)</span><br><span class="line">                fig_loss[epoch] = loss_train</span><br><span class="line">                fig_acc_val[epoch] = acc_val + <span class="number">0.05</span>  <span class="comment"># 为了区分验证准确率和测试准确率差别, 让验证集准确率上移 0.05 单位</span></span><br><span class="line">                fig_acc_test[epoch] = acc_test</span><br><span class="line">                epoch += <span class="number">1</span>  </span><br><span class="line">                </span><br><span class="line">        <span class="comment"># 训练结束后，在测试数据集上计算 FCNN 模型的最终准确率，以评估模型效果</span></span><br><span class="line">        test_acc = sess.run(accuracy, feed_dict=test_feed)</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;After %d training step(s), test accuracy &quot;</span></span><br><span class="line">               <span class="string">&quot;using average Model is %g &quot;</span> % (TRAINING_STEPS, test_acc))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### -------- 可视化 -------- ###</span></span><br><span class="line">        <span class="comment"># 训练过程中损失 (Loss) 曲线:</span></span><br><span class="line">        fig1 = plt.figure()</span><br><span class="line">        plt.plot(np.arange(NUM_EPOCH), fig_loss, label=<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&quot;Iteration&quot;</span>)</span><br><span class="line">        plt.ylabel(<span class="string">&quot;Training Loss&quot;</span>)</span><br><span class="line">        <span class="comment"># plt.show()</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 验证集和测试集上的准确率 (Accuracy) 曲线:</span></span><br><span class="line">        <span class="comment"># fig2 = plt.figure(figsize=(30, 1), dpi=50)  # 设置图片大小和像素(dpi=50)</span></span><br><span class="line">        fig2 = plt.figure()</span><br><span class="line">        plt.plot(np.arange(NUM_EPOCH), fig_acc_val, label=<span class="string">&quot;Validation Accuracy&quot;</span>, color=<span class="string">&quot;blue&quot;</span>)  <span class="comment"># 绘制折线图</span></span><br><span class="line">        plt.plot(np.arange(NUM_EPOCH), fig_acc_test, label=<span class="string">&quot;Test Accuracy&quot;</span>, color=<span class="string">&quot;green&quot;</span>)  <span class="comment"># 绘制折线图</span></span><br><span class="line">        plt.xlabel(<span class="string">&quot;Iteration&quot;</span>)  <span class="comment"># 设置 x 轴标签</span></span><br><span class="line">        plt.ylabel(<span class="string">&quot;Modul Accuracy&quot;</span>)  <span class="comment"># 设置 y 轴标签</span></span><br><span class="line">        y_ticks = np.arange(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0.15</span>)  <span class="comment"># 设置 y 轴刻度</span></span><br><span class="line">        plt.yticks(y_ticks) </span><br><span class="line">        plt.grid(alpha=<span class="number">0.4</span>, linestyle=<span class="string">&#x27;-.&#x27;</span>)  <span class="comment"># 绘制网格 &gt;&gt;&gt;&gt; alpha 设置网格透明度; linestyle 设置网格样式</span></span><br><span class="line">        <span class="comment"># plt.legend(prop=my_font, loc=&#x27;upper left&#x27;)  # 设置图例字体及其位置 (默认右上)</span></span><br><span class="line">        plt.show()</span><br><span class="line">        <span class="comment">### ======================= ###</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">############################   Main Function as the begin of program  ###########################</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">arg=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">###################### Functions for downloading and reading MNIST data. ######################</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; </span></span><br><span class="line"><span class="string">    ## 初始化：下载或读取用于训练、测试以及验证的 MNIST 手写数字图片（28px * 28px）数据集 ##</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        MNIST 数据集分成两部分：60000行的训练数据集（mnist.train）和10000行的测试数据集（mnist.test）。每一个 MNIST 数据单</span></span><br><span class="line"><span class="string">    元（数据对象）有两部分组成：一张包含手写数字的图片和一个对应的标签。比如训练数据集的图片是 mnist.train.images ，训练数据集的标签是 mnist.train.labels。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        mnist.train.images 是一个形状为 [60000, 784] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的像素点。</span></span><br><span class="line"><span class="string">        mnist.train.labels 是一个形状为 [60000, 10] 的张量，第一个维度数字用来索引图片，第二个维度数字用来索引每张图片中的分类标签（one-hot vectors）。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        实际，read_data_sets 类会将数据从原始数据包格式解析成训练和测试神经网络时的数据格式 。read_data_sets 会自动将 MNIST 数据集划分为 train（55000）、test（10000） 以及 validation（5000）三个数据集。</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    print(<span class="string">&quot;Loading training and validation data...&quot;</span>)</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 指定 MNIST 数据集的下载和读取的路径：</span></span><br><span class="line">    MNIST_DATA_PATH = <span class="string">&quot;./MNIST_data/&quot;</span></span><br><span class="line">    mnist = input_data.read_data_sets(MNIST_DATA_PATH, one_hot=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print mnist.train dataSet size :</span></span><br><span class="line">    print(<span class="string">&quot;Training Data Size : &quot;</span>, mnist.train.num_examples)</span><br><span class="line">    <span class="comment"># print mnist.validation dataSet size :</span></span><br><span class="line">    print(<span class="string">&quot;Validation Data Size : &quot;</span>, mnist.validation.num_examples)</span><br><span class="line">    <span class="comment"># print mnist.test dataSet size :</span></span><br><span class="line">    print(<span class="string">&quot;Testing Data Size : &quot;</span>, mnist.test.num_examples)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print mnist.train.images[0] / mnist.train.labels[0] Format</span></span><br><span class="line">    <span class="comment"># print(&quot;Example training data[0] pix-matirx : &quot;, &quot;\n&quot;, mnist.train.images[0])</span></span><br><span class="line">    <span class="comment"># print(&quot;Example training picture shape : &quot; + str(len(mnist.train.images[0])))</span></span><br><span class="line">    <span class="comment"># print(&quot;Example training data lable : &quot;, mnist.train.labels[0])</span></span><br><span class="line">    <span class="comment"># print(&quot;Example training lable shape : &quot; + str(len(mnist.train.labels[0])))</span></span><br><span class="line">    </span><br><span class="line">    time_diff = get_time_dif(start_time)</span><br><span class="line">    print(<span class="string">&quot;Time Usage:&quot;</span>, time_diff)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">###################### Functions for Training FCNN Model By Using MNIST data. ######################</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># begin to train model:</span></span><br><span class="line">    train(mnist)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># TF 提供的一个主程序入口，tf.app.run 会调用上面定义的 main 函数：</span></span><br><span class="line">    <span class="comment"># tf.app.run()</span></span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>样例程序输出信息如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">Loading training and validation data...</span><br><span class="line">Extracting ./MNIST_data/train-images-idx3-ubyte.gz</span><br><span class="line">Extracting ./MNIST_data/train-labels-idx1-ubyte.gz</span><br><span class="line">Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz</span><br><span class="line">Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz</span><br><span class="line">Training Data Size :  55000</span><br><span class="line">Validation Data Size :  5000</span><br><span class="line">Testing Data Size :  10000</span><br><span class="line">Time Usage: 0:00:00.347534</span><br><span class="line">Training and evaluating...</span><br><span class="line">Iter:      0, Train Loss:    3.7, Train Acc:  57.00%, Val Loss:    4.5, Val Acc:  38.60%, Time: 0:00:00.421596</span><br><span class="line">Iter:   1000, Train Loss:  0.094, Train Acc: 100.00%, Val Loss:   0.17, Val Acc:  97.18%, Time: 0:00:01.537846</span><br><span class="line">Iter:   2000, Train Loss:  0.089, Train Acc: 100.00%, Val Loss:   0.15, Val Acc:  97.72%, Time: 0:00:02.682101</span><br><span class="line">Iter:   3000, Train Loss:  0.083, Train Acc: 100.00%, Val Loss:   0.14, Val Acc:  97.96%, Time: 0:00:03.847363</span><br><span class="line">Iter:   4000, Train Loss:  0.077, Train Acc: 100.00%, Val Loss:   0.14, Val Acc:  97.90%, Time: 0:00:05.014624</span><br><span class="line">Iter:   5000, Train Loss:  0.068, Train Acc: 100.00%, Val Loss:   0.13, Val Acc:  98.08%, Time: 0:00:06.187306</span><br><span class="line">Iter:   6000, Train Loss:  0.066, Train Acc: 100.00%, Val Loss:   0.12, Val Acc:  98.24%, Time: 0:00:07.310558</span><br><span class="line">Iter:   7000, Train Loss:  0.065, Train Acc: 100.00%, Val Loss:   0.12, Val Acc:  98.20%, Time: 0:00:08.489824</span><br><span class="line">Iter:   8000, Train Loss:  0.058, Train Acc: 100.00%, Val Loss:   0.11, Val Acc:  98.40%, Time: 0:00:09.660084</span><br><span class="line">Iter:   9000, Train Loss:  0.059, Train Acc: 100.00%, Val Loss:   0.11, Val Acc:  98.42%, Time: 0:00:10.893361</span><br><span class="line">Iter:  10000, Train Loss:  0.056, Train Acc: 100.00%, Val Loss:   0.11, Val Acc:  98.40%, Time: 0:00:12.093628</span><br><span class="line">Iter:  11000, Train Loss:  0.053, Train Acc: 100.00%, Val Loss:   0.11, Val Acc:  98.42%, Time: 0:00:13.272892</span><br><span class="line">Iter:  12000, Train Loss:  0.051, Train Acc: 100.00%, Val Loss:    0.1, Val Acc:  98.38%, Time: 0:00:14.393144</span><br><span class="line">Iter:  13000, Train Loss:  0.051, Train Acc: 100.00%, Val Loss:    0.1, Val Acc:  98.40%, Time: 0:00:15.549404</span><br><span class="line">Iter:  14000, Train Loss:  0.051, Train Acc: 100.00%, Val Loss:    0.1, Val Acc:  98.54%, Time: 0:00:16.730667</span><br><span class="line">Iter:  15000, Train Loss:  0.045, Train Acc: 100.00%, Val Loss:  0.099, Val Acc:  98.48%, Time: 0:00:17.896928</span><br><span class="line">Iter:  16000, Train Loss:  0.047, Train Acc: 100.00%, Val Loss:  0.099, Val Acc:  98.48%, Time: 0:00:19.068191</span><br><span class="line">Iter:  17000, Train Loss:  0.044, Train Acc: 100.00%, Val Loss:  0.096, Val Acc:  98.50%, Time: 0:00:20.166436</span><br><span class="line">Iter:  18000, Train Loss:  0.046, Train Acc: 100.00%, Val Loss:  0.096, Val Acc:  98.60%, Time: 0:00:21.333697</span><br><span class="line">Iter:  19000, Train Loss:  0.044, Train Acc: 100.00%, Val Loss:  0.094, Val Acc:  98.56%, Time: 0:00:22.496959</span><br><span class="line">Iter:  20000, Train Loss:  0.049, Train Acc: 100.00%, Val Loss:  0.094, Val Acc:  98.54%, Time: 0:00:23.666221</span><br><span class="line">Iter:  21000, Train Loss:  0.044, Train Acc: 100.00%, Val Loss:  0.093, Val Acc:  98.52%, Time: 0:00:24.835482</span><br><span class="line">Iter:  22000, Train Loss:  0.048, Train Acc: 100.00%, Val Loss:  0.093, Val Acc:  98.52%, Time: 0:00:26.002744</span><br><span class="line">Iter:  23000, Train Loss:  0.041, Train Acc: 100.00%, Val Loss:  0.093, Val Acc:  98.46%, Time: 0:00:27.125994</span><br><span class="line">Iter:  24000, Train Loss:  0.042, Train Acc: 100.00%, Val Loss:  0.092, Val Acc:  98.52%, Time: 0:00:28.296257</span><br><span class="line">Iter:  25000, Train Loss:  0.047, Train Acc: 100.00%, Val Loss:  0.092, Val Acc:  98.54%, Time: 0:00:29.459517</span><br><span class="line">Iter:  26000, Train Loss:   0.04, Train Acc: 100.00%, Val Loss:  0.091, Val Acc:  98.54%, Time: 0:00:30.612776</span><br><span class="line">Iter:  27000, Train Loss:  0.043, Train Acc: 100.00%, Val Loss:  0.091, Val Acc:  98.58%, Time: 0:00:31.786038</span><br><span class="line">Iter:  28000, Train Loss:  0.041, Train Acc: 100.00%, Val Loss:  0.091, Val Acc:  98.54%, Time: 0:00:32.886282</span><br><span class="line">Iter:  29000, Train Loss:  0.041, Train Acc: 100.00%, Val Loss:   0.09, Val Acc:  98.54%, Time: 0:00:34.055544</span><br><span class="line">After 30000 training step(s), test accuracy using average Model is 0.9828</span><br></pre></td></tr></table></figure>

<p>训练过程中损失 (Loss) 曲线如下：</p>
<div align=center><img src="https://s2.loli.net/2023/05/30/Uqku7wKWnoASOmc.png"></div>

<p>可见，随着训练迭代过程的增加，模型的损失函数逐渐收敛于一个较小值（接近 0）。</p>
<p>训练过程中，验证集和测试集上的准确率 (Accuracy) 变化曲线:</p>
<div align=center><img src="https://s2.loli.net/2023/05/30/hWBMrdlwe19E4qy.png"></div>

<p>可见，对于分布均匀的数据集，训练过程中模型在验证集上表现可以完全表征其在测试集上的效果！！！</p>
<hr>
<h3 id="模型效果评估"><a href="#模型效果评估" class="headerlink" title="模型效果评估"></a>模型效果评估</h3><p>上述 TF 实现 MNIST 图像识别问题样例中，我们设置了一系列的参数：初始学习率、学习率衰减率、隐藏层节点数、迭代轮数等，并且我们知道神经网络模型的最终表达效果是受上述参数影响的。</p>
<p>那么如何设置这些参数的取值？！！大部分情况下，配置神经网络的参数是需要通过不断对比实验来进行调整，以实现最佳的神经网络模型效果（关于更多神经网络调参内容可参考【&gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/">一文详解深度神经网络调参技巧</a> &lt;&lt;&lt;&lt;】）。</p>
<p>那么，如何评估不同参数设置下的模型效果优劣？！！</p>
<hr>
<h4 id="测试集评估"><a href="#测试集评估" class="headerlink" title="测试集评估"></a>测试集评估</h4><p>神经网络模型效果的好坏可以通过 &gt;&gt;&gt;&gt; <font color="red">模型对未知数据（测试集数据）的预测情况来评判的。</font></p>
<p>但我们不能直接通过模型在测试数据上的效果来选择参数（测试数据集变成了训练数据集的一部分参与训练，这时设置测试集是没有意义的；或者测试集较好表现成为模型优化的目标），这可能导致神经网络过度拟合测试数据，从而失去了对未知数据的预判能力。</p>
<p>也就是说，<font color="red">测试数据不应该对我们最终学得（/训练得到）的神经网络模型提供任何支持</font>，这样才能 <font color="green">保证通过测试数据评估出来的模型效果和在真实场景下模型对未知数据的预测效果是最接近的！！！</font> &lt;&lt;&lt;&lt; 测试数据集对应的是部分未知数据，所以需要保证训练过程中测试数据不可见。</p>
<p>| ================================================== <strong>Split Line</strong> =============================================== |</p>
<p>由于测试数据在训练过程的不可见，那么如何评判在 <strong>优化/训练过程中</strong> 神经网络模型不同参数设置下的效果？！！</p>
<p>👇👇👇 <strong>引入验证集（Validation Set）</strong> 👇👇👇</p>
<p>一般会从训练集抽取一部分数据作为验证数据集，通过训练模型在验证数据集上的表现，来（代替测试集）评价优化过程中不同参数选取下模型的好坏。</p>
<p>👇👇👇 <strong>K 折（K Fold Cross Validation）</strong> 👇👇👇</p>
<p>一般是将整个数据集分成 <code>k</code> 个子集，每个子集均做一次测试集，其余的作为训练集进行训练。交叉验证需要重复 <code>k</code> 次，每次选择一个子集作为测试集，并将 k 次的平均交叉验证识别正确率作为结果。</p>
<p>由于神经网络本身训练时间就比较长，采用交叉验证会花费大量的时间（适合小数据集）。所以在海量数据下，一般会更多的采用验证数据集的形式来评测模型的效果。</p>
<hr>
<h4 id="验证集近似评估"><a href="#验证集近似评估" class="headerlink" title="验证集近似评估"></a>验证集近似评估</h4><p>为了说明 &gt;&gt;&gt;&gt; 验证数据可以代替测试集（默认数据分布均匀），近似作为模型效果的评价标准，我们将对比不同迭代轮数情况下，模型在验证数据和测试数据上的准确率：</p>
<div align=center><img src="https://s2.loli.net/2023/05/30/VjX5LKm7cFqwaBQ.png"></div>

<p>上图给出了每 <code>1000</code> 轮 FCNN 模型在验证、测试数据集上的准确率变换曲线。可以看出，虽然两条曲线不完全重合，但这两条曲线的趋势基本一致，而且他们的相关系数（<code>correlation coefficient</code>）大于 <code>0.9999</code>。</p>
<p>实验说明 &gt;&gt;&gt;&gt; <font color="red">模型在验证数据集上的表现，完全可以近似作为评价不同神经网络模型的标准，或者作为训练迭代轮数的依据！！！</font></p>
<p>| ================================================== <strong>Split Line</strong> =============================================== |</p>
<p>👇👇👇 <strong>需要注意的是</strong> 👇👇👇</p>
<p>上面我们所说的测试数据集训练过程不可见，并不是严格不可见。你会看到很多资料不会划分验证数据集，直接使用测试数据集作为模型训练过程中是否近优的评价标准。</p>
<p>个人理解 &gt;&gt;&gt;&gt; 理想情况下，我们希望选取的未划分的样本数据集样本对于问题的数据分布是均匀的。但如果验证数据分布不能很好地代表测试数据分布，那么模型在这两个数据集上的表现就可能不一样。</p>
<p>一般来说，选取的验证数据分布越接近测试数据分布，模型在验证数据上的表现就越可以体现神经网络模型在测试数据（未知数据）下的效果！！！</p>
<hr>
<h3 id="不同优化方法模型效果对比"><a href="#不同优化方法模型效果对比" class="headerlink" title="不同优化方法模型效果对比"></a>不同优化方法模型效果对比</h3><p>了解了神经网络模型效果评估标准后，这一小节将通过模型在 MNIST 测试集上的预测准确率表现对比上一篇博文中提到的神经网络结构设计和参数优化的不同方法，从实际问题中展示不同优化方法所带来的性能提升。</p>
<p>下图，给出了在相同神经网络参数下，设置不同优化方法的对比实验，经过 <code>30000</code> 轮训练迭代后，得到的各个模型最终的正确率（<code>10</code> 次运行结果的平均值）：</p>
<div align=center><img src="https://s2.loli.net/2023/05/30/H61iqQxElZLOGe4.png"></div>

<p>👇👇👇 <strong>神经网路模型结构影响</strong> 👇👇👇</p>
<p>可以看出 &gt;&gt;&gt;&gt; 调整神经网络模型的结构（不使用隐藏层或没有激活函数）对最终的准确率有非常大的影响。这说明神经网络的结构设计对最终模型的效果有本质的影响。</p>
<p>后面会介绍一种更加特殊的神经网络结构 <code>CNN</code>（卷积神经网络），它可以更加有效的处理图像信息。通过 <code>CNN</code> 可以进一步将 MNIST 识别模型准确率提高到 <code>99.5%</code>。</p>
<p>👇👇👇 <strong>滑动平均模型/指数衰减学习率/正则化影响</strong> 👇👇👇</p>
<p>从上图数字中发现 &gt;&gt;&gt;&gt; 使用滑动平均模型、指数衰减学习率和使用正则化带来的正确率提升并不是特别明显。其中，使用了所有优化方法的模型、和不使用滑动平均模型，以及不使用指数衰减学习率的模型正确率都可以达到约 <code>98.4%</code> 。</p>
<p>那么是不是意味着这些优化方法对模型准确率提升不大？！！</p>
<p>答案肯定是否定的！！！这里准确率提升不是特别明显，是由于 MNIST 数据集简单，模型收敛速度很快（梯度较小），而滑动平均模型以及指数衰减学习率在一定程度上都是限制神经网络中参数的更新速度，所以这两种优化对最终模型影响不大。</p>
<p>从上面模型在验证数据和测试数据上的准确率曲线可以看出，在迭代 <code>4000</code> 次以后就已经接近最终的准确率了（收敛太快）。</p>
<p>| ================================================== <strong>Split Line</strong> =============================================== |</p>
<p>下面我们将进一步分析滑动平均模型、指数衰减学习率和使用正则化对训练模型的影响：</p>
<p><strong>[1] &gt;&gt;&gt;&gt; 滑动平均模型和指数衰减学习率</strong></p>
<p>先给出不同迭代轮数时，使用了所有优化方法的模型的正确率与平均绝对梯度的变化趋势图：</p>
<div align=center><img src="https://s2.loli.net/2023/05/31/MkgaebKwiFlsORJ.png"></div>

<p>可以看出，前 <code>4000</code> 轮迭代对模型的改变是最大的。在 4000 轮迭代之后，由于梯度比较小，所以参数的改变也就比较缓慢了。于是滑动平均模型或者指数衰减学习率的作用也就没那么显著了。</p>
<p>不同迭代轮数时，模型正确率和衰减之后学习率的变化趋势图如下：</p>
<div align=center><img src="https://s2.loli.net/2023/05/31/GmCeI4qvkBy2a3b.png"></div>

<p>可以看出，学习率曲线呈现阶梯状衰减方式。在前 <code>4000</code> 轮时，衰减之后的学习率和初始学习率的差距并不大。那么，能否说明这些限制网络参数的更新速度的优化方法作用不大？！！</p>
<p>实际上，当问题更加复杂时，模型迭代不会这么快收敛，这时滑动平均模型和指数衰减学习率会发挥更大的作用！！！例如 Cifar-10 图像分类数据集上，使用滑动平均模型可以将模型识别的错误率降低 <code>11%</code>，使用指数衰减学习率可以将识别错误率降低 <code>7%</code>。</p>
<p>| ================================================== <strong>Split Line</strong> =============================================== |</p>
<p><strong>[2] &gt;&gt;&gt;&gt; 正则化</strong></p>
<p>相较于滑动平均模型和指数衰减学习率，使用加入了正则化损失函数（结构风险）给模型效果带来的提升要相对显著。</p>
<p>下图对比了两个使用了不同损失函数的神经网络模型，其中一个模型只最小化交叉熵损失，另一个模型优化的是交叉熵损失和 L2 正则化损失的和：</p>
<div align=center><img src="https://s2.loli.net/2023/05/31/u2Dj8cPCdVGzEMn.png"></div>

<p>显而易见，只优化交叉熵的模型在训练数据上的交叉熵损失（灰色虚线）要比优化总损失的模型更小（黑色虚线）。然而在测试数据上，优化总损失的模型（黑色实线）却要好于只优化交叉熵的模型（灰色实线）。这就是前面我们提到过的过拟合问题。</p>
<p>这是由于只优化交叉熵的模型可能更好的拟合训练数据（损失更小），却不能很好的挖掘数据中的潜在规律来判断未知的测试数据，故在测试数据上的准确率低。</p>
<p>我们再给出不同模型的损失函数变化规律：</p>
<div align=center><img src="https://s2.loli.net/2023/05/31/VorUCn3NpdkItSX.png"></div>

<p>从左侧只优化交叉熵模型子图可以看出，随着迭代的进行，正则化损失是在不断增大的。</p>
<p>而由于 MNIST 问题相对比较简单，迭代后期的梯度很小，所以正则化损失的增长也很慢。对于更加复杂的问题，迭代后期的梯度更大，就会发现总损失（交叉熵损失加上正则化损失）会呈现一个 <code>U</code> 型！！！</p>
<p>| ================================================== <strong>Split Line</strong> =============================================== |</p>
<p><font color="red">↓↓↓↓↓↓ 结论 ↓↓↓↓↓↓</font></p>
<p>可以发现，上述的这些优化方法确实可以解决前面博文中提到的神经网络优化过程中的问题。当需要解决的问题和使用到的神经网络模型更加复杂时，上面的这些优化方法将更有可能对训练效果产生更大的影响。</p>
<hr>
<h2 id="TF-变量命名空间管理"><a href="#TF-变量命名空间管理" class="headerlink" title="TF 变量命名空间管理"></a>TF 变量命名空间管理</h2><p>由于编程习惯，我们通常喜欢采用模块化编程风格，提高代码可读性。</p>
<p>上面给出的 TF 实现 MNIST 手写体数字图像识别示例中，将计算神经网络前向传播结果的过程抽象成了一个函数（模块）:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inference</span>(<span class="params">input_tensor, avg_class, weights1, biases1, weights2, biases2</span>)</span></span><br></pre></td></tr></table></figure>

<p>这样带来的一个好处：在训练、测试以及预测的过程中可以统一调用同一个函数来得到模型的前向传播结果。</p>
<hr>
<h3 id="变量的重用以及命名空间"><a href="#变量的重用以及命名空间" class="headerlink" title="变量的重用以及命名空间"></a>变量的重用以及命名空间</h3><p>从上述 FP 过程定义中可以看到，这个函数的形参中需要包括神经网络中的所有参数。</p>
<p>你想过没有：当神经网络的结构更加复杂，引入的参数更多时，就需要一个更好的方式来传递和管理神经网络中的参数了 &gt;&gt;&gt;&gt; <strong>TensorFlow 支持通过变量名称来创建或者获取一个变量的机制。</strong></p>
<p>通过这种机制，我们可以在不同函数中可以直接通过变量的名称来使用变量（变量重用），而不要将变量通过参数的形式到处传递。</p>
<p>TF 中通过 <code>tf.get_variable</code> 函数和 <code>tf.variable_scope</code> 函数来分别实现 TF 变量的重用和命名空间管理。</p>
<hr>
<h4 id="TF-变量重用"><a href="#TF-变量重用" class="headerlink" title="TF 变量重用"></a>TF 变量重用</h4><p>除了之前提到过的 <code>tf.Variable()</code> 变量创建函数，TensorFlow 中还支持通过 <code>tf.get_variable</code> 函数来获取已存在变量（不存在则创建）。</p>
<p><strong>[1] &gt;&gt;&gt;&gt; tf.Variable</strong></p>
<p>重新认识一下 tf.Variable() 函数：</p>
<p>tf.Variable 函数用于生成一个初始值为 <code>initial-value</code> 的变量（必须指定初始化值）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(</span><br><span class="line">    initial_value=<span class="literal">None</span>, </span><br><span class="line">    trainable=<span class="literal">True</span>, </span><br><span class="line">    collections=<span class="literal">None</span>, </span><br><span class="line">    validate_shape=<span class="literal">True</span>, </span><br><span class="line">    caching_device=<span class="literal">None</span>, </span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    variable_def=<span class="literal">None</span>, </span><br><span class="line">    dtype=<span class="literal">None</span>, </span><br><span class="line">    expected_shape=<span class="literal">None</span>, </span><br><span class="line">    import_scope=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>经常使用的是：initial_value（初始化）、name（命名）、dtype（变量类型）、trainable（是否可训练/优化）。</p>
<p><font color="red">↓↓↓↓↓↓ 参数解释 ↓↓↓↓↓↓</font></p>
<ul>
<li><font color="yellow">initial_value</font>：Variable 初始值，为 Tensor 或可转换为 Tensor 的 Python 对象。如果 <code>validate_shape = True（默认）</code>，则初始值必须具有指定的形状；</li>
<li><font color="yellow">validate_shape</font>：默认为 True，initial_value 必须具有指定的形状。如果为 False，则允许使用未知形状的值初始化变量；</li>
<li><font color="yellow">name</font>：变量的可选名称，默认为 “Variable” 并自动获取；</li>
<li><font color="yellow">dtype</font>：如果设置，则 initial_value 将转换为给定类型；如果为 None，则保留数据类型；</li>
<li><font color="yellow">trainable</font>：用于标识模型训练时是否更新当前参数；如果为 True，当前变量会被添加到图形集合 GraphKeys.TRAINABLE_VARIABLES中；</li>
<li><font color="yellow">collections</font>：</li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>TensorFlow 入门之 MNIST 手写体数字识别问题</p><p><a href="https://www.orangeshare.cn/2018/04/05/tensorflow-ru-men-zhi-mnist-shou-xie-ti-shu-zi-shi-bie-wen-ti/">https://www.orangeshare.cn/2018/04/05/tensorflow-ru-men-zhi-mnist-shou-xie-ti-shu-zi-shi-bie-wen-ti/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Waldeinsamkeit</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2018-04-05</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-06-07</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/TensorFlow/">TensorFlow</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/01/01/wang-zhan-ji-chu-zhi-url-jie-gou-jie-xi/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">网站基础之 URL 结构解析</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2018/04/04/tensorflow-ru-men-zhi-shen-du-xue-xi-he-shen-ceng-shen-jing-wang-luo/"><span class="level-item">TensorFlow 入门之深度学习和深层神经网络</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen  order-3 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Waldeinsamkeit"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Waldeinsamkeit</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">100</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">13</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">43</p></a></div></div></nav></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#提纲"><span class="level-left"><span class="level-item">1</span><span class="level-item">提纲</span></span></a></li><li><a class="level is-mobile" href="#初识-MNIST-数据集"><span class="level-left"><span class="level-item">2</span><span class="level-item">初识 MNIST 数据集</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#MNIST-数据单元"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">MNIST 数据单元</span></span></a></li><li><a class="level is-mobile" href="#TensorFlow-Support"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">TensorFlow Support</span></span></a></li></ul></li><li><a class="level is-mobile" href="#MNIST-FCNN-模型训练以及不同优化效果对比"><span class="level-left"><span class="level-item">3</span><span class="level-item">MNIST FCNN 模型训练以及不同优化效果对比</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#TF-实现-MNIST-图像识别问题"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">TF 实现 MNIST 图像识别问题</span></span></a></li><li><a class="level is-mobile" href="#模型效果评估"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">模型效果评估</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#测试集评估"><span class="level-left"><span class="level-item">3.2.1</span><span class="level-item">测试集评估</span></span></a></li><li><a class="level is-mobile" href="#验证集近似评估"><span class="level-left"><span class="level-item">3.2.2</span><span class="level-item">验证集近似评估</span></span></a></li></ul></li><li><a class="level is-mobile" href="#不同优化方法模型效果对比"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">不同优化方法模型效果对比</span></span></a></li></ul></li><li><a class="level is-mobile" href="#TF-变量命名空间管理"><span class="level-left"><span class="level-item">4</span><span class="level-item">TF 变量命名空间管理</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#变量的重用以及命名空间"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">变量的重用以及命名空间</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#TF-变量重用"><span class="level-left"><span class="level-item">4.1.1</span><span class="level-item">TF 变量重用</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="When Art Meets Tech" height="28"></a><p class="is-size-7"><span>&copy; 2023 Waldeinsamkeit</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>