<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Web Crawler 教程之网络爬虫通用流程解读 - When Art Meets Tech</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="When Art Meets Tech"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="When Art Meets Tech"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="全流程解读网络爬虫通用编写流程，帮助你完成网络爬虫的快速入门指导！！！"><meta property="og:type" content="blog"><meta property="og:title" content="Web Crawler 教程之网络爬虫通用流程解读"><meta property="og:url" content="https://www.orangeshare.cn/2020/01/21/web-crawler-jiao-cheng-zhi-wang-luo-pa-chong-tong-yong-liu-cheng-jie-du/"><meta property="og:site_name" content="When Art Meets Tech"><meta property="og:description" content="全流程解读网络爬虫通用编写流程，帮助你完成网络爬虫的快速入门指导！！！"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://s2.loli.net/2022/12/31/5Cv3r9IoJwZilF2.png"><meta property="og:image" content="https://s2.loli.net/2022/12/31/34wsG6zg2SJfViY.png"><meta property="og:image" content="https://s2.loli.net/2022/12/31/Mzvnyak6S7Bd1Eg.png"><meta property="og:image" content="https://s2.loli.net/2022/12/31/4enHmYRyrs8DpM1.png"><meta property="og:image" content="https://s2.loli.net/2022/12/31/WHNeTp64cmIUv1E.png"><meta property="og:image" content="https://s2.loli.net/2022/12/31/moBkhNTPJ4wOfvi.png"><meta property="og:image" content="https://s2.loli.net/2022/12/31/97YUVqSmG4sRaQt.png"><meta property="og:image" content="https://s2.loli.net/2022/12/31/97YUVqSmG4sRaQt.png"><meta property="og:image" content="https://s2.loli.net/2022/12/31/wcJx5Aus8bk3QHr.png"><meta property="article:published_time" content="2020-01-21T04:32:14.000Z"><meta property="article:modified_time" content="2023-01-12T08:28:07.939Z"><meta property="article:author" content="Waldeinsamkeit"><meta property="article:tag" content="Spider"><meta property="article:tag" content="Web Crawler"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://s2.loli.net/2022/12/31/5Cv3r9IoJwZilF2.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.orangeshare.cn/2020/01/21/web-crawler-jiao-cheng-zhi-wang-luo-pa-chong-tong-yong-liu-cheng-jie-du/"},"headline":"When Art Meets Tech","image":["https://s2.loli.net/2022/12/31/5Cv3r9IoJwZilF2.png","https://s2.loli.net/2022/12/31/34wsG6zg2SJfViY.png","https://s2.loli.net/2022/12/31/Mzvnyak6S7Bd1Eg.png","https://s2.loli.net/2022/12/31/4enHmYRyrs8DpM1.png","https://s2.loli.net/2022/12/31/WHNeTp64cmIUv1E.png","https://s2.loli.net/2022/12/31/moBkhNTPJ4wOfvi.png","https://s2.loli.net/2022/12/31/97YUVqSmG4sRaQt.png","https://s2.loli.net/2022/12/31/97YUVqSmG4sRaQt.png","https://s2.loli.net/2022/12/31/wcJx5Aus8bk3QHr.png"],"datePublished":"2020-01-21T04:32:14.000Z","dateModified":"2023-01-12T08:28:07.939Z","author":{"@type":"Person","name":"Waldeinsamkeit"},"description":"全流程解读网络爬虫通用编写流程，帮助你完成网络爬虫的快速入门指导！！！"}</script><link rel="canonical" href="https://www.orangeshare.cn/2020/01/21/web-crawler-jiao-cheng-zhi-wang-luo-pa-chong-tong-yong-liu-cheng-jie-du/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Oxanium:wght@300;400;600&amp;family=Roboto+Mono"><link rel="stylesheet" href="/css/cyberpunk.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"></head>    <body class="is-3-column">    <nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="When Art Meets Tech" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Hexo Search" href="https://hexo.io/zh-cn/"><i class="fab fa-hotjar"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>Web Crawler 教程之网络爬虫通用流程解读</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="${date_xml(page.date)}" title="${date_xml(page.date)}">2020-01-21</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="${date_xml(page.updated)}" title="${date_xml(page.updated)}">2023-01-12</time></span><span class="level-item"><a class="link-muted" href="/categories/Spider/">Spider</a></span><span class="level-item">2 hours read (About 19321 words)</span></div></div><div class="content"><p>全流程解读网络爬虫通用编写流程，帮助你完成网络爬虫的快速入门指导！！！</p>
<a id="more"></a>

<p>编写爬虫脚本的通用流程如下：</p>
<ol>
<li>通过 Python 网络模块（urllib/requests）发送 URL 请求以获取网页的 HTML 对象信息；</li>
<li>通过浏览器并借助网页元素审查工具分析网页结构以及元素节点信息；</li>
<li>根据网页结构以及元素节点信息，借助 HTML 对象解析工具（Regular Expression/Xpath/Beautiful Soup 4）以解析页面提取有效数据；</li>
<li>将提取到的有效数据持久化到本地磁盘（文件）或数据库。</li>
</ol>
<hr>
<p>下面将依照上述网络爬虫通用流程，逐布完善我们的网络爬虫脚本，最终已给出一个爬虫全流程示例。</p>
<h2 id="请求获取网页-HTML-信息"><a href="#请求获取网页-HTML-信息" class="headerlink" title="请求获取网页 HTML 信息"></a>请求获取网页 HTML 信息</h2><p>首先来看，<font color="red">如何使用 Python 网络模块（urllib/requests）发送 URL 请求以获取网页的 HTML 信息？！！</font></p>
<p>以 Python 内置的 urllib 网络库为例 &gt;&gt;&gt;</p>
<h3 id="Use-Built-in-UrlLib-Lib"><a href="#Use-Built-in-UrlLib-Lib" class="headerlink" title="Use Built-in UrlLib Lib"></a>Use Built-in UrlLib Lib</h3><p><code>urllib</code> 库属于 Python 的标准库内置模块，故安装 Python 后即可使用，无须单独安装。</p>
<p>Python 3 中，统一为 <code>urllib</code> 库，已经不存在 <code>urllib2</code> 库了。较低的 Python 2 中支持 <code>urllib &amp;&amp; urlib2</code> 两个库来实现网络请求的发送。</p>
<p><code>urllib</code> 库中包括了四个关键模块：</p>
<ul>
<li><code>urllib.request</code> 模块：用来发送 Request 以及获取 Request 的响应结果（Response）；</li>
<li><code>urllib.error</code> 模块：定义了 urllib.request 模块可能产生的异常，你可以通过异常处理机制进行捕获处理；</li>
<li><code>urllib.parse</code> 模块：用来解析和处理 URL，例如 URL 编码以及解码；</li>
<li><code>urllib.robotparser</code> 模块：用来解析页面的 robots.txt 协议文件。</li>
</ul>
<hr>
<h4 id="模拟发送请求"><a href="#模拟发送请求" class="headerlink" title="模拟发送请求"></a>模拟发送请求</h4><p>urllib.request 模块提供了基本的 HTTP Request 的构造方法，并且可以模拟发起一个浏览器的请求（Request）过程。</p>
<p>先来感受一下它的强大之处，以向百度（<a target="_blank" rel="noopener" href="http://www.baidu.com/">http://www.baidu.com/</a>）发起请求为例，获取百度首页的 HTML 信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 urllib 网络库的 request 请求模块:</span></span><br><span class="line"><span class="comment"># from urllib import request</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向 URL（http://www.baidu.com/）发送请求 &lt;&lt;&lt; URL 中必须带有 `HTTP/HTTPS` 传输协议:</span></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&quot;http://www.baidu.com/&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印返回类型:</span></span><br><span class="line">print(<span class="built_in">type</span>(response))</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &#x27;http.client.HTTPResponse&#x27;&gt;</span><br></pre></td></tr></table></figure>

<p>可见，通过 urllib.request 模块的 <code>urlopen(url)</code> 方法向 URL 发送请求后，会返回一个百度首页的响应对象（<code>HTTPResponse</code>）。</p>
<p>已经获取到了网站的响应消息，如何通过响应消息对象获取我们需要的信息：</p>
<p><strong>响应对象（HTTPResponse）重要属性与方法支持&gt;&gt;&gt;&gt;</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主要方法：</span></span><br><span class="line">response.getcode() <span class="comment"># 获取请求的 HTTP 响应码</span></span><br><span class="line">response.read() <span class="comment"># 获取网页的 HTML 内容；</span></span><br><span class="line">response.getheaders() <span class="comment"># 获取响应的所有头信息；</span></span><br><span class="line">response.getheader(name) <span class="comment"># 根据响应头信息中的头部字段名（关键字），获取相应字段名所对应的值；</span></span><br><span class="line">response.geturl() <span class="comment"># 获取响应对象的 URL 地址；</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主要属性：</span></span><br><span class="line">response.status <span class="comment"># 获取响应的状态码</span></span><br><span class="line">response.msg</span><br><span class="line">response.version</span><br><span class="line">response.closed</span><br></pre></td></tr></table></figure>

<p>示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response = urllib.request.urlopen(<span class="string">&quot;https://www.baidu.com&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(response.geturl())</span><br><span class="line">http://www.baidu.com/</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(response.getcode())</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(response.status)</span><br><span class="line"><span class="number">200</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(response.getheaders())</span><br><span class="line">[(<span class="string">&#x27;Server&#x27;</span>, <span class="string">&#x27;BWS/1.1&#x27;</span>), (<span class="string">&#x27;Date&#x27;</span>, <span class="string">&#x27;Fri, 30 Dec 2022 09:07:13 GMT&#x27;</span>), (<span class="string">&#x27;Content-Type&#x27;</span>, <span class="string">&#x27;text/html; charset=utf-8&#x27;</span>), (<span class="string">&#x27;Transfer-Encoding&#x27;</span>, <span class="string">&#x27;chunked&#x27;</span>), (<span class="string">&#x27;Connection&#x27;</span>, <span class="string">&#x27;close&#x27;</span>), (<span class="string">&#x27;Bdpagetype&#x27;</span>, <span class="string">&#x27;1&#x27;</span>), (<span class="string">&#x27;Bdqid&#x27;</span>, <span class="string">&#x27;0xf71f01670008eef5&#x27;</span>), (<span class="string">&#x27;P3p&#x27;</span>, <span class="string">&#x27;CP=&quot; OTI DSP COR IVA OUR IND COM &quot;&#x27;</span>), (<span class="string">&#x27;P3p&#x27;</span>, <span class="string">&#x27;CP=&quot; OTI DSP COR IVA OUR IND COM &quot;&#x27;</span>), (<span class="string">&#x27;Set-Cookie&#x27;</span>, <span class="string">&#x27;BAIDUID=A401074D3A894C1EF658A7E9ED45D774:FG=1; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com&#x27;</span>), (<span class="string">&#x27;Set-Cookie&#x27;</span>, <span class="string">&#x27;BIDUPSID=A401074D3A894C1EF658A7E9ED45D774; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com&#x27;</span>), (<span class="string">&#x27;Set-Cookie&#x27;</span>, <span class="string">&#x27;PSTM=1672391233; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com&#x27;</span>), (<span class="string">&#x27;Set-Cookie&#x27;</span>, <span class="string">&#x27;BAIDUID=A401074D3A894C1EB80F6C57BCB052E4:FG=1; max-age=31536000; expires=Sat, 30-Dec-23 09:07:13 GMT; domain=.baidu.com; path=/; version=1; comment=bd&#x27;</span>), (<span class="string">&#x27;Set-Cookie&#x27;</span>, <span class="string">&#x27;BDSVRTM=0; path=/&#x27;</span>), (<span class="string">&#x27;Set-Cookie&#x27;</span>, <span class="string">&#x27;BD_HOME=1; path=/&#x27;</span>), (<span class="string">&#x27;Set-Cookie&#x27;</span>, <span class="string">&#x27;H_PS_PSSID=36558_37647_37906_36920_37990_37926_37901_26350_37957_37881; path=/; domain=.baidu.com&#x27;</span>), (<span class="string">&#x27;Traceid&#x27;</span>, <span class="string">&#x27;1672391233035428020217806952993540075253&#x27;</span>), (<span class="string">&#x27;Vary&#x27;</span>, <span class="string">&#x27;Accept-Encoding&#x27;</span>), (<span class="string">&#x27;X-Frame-Options&#x27;</span>, <span class="string">&#x27;sameorigin&#x27;</span>), (<span class="string">&#x27;X-Ua-Compatible&#x27;</span>, <span class="string">&#x27;IE=Edge,chrome=1&#x27;</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(response.getheader(<span class="string">&quot;Server&quot;</span>))</span><br><span class="line">BWS/<span class="number">1.1</span></span><br></pre></td></tr></table></figure>

<p>上面，通过调用响应对象（HTTPResponse）的属性与方法，分别输出了：响应对象的 URL 地址、响应状态码、响应头信息，以及通过传递一个头部字段名称获取了 Server 的类型。</p>
<hr>
<h4 id="获取-HTML-信息"><a href="#获取-HTML-信息" class="headerlink" title="获取 HTML 信息"></a>获取 HTML 信息</h4><p>我们提到，通过响应对象（HTTPResponse）的 <code>read()</code> 方法可以获取到百度首页的 HTML 内容，即抓取到了网页的源代码，尝试一下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response = urllib.request.urlopen(<span class="string">&quot;https://www.baidu.com&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>html = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(html)</span><br><span class="line"><span class="comment"># 内容过长，这里只截取了一部分：</span></span><br><span class="line">&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt; &lt;html&gt;&lt;head&gt;&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot;&gt;&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot;&gt;&lt;meta content=&quot;always&quot; name=&quot;referrer&quot;&gt;&lt;meta name=&quot;theme-color&quot; content=&quot;#2932e1&quot;&gt;&lt;meta name=&quot;description&quot; content=&quot;全球最大的中文搜索引擎、致力于让网民更便捷地获取信息，找到...&quot;&gt;...&lt;/html&gt;</span><br></pre></td></tr></table></figure>

<p>可以看到，这里我们成功抓取到了百度首页的 HTML 源代码。那得到源代码之后呢？？？ &gt;&gt;&gt; 我们想要的链接、图片地址、文本信息不就都可以提取出来了吗？！！</p>
<p><strong>字节串解码 &gt;&gt;&gt;&gt;</strong></p>
<p>需要注意的是，响应对象（HTTPResponse）的 <code>read()</code> 方法提取 HTML 信息，返回的数据是字节串类型（bytes）的。</p>
<p>为了将其转化为易于处理的字符串类型，故需要 <font color="green">对字节串进行解码操作</font>，这就涉及到 Python 中常用的编码、解码操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按照特定字符集（&#x27;utf-8&#x27;），将字符串编码为采用特定字符编码的字节串：</span></span><br><span class="line">string.encode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照特定字符集（&#x27;utf-8&#x27;），将采用特定字符编码的字节串解码为字符串：</span></span><br><span class="line"><span class="built_in">bytes</span>.decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要注意的是，编码 &amp;&amp; 解码时使用的字符集必须一致，否则会出现乱码的情况!!!</span></span><br></pre></td></tr></table></figure>

<p>由于 <code>read()</code> 方法返回的是采用 UTF-8 字符编码之后的字节串（bytes），故解码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>自此，我们已经基本完成了本节的目标：<font color="red">使用 Python 网络模块（urllib/requests）发送 URL 请求以获取网页的 HTML 信息。</font></p>
<hr>
<h4 id="URL-的编码和解码"><a href="#URL-的编码和解码" class="headerlink" title="URL 的编码和解码"></a>URL 的编码和解码</h4><p>我们知道，WEB 浏览器会通过 URL 发送一个请求，实现从相应的 Web 服务器请求特定的资源。</p>
<p>如果 URL 路径或者查询参数中，带有中文或者特殊字符的时候，浏览器在发送请求前会对 URL 进行 URL 编码，这是 <strong>URL 编码协议</strong> 规定的。</p>
<p>URL 编码协议中规定：<font color="red">URL 只允许使用 ASCII 字符集中可以显示的字符来通过因特网进行发送。</font></p>
<p>关于 URL 编码更详细的说明，请参见博文系列中 [ &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2019/01/01/wang-zhan-ji-chu-zhi-url-jie-gou-jie-xi/">网站基础之 URL 结构解析</a> &lt;&lt;&lt;&lt; ] 关于 URL 编码的说明。</p>
<p><strong>以百度搜索查找关键词信息为例 &gt;&gt;&gt;&gt;</strong></p>
<p>打开 <a target="_blank" rel="noopener" href="https://www.baidu.com/">百度首页</a>，在搜索框中输入：<strong>爬虫</strong>，然后点击 “百度一下”。</p>
<p>当搜索结果显示后，此时地址栏的 URL 信息显示如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h ttps://www.baidu.com/s?ie=utf-8&amp;f=8&amp;rsv_bp=1&amp;rsv_idx=1&amp;tn=baidu&amp;wd=爬虫&amp;fenlei=256&amp;rsv_pq=0xbf2a62e800111815&amp;rsv_t=b88069xmyZzI8CzsTc9MHk9vLc%2Bpuy4NY0tyL6LdQx6z%2BCTKd1ZouF0Rn%2FFk&amp;rqlang=en&amp;rsv_enter=1&amp;rsv_dl=tb&amp;rsv_sug3=9&amp;rsv_sug1=9&amp;rsv_sug7=101&amp;rsv_sug2=0&amp;rsv_btype=i&amp;inputT=1812&amp;rsv_sug4=2982&amp;rsv_sug=1</span><br></pre></td></tr></table></figure>

<p>可以看到，URL 中包含了很多的查询字符串，你可以找到一个 <code>wd=爬虫</code>，其中 <code>wd</code> 表示查询字符串的键（word），而 <code>爬虫</code> 则代表你输入的值。</p>
<p>你可以在网页地址栏中删除多余的查询字符串，只保留 <code>wd=爬虫</code>，如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.baidu.com/s?wd=爬虫</span><br></pre></td></tr></table></figure>

<p>然后使用修改后的 URL 进行搜索，你会发现仍然得到了相同的页面。这是由于 <code>wd</code> 参数是 <strong>百度搜索的 关键查询参数</strong>。</p>
<p><strong>| &gt;&gt;&gt; ============================================ Split Line ========================================= &lt;&lt;&lt; |</strong></p>
<p>接下来，使用上面 【 1.1.1 &amp;&amp; 1.1.2 】小节介绍到的方法来抓取上述页面的 HTML 信息：</p>
<p><strong>[1] &gt;&gt;&gt; 编码 URL 查询字符串</strong></p>
<p>在模拟发送请求前，你需要对 URL 中不符合 URL 编码协议的查询字符串进行编码处理。</p>
<p><code>urllib.parse</code> 模块提供了 URL 的编码和解码方法，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码：</span></span><br><span class="line">urllib.parse.urlencode(&#123;<span class="string">&#x27;key&#x27;</span>:<span class="string">&#x27;value&#x27;</span>&#125;) <span class="comment"># 编码查询字符串字典</span></span><br><span class="line">urllib.parse.quote(string) <span class="comment"># 编码查询字符串</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 解码：</span></span><br><span class="line">urllib.parse.unquote(string) <span class="comment"># 对编码的查询字符串进行解码，以还原 URL</span></span><br></pre></td></tr></table></figure>

<p>注意 <code>urlencode(dict)</code> &amp;&amp; <code>quote(string)</code> 方法的使用差异：</p>
<p><font color="red">【示例一】</font> &gt;&gt;&gt; urlencode</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 urllib 网络库的 URL 解析模块 parse:</span></span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建查询字符串字典：</span></span><br><span class="line">query_str = &#123;<span class="string">&quot;wd&quot;</span>: <span class="string">&quot;爬虫&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码查询字符串字典：</span></span><br><span class="line">res = urllib.parse.urlencode(query_str)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 Python Format 字符串格式化方法，拼接 URL 地址：</span></span><br><span class="line">url = <span class="string">&quot;http://www.baidu.com/s?&#123;&#125;&quot;</span>.<span class="built_in">format</span>(res)</span><br><span class="line"><span class="comment"># 打印完整的编码后的 URL：</span></span><br><span class="line">print(url)</span><br></pre></td></tr></table></figure>

<p>编码后的完整 URL 如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.baidu.com/s?wd=%E7%88%AC%E8%99%AB</span><br></pre></td></tr></table></figure>

<p><font color="red">【示例二】</font> &gt;&gt;&gt; quote（URL 编码后结果同上）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 urllib 网络库的 URL 解析模块 parse:</span></span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建查询字符串：</span></span><br><span class="line">word = <span class="string">&quot;爬虫&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码查询字符串：</span></span><br><span class="line">res = urllib.parse.quote(word)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意与 urlencode 的不同：</span></span><br><span class="line">url = <span class="string">&quot;http://www.baidu.com/s?w=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(res)</span><br><span class="line"><span class="comment"># 打印完整的编码后的 URL：</span></span><br><span class="line">print(url)</span><br></pre></td></tr></table></figure>

<p>我们知道，经过浏览器编码处理之后的 URL 才是最终请求中的 URL，故通过编码后的 URL 必然也是可以访问相应的 Web 服务器的。你可以认为我们浏览器中通常可见的未编码处理的 URL 是给用户看的，而编码之后的 URL 是给设备使用的。</p>
<p><strong>[2] &gt;&gt;&gt; URL 地址拼接</strong></p>
<p>上面我们使用了 Python 的 Format 格式化方法进行了 URL 地址的拼接，你还可以：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python 字符串加法：</span></span><br><span class="line">baseurl = <span class="string">&#x27;http://www.baidu.com/s?&#x27;</span></span><br><span class="line">params = <span class="string">&#x27;wd=%E7%88%AC%E8%99%AB&#x27;</span></span><br><span class="line">url = baseurl + params</span><br><span class="line">print(url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Python 格式化占位符：</span></span><br><span class="line">params = <span class="string">&#x27;wd=%E7%88%AC%E8%99%AB&#x27;</span></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?%s&#x27;</span> % params</span><br><span class="line">print(url)</span><br></pre></td></tr></table></figure>

<p><strong>[3] &gt;&gt;&gt; 模拟发送请求 &amp;&amp; 获取 HTML 信息</strong></p>
<p>准备好访问的 URL，就可以模拟发送，以及抓取目标页面的 HTML 信息了~~~</p>
<hr>
<p>为了更好的使用 urllib 网络库编写爬虫，我们需要继续深入解读一下其中关键函数的用法：</p>
<h4 id="深入解读-UrlLib-库"><a href="#深入解读-UrlLib-库" class="headerlink" title="深入解读 UrlLib 库"></a>深入解读 UrlLib 库</h4><p>上面对 <code>urlopen()</code> 方法的简单使用，可以实现对简单页面的 GET 请求抓取。</p>
<p>如果我们想给 URL 传递一些隐式的参数该怎么实现呢（POST 请求）？？？</p>
<p><strong>[1] &gt;&gt;&gt; urllib.request.urlopen() 详解</strong></p>
<p>先来看一下 urlopen() 函数的 API：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=<span class="literal">None</span>, [timeout, ]*, cafile=<span class="literal">None</span>, capath=<span class="literal">None</span>, cadefault=<span class="literal">False</span>, context=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>可以发现，除了可以传递 URL 外，我们还可以传递其它的内容：比如 data（附加参数），timeout（超时时间）等等。</p>
<p><font color="red">1）data 参数（可选）</font></p>
<p>data 参数，需要的是字节流编码格式的内容，即 bytes 类型。也就是说，需要将构建的附加参数转化为字节流才可传入。</p>
<p>并且需要注意的是，如果你传入了 data 参数，它的请求方式就不再是 GET 方式请求，而是 POST 。看如下示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建参数字典：</span></span><br><span class="line">params = &#123;<span class="string">&quot;word&quot;</span>: <span class="string">&quot;hello&quot;</span>&#125;</span><br><span class="line"><span class="comment"># 采用 &quot;UTF-8&quot; 字符编码，编码参数字典字符串为字节串：</span></span><br><span class="line">data = <span class="built_in">bytes</span>(parse.urlencode(params), encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 发送 POST 请求：</span></span><br><span class="line">response = request.urlopen(<span class="string">&quot;http://httpbin.org/post&quot;</span>, data=data)</span><br><span class="line">print(response.read().decode(<span class="string">&quot;utf-8&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>这里是通过向 HTTP 测试网站：<a target="_blank" rel="noopener" href="http://httpbin.org/">httpbin.org</a>，发送 POST 请求来查看发送的请求和收到的响应信息。输出（请求）如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">&quot;data&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">  <span class="string">&quot;files&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">&quot;form&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;word&quot;</span>: <span class="string">&quot;hello&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">    <span class="string">&quot;Content-Length&quot;</span>: <span class="string">&quot;10&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/x-www-form-urlencoded&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;httpbin.org&quot;</span>,</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Python-urllib/3.7&quot;</span>,</span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-63aeec25-63fa00987b2dca0168c5ef58&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;json&quot;</span>: null,</span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;120.208.214.164&quot;</span>,</span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;http://httpbin.org/post&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，我们发送的附加参数出现在了 <code>form</code> 中，这表明是模拟了表单提交的方式，以 POST 方式传输数据。</p>
<blockquote>
<p>注意：HTTP 测试网站 &gt;&gt;&gt; <a target="_blank" rel="noopener" href="http://httpbin.org/">httpbin.org</a>，能测试 HTTP 请求和响应的各种信息，比如 cookie、IP、headers 和登录验证等，且支持 GET、POST 等多种方法，对 Web 开发和测试很有帮助。</p>
</blockquote>
<hr>
<p><font color="red">2）timeout 参数（可选）</font></p>
<p>timeout 参数可以设置请求的超时时间（单位为秒），支持 HTTP 、HTTPS 、FTP 请求。</p>
<p>如果请求超出了设置的这个时间还没有得到响应，就会抛出异常；如果不指定，就会使用全局默认时间。</p>
<p>来个实例感受一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">response = request.urlopen(<span class="string">&quot;http://httpbin.org/get&quot;</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line">print(response.read().decode(<span class="string">&quot;utf-8&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>这里我们 请求了 <code>http://httpbin.org/get</code> 这个测试链接，并且设置了超时时间是 <code>0.1</code> 秒（基本不可能得到服务器响应），于是抛出一个超市异常：<code>socket.timeout: timed out</code>。</p>
<p>因此，你可以通过设置超时时间来控制一个网页如果长时间未响应就跳过它的抓取：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">&#x27;http://httpbin.org/get&#x27;</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> socket.timeout:</span><br><span class="line">    print(<span class="string">&quot;Time Out !!!&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>或者，你可以捕获这个异常，做一些其它的处理。</p>
<hr>
<p><font color="red">3）其它参数</font></p>
<p>其中，<code>context</code> 参数，它必须是 <code>ssl.SSLContext</code> 类型，用来指定 SSL 设置。</p>
<p><code>cafile</code> 和 <code>capath</code>两个参数是指定 CA 证书和它的路径，这个在请求 HTTPS 链接时会有用。</p>
<p><code>cadefault</code> 参数现在已经弃用了，默认为 False。</p>
<hr>
<p><strong>[2] &gt;&gt;&gt; urllib.request.Request() 详解</strong></p>
<p>从上一小节可以看出，单纯使用 urlopen() 方法不足以构建一个完整的 HTTP 请求。</p>
<p>考虑一下，假设请求中需要添加请求头（Request Headers）等信息时，比如重构 User-Agent（用户代理，指用户使用的浏览器）使程序更像人类的请求，而非机器（反爬第一步）。怎么办？？？</p>
<p>我们可以使用更强大的 Request 类来构建一个请求，然后发送：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(<span class="string">&quot;https://www.baidu.com&quot;</span>)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">print(response.getcode())</span><br></pre></td></tr></table></figure>

<p>可以发现，我们依然使用 <code>urlopen()</code> 方法来发送这个请求，只不过这次 urlopen(url/Request) 方法的参数不再是一个 URL，而是一个 Request。<font color="red">通过构造这种数据结构（Request），一方面我们可以将请求独立成一个对象，另一方面可配置参数更加丰富和灵活</font>。</p>
<p>Request 类的构造方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span>(<span class="params">url, data=<span class="literal">None</span>, headers=&#123;&#125;, origin_req_host=<span class="literal">None</span>, unverifiable=<span class="literal">False</span>, method=<span class="literal">None</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>| &gt;&gt;&gt; ============================================ <strong>参数说明</strong> ========================================= &lt;&lt;&lt; |</p>
<p><strong>url</strong> 参数（必备），其它参数可选。</p>
<p><strong>data</strong> 参数同 urlopen 方法，必须传 bytes（字节流）类型的数据。如果添加参数是一个字典，可以先用 <code>urllib.parse.urlencode()</code> 编码为字符串，然后转化为字节串。</p>
<p><strong>headers</strong> 参数是一个字典，你可以在构造 Request 时通过 headers 参数传递，也可以通过调用 Request 对象的 <code>add_header()</code> 方法来添加请求头。最常用的就是重构 User-Agent，默认的 User-Agent 是 <code>Python-urllib</code>（爬虫访问），你可以通过重构它来伪装成浏览器（而非爬虫），使程序更像人类的请求，而非机器。</p>
<p><strong>origin_req_host</strong> 指的是请求方的 host 名称或者 IP 地址。</p>
<p><strong>unverifiable</strong> 指的是这个请求是否是无法验证的，默认是 False。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个 HTML 文档中的图片，但是我们没有自动抓取图像的权限，这时 unverifiable 的值就是 True。</p>
<p><strong>method</strong> 是一个字符串，它用来指示请求使用的方法，比如 “GET”，”POST”，”PUT” 等等。</p>
<p>| &gt;&gt;&gt; ============================================================================================ &lt;&lt;&lt; |</p>
<p>构建一个 Request 来感受一下其强大：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://httpbin.org/post&quot;</span></span><br><span class="line"></span><br><span class="line">params = &#123;<span class="string">&quot;word&quot;</span>: <span class="string">&quot;Spider&quot;</span>&#125;</span><br><span class="line">data = <span class="built_in">bytes</span>(urllib.parse.urlencode(params), encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求头信息：</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="comment"># 伪装成：Windows IE</span></span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;</span>,</span><br><span class="line">    <span class="string">&quot;host&quot;</span>: <span class="string">&quot;httpbin.org&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">req = urllib.request.Request(url=url, data=data, headers=headers, method=<span class="string">&quot;POST&quot;</span>)</span><br><span class="line">response = urllib.request.urlopen(req, timeout=<span class="number">1</span>)</span><br><span class="line">print(response.read().decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>运行结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">&quot;data&quot;</span>: <span class="string">&quot;&quot;</span>,</span><br><span class="line">  <span class="string">&quot;files&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">&quot;form&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;word&quot;</span>: <span class="string">&quot;Spider&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Content-Length&quot;</span>: <span class="string">&quot;11&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/x-www-form-urlencoded&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;httpbin.org&quot;</span>,</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-63af03d3-26062be47c618d2115808836&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;json&quot;</span>: null,</span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;120.208.214.164&quot;</span>,</span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;http://httpbin.org/post&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，我们已经成功设置了 data，headers 以及 method。</p>
<p>另外，你也可以通过 Request 对象的 <code>add_header(key, value)</code> 方法来添加 headers：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">req = urllib.request.Request(url=url, data=data, method=<span class="string">&quot;POST&quot;</span>)</span><br><span class="line">req.add_header(<span class="string">&quot;User-Agent&quot;</span>, <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>[3] &gt;&gt;&gt; urllib.request 高级特性</strong></p>
<p>上面，我们虽然可以构造 Request 请求对象，但是一些更高级但常见的操作，如  authenticaton（授权验证），redirections（重定向)、Cookies、代理 IP 怎么设置？？？</p>
<p>这就需要更强大的工具 <strong>Handler</strong> 登场了~~~</p>
<p>简而言之，你可以把它理解为各种处理器（Handler）：有专门处理登录验证的，有处理 Cookies 的，有处理代理设置的，利用它们我们几乎可以做到任何 HTTP 请求中所有的事情。</p>
<p><strong>处理器（Handler）类说明 &gt;&gt;&gt;&gt;</strong></p>
<p>首先需要说明下 <code>urllib.request.BaseHandler</code>，它是所有其他 Handler 的父类，提供了最基本的 Handler 的方法。</p>
<p>接下来就有各种 Handler 类继承这个 BaseHandler：</p>
<ul>
<li>HTTPDefaultErrorHandler：用于处理 HTTP 响应错误的处理器（错误都会抛出 HTTPError 类型的异常）；</li>
<li>HTTPRedirectHandler：用于处理重定向的处理器；</li>
<li>HTTPCookieProcessor：用于处理 Cookie 的处理器；</li>
<li>ProxyHandler：用于设置代理的处理器（默认代理为空）；</li>
<li>HTTPPasswordMgr：用于管理密码的处理器，它维护了用户名密码的表；</li>
<li>HTTPBasicAuthHandler：用于管理认证的处理器，如果一个链接打开时需要认证，那么可以用它来解决认证问题。</li>
</ul>
<p>其他的 Handler，可参考 &gt;&gt;&gt; <a target="_blank" rel="noopener" href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler">官方文档</a>。</p>
<p><strong>OpenerDirector 概念 &gt;&gt;&gt;&gt;</strong></p>
<p>OpenerDirector 也叫 <code>Opener</code>。前面的 <code>urllib.request.urlopen()</code> 方法实际上就是一个 Opener。</p>
<p>为什么要引入 Opener 呢？？？</p>
<p>之前我们使用的 Request、urlopen() 相当于类库为你封装好了极其常用的请求方法，利用它们两个就可以完成基本的请求。但是现在我们需要实现更高级的功能，所以就要用到比调用 Request &amp;&amp; urlopen() 的对象的更普遍的对象，也就是 Opener。</p>
<p>并且，Opener 可以使用 <code>open()</code> 方法，返回的类型和 urlopen() 如出一辙。</p>
<p>那么 Opener 和 Handler 有什么关系？简而言之，就是利用 Handler 来构建 Opener。</p>
<hr>
<p><strong>代理设置样例 &gt;&gt;&gt;&gt;</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">proxy_handler = urllib.request.ProxyHandler(&#123;</span><br><span class="line"><span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;http://218.202.111.10:80&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;https&#x27;</span>: <span class="string">&#x27;https://180.250.163.34:8888&#x27;</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = urllib.request.build_opener(proxy_handler)</span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure>

<p>上面，ProxyHandler 的参数是一个字典，key 是协议类型，比如 http 还是 https 等，value 是代理链接，可以添加多个代理。</p>
<p>然后利用 build_opener() 方法，利用 ProxyHandler 构造一个 Opener ，然后发送请求即可。</p>
<p><strong>Cookie 设置样例 &gt;&gt;&gt;&gt;</strong></p>
<p>如何将网站的 Cookie 获取下来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">&quot;=&quot;</span>+item.value)</span><br></pre></td></tr></table></figure>

<p>首先，声明了一个 <code>CookieJar</code> 对象，接下来使用 <code>HTTPCookieProcessor</code> 来构建一个 handler ，最后利用 <code>build_opener</code> 方法构建出 opener ，执行 open() 即可。</p>
<p>程序运行结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">BAIDUID=552C3DCBEB1E5259021C3B13D89EFE9E:FG=<span class="number">1</span></span><br><span class="line">BIDUPSID=552C3DCBEB1E5259DB7B1C021AEC02BD</span><br><span class="line">H_PS_PSSID=<span class="number">36548_37647_38024_38012_36921_37990_37793_37922_38000_37901_26350_37881</span></span><br><span class="line">PSTM=<span class="number">1672418818</span></span><br><span class="line">BDSVRTM=<span class="number">0</span></span><br><span class="line">BD_HOME=<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>可以看到输出了每一条 Cookie 的名称还有值。</p>
<p>不过既然能输出，那可不可以输出成文件格式呢？？？我们知道很多 Cookie 实际也是以文本形式保存的，实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;cookie.txt&#x27;</span></span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>这时的 CookieJar 就需要换成 <code>MozillaCookieJar</code> ，生成文件时需要用到它，它是 CookieJar 的子类，可以用来处理 Cookie 和文件相关的事件，读取和保存 Cookie ，它可以将 Cookie 保存成 Mozilla 型的格式。</p>
<p>运行之后可以发现生成了一个 <code>cookie.txt</code> 文件，内容如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Netscape HTTP Cookie File</span><br><span class="line"># http://curl.haxx.se/rfc/cookie_spec.html</span><br><span class="line"># This is a generated file!  Do not edit.</span><br><span class="line"></span><br><span class="line">.baidu.com  TRUE    /   FALSE   1703955537  BAIDUID 57210CFA95ED4AAE5E41CE04319F9861:FG=1</span><br><span class="line">.baidu.com  TRUE    /   FALSE   3819903184  BIDUPSID    57210CFA95ED4AAEE6DCD16048C7FA1B</span><br><span class="line">.baidu.com  TRUE    /   FALSE       H_PS_PSSID  36552_37647_37906_38014_37625_36920_37989_37936_37951_37903_26350_22158_37881</span><br><span class="line">.baidu.com  TRUE    /   FALSE   3819903184  PSTM    1672419537</span><br><span class="line">www.baidu.com   FALSE   /   FALSE       BDSVRTM 0</span><br><span class="line">www.baidu.com   FALSE   /   FALSE       BD_HOME 1</span><br></pre></td></tr></table></figure>

<p>另外还有一个 <code>LWPCookieJar</code>，同样可以读取和保存 Cookie。但是保存的格式和 MozillaCookieJar 的不一样，它会保存成与libwww-perl的Set-Cookie3文件格式的 Cookie。使用时只需要在声明时就改为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)</span><br></pre></td></tr></table></figure>

<p>生成文件内容如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#LWP-Cookies-2.0</span><br><span class="line">Set-Cookie3: BAIDUID=&quot;E4A0DC4870957473807CD2478492DDD5:FG=1&quot;; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; expires=&quot;2023-12-30 17:03:19Z&quot;; comment=bd; version=0</span><br><span class="line">Set-Cookie3: BIDUPSID=E4A0DC4870957473D133AD644C840120; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; expires=&quot;2091-01-17 20:17:26Z&quot;; version=0</span><br><span class="line">Set-Cookie3: H_PS_PSSID=36554_37647_38024_37907_38018_37623_36920_37990_37797_37927_37952_37904_26350_37881; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; discard; version=0</span><br><span class="line">Set-Cookie3: PSTM=1672419800; path=&quot;/&quot;; domain=&quot;.baidu.com&quot;; path_spec; domain_dot; expires=&quot;2091-01-17 20:17:26Z&quot;; version=0</span><br><span class="line">Set-Cookie3: BDSVRTM=0; path=&quot;/&quot;; domain=&quot;www.baidu.com&quot;; path_spec; discard; version=0</span><br><span class="line">Set-Cookie3: BD_HOME=1; path=&quot;/&quot;; domain=&quot;www.baidu.com&quot;; path_spec; discard; version=0</span><br></pre></td></tr></table></figure>

<p>既然生成了 Cookie 文件，怎样从文件读取并利用呢？？？</p>
<p>以 LWPCookieJar 格式为例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">cookie = http.cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(<span class="string">&#x27;cookie.txt&#x27;</span>, ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">response = opener.<span class="built_in">open</span>(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line">print(response.status)</span><br></pre></td></tr></table></figure>

<p>前提是我们首先利用上面的方式生成了 LWPCookieJar 格式的 Cookie ，然后利用 <code>load()</code> 方法，传入文件名称，后面同样的方法构建 handler 和 opener 即可。</p>
<p><strong>| &gt;&gt;&gt; ============================================== Split Line =========================================== &lt;&lt;&lt; |</strong></p>
<p>事实上，Python 内置的 urllib 网络库的使用较为 “繁琐”，不利于初学者的掌握。</p>
<p>后续，我们会引入一个第三方的，方便、快捷的 <strong>Requests</strong> 库，Requests 库是在 urllib 的基础上开发而来，其宗旨就是 <strong>“让 HTTP 服务于人类”</strong>。</p>
<p>关于 Requests 库的使用说明可以参见博文系列中 [ &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2020/01/22/web-crawler-jiao-cheng-zhi-wang-luo-pa-chong-gong-ju-ku/">Web Crawler 教程之网络爬虫工具库</a> &lt;&lt;&lt;&lt;] 中 Requests 网络库部分的说明。</p>
<hr>
<p>↓↓↓↓↓↓↓ 反爬第一步 ↓↓↓↓↓↓↓</p>
<h3 id="User-Agent"><a href="#User-Agent" class="headerlink" title="User-Agent"></a>User-Agent</h3><p>User-Agent（UA）即用户代理，它是一个特殊字符串头部字段（headers）。</p>
<p>网站服务器，可以通过识别请求头中的 UA 来确定用户所使用的操作系统版本、CPU 类型、浏览器版本等信息，然后通过判断 UA 来给客户端发送不同的页面。</p>
<p>大多数网站，会通过识别请求头中 <code>User-Agent</code> 信息来判断是否是爬虫访问网站（<strong>Python-urllib</strong>）。例如，一旦检测到是爬虫在访问，会对发送请求的 IP 进行预警并重点监控，如果发现 IP 超过规定时间内的访问次数， 将在一段时间内禁止其再次访问网站（<strong>封 IP</strong>）。如果你在爬虫时登录了该网站，甚至会被封禁登录账户（<strong>封账户</strong>）。</p>
<p>所以，你需要重构爬虫程序访问时的 User-Agent，这是必要的，这是反爬策略的第一步！！！</p>
<p>你可以，<strong>基于常见的浏览器 User-Agent 重构爬虫 UA &gt;&gt;&gt;&gt;</strong></p>
<table>
<thead>
<tr>
<th>系统</th>
<th>浏览器</th>
<th>User-Agent字符串</th>
</tr>
</thead>
<tbody><tr>
<td>Mac</td>
<td>Chrome</td>
<td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.75 Safari/537.36</td>
</tr>
<tr>
<td>Mac</td>
<td>Firefox</td>
<td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:65.0) Gecko/20100101 Firefox/65.0</td>
</tr>
<tr>
<td>Mac</td>
<td>Safari</td>
<td>Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.3 Safari/605.1.15</td>
</tr>
<tr>
<td>Windows</td>
<td>Edge</td>
<td>Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763</td>
</tr>
<tr>
<td>Windows</td>
<td>IE</td>
<td>Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko</td>
</tr>
<tr>
<td>Windows</td>
<td>Chrome</td>
<td>Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36</td>
</tr>
<tr>
<td>iOS</td>
<td>Chrome</td>
<td>Mozilla/5.0 (iPhone; CPU iPhone OS 7_0_4 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) CriOS/31.0.1650.18 Mobile/11B554a Safari/8536.25</td>
</tr>
<tr>
<td>iOS</td>
<td>Safari</td>
<td>Mozilla/5.0 (iPhone; CPU iPhone OS 8_3 like Mac OS X) AppleWebKit/600.1.4 (KHTML, like Gecko) Version/8.0 Mobile/12F70 Safari/600.1.4</td>
</tr>
<tr>
<td>Android</td>
<td>Chrome</td>
<td>Mozilla/5.0 (Linux; Android 4.2.1; M040 Build/JOP40D) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.59 Mobile Safari/537.36</td>
</tr>
<tr>
<td>Android</td>
<td>Webkit</td>
<td>Mozilla/5.0 (Linux; U; Android 4.4.4; zh-cn; M351 Build/KTU84P) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>更多浏览器 UA 信息（移动端 &amp;&amp; PC端）可参考 [ &gt;&gt;&gt; <a target="_blank" rel="noopener" href="http://tools.jb51.net/table/useragent">常见的浏览器 User-Agent</a> &lt;&lt;&lt; ]</p>
<p><strong>如何查看本机的浏览器版本以及 UA 信息 &gt;&gt;&gt;&gt;</strong> [ &gt;&gt;&gt; <a target="_blank" rel="noopener" href="https://useragent.buyaocha.com/">Browser Version &amp;&amp; UA 在线识别工具</a> &lt;&lt;&lt; ]</p>
<hr>
<h4 id="爬虫程序-UA-信息"><a href="#爬虫程序-UA-信息" class="headerlink" title="爬虫程序 UA 信息"></a>爬虫程序 UA 信息</h4><p>你可以通过 HTTP 测试网站 &gt;&gt;&gt; <a target="_blank" rel="noopener" href="http://httpbin.org/">http://httpbin.org/</a> &gt;&gt;&gt; 发送一个 GET 请求来获取请求头信息，从而获取爬虫程序的 UA 信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(<span class="string">&quot;http://httpbin.org/get&quot;</span>)</span><br><span class="line">print(response.read().decode(<span class="string">&quot;utf-8&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>输出的请求头信息如下：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;args&quot;</span>: &#123;&#125;,</span><br><span class="line"></span><br><span class="line">  # 请求头信息</span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;,</span><br><span class="line">    &quot;User-Agent&quot;: &quot;Python-urllib/3.7&quot;, # User-Agent 信息包含在请求头中</span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root=1-63afe3f5-431e63ad6501f31a0c7aca33&quot;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  &quot;origin&quot;: &quot;120.208.214.xxx&quot;,</span><br><span class="line">  &quot;url&quot;: &quot;http://httpbin.org/get&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，爬虫程序的 User-Agent 竟然是 <strong>Python-urllib/3.7</strong>，网站基于此会判断出是爬虫脚本在访问。</p>
<hr>
<p>所以，我们需要重构爬虫程序访问时的 User-Agent，以伪装成 “浏览器” 访问网站：</p>
<h4 id="重构爬虫-UA-信息"><a href="#重构爬虫-UA-信息" class="headerlink" title="重构爬虫 UA 信息"></a>重构爬虫 UA 信息</h4><p>你可以使用 <code>urllib.request</code> 中的 <code>Request</code> 类来重构 User-Agent 信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://httpbin.org/get&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重构请求头中 User-Agent：伪装成 Chrome 浏览器，可以使用上面常见的浏览器 User-Agent 信息支持</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">req = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">&quot;utf-8&quot;</span>))</span><br></pre></td></tr></table></figure>

<p>输出的请求信息如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;args&quot;</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">&quot;headers&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;Host&quot;</span>: <span class="string">&quot;httpbin.org&quot;</span>,</span><br><span class="line">    <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36&quot;</span>, </span><br><span class="line">    <span class="string">&quot;X-Amzn-Trace-Id&quot;</span>: <span class="string">&quot;Root=1-63afed6e-65a5d0397ba0e73d1fbf3e1b&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">&quot;origin&quot;</span>: <span class="string">&quot;120.208.214.164&quot;</span>,</span><br><span class="line">  <span class="string">&quot;url&quot;</span>: <span class="string">&quot;http://httpbin.org/get&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，网站接收到的请求头信息中的 UA 信息已经变为伪装的 Chrome 浏览器 UA 了。</p>
<hr>
<h4 id="构建-UA-代理池"><a href="#构建-UA-代理池" class="headerlink" title="构建 UA 代理池"></a>构建 UA 代理池</h4><p>如果短时间内总是使用一个 UA 来高频率访问网站，可能会引起网站的警觉，认为是爬虫在访问，从而封禁 IP（账户）。</p>
<p>因此，我们需要构建用户代理池（User-Agent Pool），避免总是使用一个 UA 来访问网站。</p>
<p><font color="red">用户代理池（User-Agent Pool），就是把多个浏览器的 UA 信息放入一个列表中，访问网站时从中随机选择一个浏览器 UA。</font></p>
<p><strong>自定义 UA 代理池 &gt;&gt;&gt;&gt;</strong></p>
<p>通过收集的浏览器 UA 来自定义一个 User-Agent Pool，然后随机获取 UA：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建随机产生 UA 的自定义用户代理池</span></span><br><span class="line">ua_pool = [</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Maxthon 2.0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Opera/9.80 (Windows NT 6.1; U; en) Presto/2.8.131 Version/11.11&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv:2.0.1) Gecko/20100101 Firefox/4.0.1&#x27;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 Python 随机模块 random 随机产生 UA</span></span><br><span class="line"><span class="comment"># ua_info = ua_pool[random.randint(0, len(ua_pool)-1)]</span></span><br><span class="line">ua_info = random.choice(ua_pool)</span><br><span class="line">print(ua_info)</span><br></pre></td></tr></table></figure>

<hr>
<p>除了上述的自定义用户代理池的方法，还有专门的第三方库支持随机获取浏览器 UA 信息（不用手动收集）。</p>
<p><strong>随机产生 UA 的用户代理池第三方库 &gt;&gt;&gt;&gt; fake-useragent</strong></p>
<p>由于 <code>fake-useragent</code> 库是一个第三方库，故需要单独按照：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install fake-useragent</span><br></pre></td></tr></table></figure>

<p>安装成功之后，来看如何使用 <code>fake-useragent</code> 模块随机产生一个 UA：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用第三方用户代理池库随机产生 UA</span></span><br><span class="line"><span class="keyword">import</span> fake_useragent</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化用户代理词对象</span></span><br><span class="line">ua = fake_useragent.UserAgent()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机产生一个 UA：</span></span><br><span class="line"></span><br><span class="line">print(ua.random)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># Mozilla/5.0 (Windows; U; Windows NT 6.1; de-DE) AppleWebKit/534.10 (KHTML, like Gecko) Chrome/8.0.552.224 Safari/534.10</span></span><br></pre></td></tr></table></figure>

<p><strong>如何产生指定浏览器的随机 UA &gt;&gt;&gt;</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用第三方用户代理池库随机产生 UA</span></span><br><span class="line"><span class="keyword">import</span> fake_useragent</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化用户代理词对象</span></span><br><span class="line">ua = fake_useragent.UserAgent()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 支持的 Browser 列表：</span></span><br><span class="line"><span class="comment"># [&quot;chrome&quot;, &quot;edge&quot;, &quot;internet explorer&quot;, &quot;firefox&quot;, &quot;safari&quot;, &quot;opera&quot;]</span></span><br><span class="line">print(ua.ie)  <span class="comment"># &quot;internet explorer（ie）&quot;</span></span><br><span class="line">print(ua.firefox)  <span class="comment"># &quot;firefox&quot;</span></span><br><span class="line">print(ua.chrome)  <span class="comment"># &quot;chrome&quot;</span></span><br><span class="line">print(ua.edge)  <span class="comment"># &quot;edge&quot;</span></span><br><span class="line">print(ua.safari)  <span class="comment"># &quot;safari&quot;</span></span><br><span class="line">print(ua.opera)  <span class="comment"># &quot;opera&quot;</span></span><br></pre></td></tr></table></figure>

<p>输入的不同浏览器的 UA 信息如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Mozilla/4.0 (compatible; MSIE 5.16; Mac_PowerPC)</span><br><span class="line">Mozilla/5.0 (X11; U; Linux i686; fr; rv:1.9.0.9) Gecko/2009042113 Ubuntu/8.04 (hardy) Firefox/3.0.9</span><br><span class="line">Mozilla/6.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.195.27 Safari/532.0</span><br><span class="line">Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17720</span><br><span class="line">Mozilla/5.0 (Macintosh; U; PPC Mac OS X; fr) AppleWebKit/412.7 (KHTML, like Gecko) Safari/412.5</span><br><span class="line">Opera/9.64 (X11; Linux i686; U; sv) Presto/2.1.1</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="实例：爬虫抓取网页信息"><a href="#实例：爬虫抓取网页信息" class="headerlink" title="实例：爬虫抓取网页信息"></a>实例：爬虫抓取网页信息</h3><p>有了上面的知识储备，这里完成我们的第一个 Python 爬虫实战案例：抓取期望的网页信息，并将其保存至本地。</p>
<p>案例说明：抓取 <a target="_blank" rel="noopener" href="https://www.baidu.com/">百度搜索</a> 关键词后检索到的首页信息 &gt;&gt;&gt;&gt;</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> fake_useragent</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取完整的 URL：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getUrl</span>(<span class="params">word</span>):</span></span><br><span class="line">    url = <span class="string">&quot;http://www.baidu.com/s?&#123;&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 编码 URL 中查询字符串：</span></span><br><span class="line">    query_str = &#123;<span class="string">&quot;wd&quot;</span>: word&#125;</span><br><span class="line">    params = urllib.parse.urlencode(query_str)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拼接 URL：</span></span><br><span class="line">    url = url.<span class="built_in">format</span>(params)</span><br><span class="line">    print(<span class="string">&quot;Request URL: &quot;</span> + url)</span><br><span class="line">    <span class="keyword">return</span> url</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机获取浏览器伪装 UA：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getUA</span>():</span></span><br><span class="line">    ua = fake_useragent.UserAgent()</span><br><span class="line">    <span class="keyword">return</span> ua.edge</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求检索到的首页信息（GET）：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">requestHtml</span>(<span class="params">url</span>):</span></span><br><span class="line">    ua_info = getUA()</span><br><span class="line">    print(<span class="string">&quot;User-Agent: &quot;</span> + ua_info)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&quot;User-Agent&quot;</span>: ua_info,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 重构请求对象：</span></span><br><span class="line">    req = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">    <span class="comment"># 发送请求：</span></span><br><span class="line">    response = urllib.request.urlopen(req)</span><br><span class="line">    print(<span class="string">&quot;Request Status Code:&quot;</span>, response.status)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 期望的网页信息：</span></span><br><span class="line">    html = response.read().decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地持久化：</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataSave</span>(<span class="params">word, data</span>):</span></span><br><span class="line">    filename = word + <span class="string">&quot;.html&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 主程序入口：</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    print(<span class="string">&quot;| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Start Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |&quot;</span>)</span><br><span class="line">    word = <span class="built_in">input</span>(<span class="string">&quot;Please Enter Your Search: &quot;</span>)</span><br><span class="line">    url = getUrl(word)</span><br><span class="line">    html = requestHtml(url)</span><br><span class="line">    dataSave(word, html)</span><br><span class="line">    print(<span class="string">&quot;| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Close Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以发现，通过百度搜索关键词后检索到的首页 HTML 代码都抓取了下来，并且保存在了被你的文件中。</p>
<blockquote>
<p>需要注意的是，查看生成的网页信息文件时，你可能会发现抓取到的不是网页信息，响应内容显示 <strong>….百度安全验证….网络不给力，请稍后重试….返回首页…问题反馈</strong>。出现此问题可能是请求头定义不完善被反爬，详细请参见后文【网络爬虫常见问题】&gt;&gt;&gt;【百度安全验证问题】。</p>
</blockquote>
<hr>
<h2 id="审查网页结构以及元素节点信息"><a href="#审查网页结构以及元素节点信息" class="headerlink" title="审查网页结构以及元素节点信息"></a>审查网页结构以及元素节点信息</h2><p>接着来看，<font color="red">如何通过浏览器并借助网页元素审查工具分析网页结构以及元素节点信息？！！</font></p>
<p>前面我们已经可以将网页中的所有信息（不管是否有用）都抓取下来了，为了提取出我们期望的信息，你必须先了解抓取网页文档的结构以及元素节点信息！！！</p>
<h3 id="网页的构成"><a href="#网页的构成" class="headerlink" title="网页的构成"></a>网页的构成</h3><p>根据 W3C 标准模式，网页一般由三部分组成：</p>
<ul>
<li>HTML：负责定义网页的内容；</li>
<li>CSS：负责描述网页的样式；</li>
<li>JavaScript：负责网页的行为。</li>
</ul>
<p>网页构成是爬虫的基础，关于 HTML &amp;&amp; CSS &amp;&amp; JavaScript 的详细说明可以参考相关博文系列。</p>
<hr>
<h3 id="动态网页-VS-静态网页"><a href="#动态网页-VS-静态网页" class="headerlink" title="动态网页 VS 静态网页"></a>动态网页 VS 静态网页</h3><p>实际上，在编写爬虫之前，你首先需要 <strong>明确待爬取页面的类型：页面是静态的，还是动态的？？？</strong></p>
<p>这是由于，对于不同的网页类型，编写爬虫程序时所使用的方法也不尽相同。</p>
<p>| &gt;&gt;&gt; ============================================= <strong>Split Line</strong> ========================================== &lt;&lt;&lt; |</p>
<h4 id="静态网页"><a href="#静态网页" class="headerlink" title="静态网页"></a>静态网页</h4><p>静态网页是网站设计的基础，早期的网站一般都是由静态网页制作的。</p>
<p>纯粹 HTML 格式的网页通常被称为 “静态网页”，静态网页是标准的 HTML 文件，它的文件扩展名是 <code>.htm</code> Or <code>.html</code>，可以包含文本、图像、声音、FLASH 动画、客户端脚本和 ActiveX 控件等。</p>
<p><font color="red">容易误解的是&gt;&gt;&gt;&gt; 静态并非静止不动</font> ，页面中也可以出现各种动态的效果，如 GIF 动画、FLASH、滚动字幕等等，这只是一种网页内容的表现形式。</p>
<p>我们知道，当页面所包含的信息量较大时，网页的生成速度会降低。而由于 <font color="red">静态网页的内容相对固定，且不需要连接后台数据库，因此响应速度非常快。</font>但静态网页的更新相较比较麻烦，需要将所有的更新内容添加的页面中，故一般适用于更新较少的展示型网站。</p>
<p><strong>静态页面抓取 &gt;&gt;&gt;</strong></p>
<p>静态网页可以通过 GET/POST 请求方法直接获取，<font color="green">它的数据全部包含在 GET/POST 请求返回的 HTML 文档中，因此爬虫程序可以直接在返回的 HTML 文档中提取数据。</font></p>
<p>也就是说，只要通过分析静态网页的 URL，找到 URL 查询参数的变化规律之后，就可以实现静态页面的抓取了。</p>
<p>与动态网页相比，静态网页对搜索引擎更加友好，有利于搜索引擎的收录。</p>
<hr>
<h4 id="动态网页"><a href="#动态网页" class="headerlink" title="动态网页"></a>动态网页</h4><p>动态网页，指的是采用了动态网页技术的页面，例如：AJAX（是指一种创建交互式、快速动态网页应用的网页开发技术）、ASP(是一种创建动态交互式网页并建立强大的 web 应用程序)、JSP(是 Java 语言创建动态网页的技术标准) 等技术。</p>
<p>动态网页中，不需要重新加载整个页面内容，就可以实现网页的局部更新。</p>
<p>也就是说，<font color="green">动态页面使用 “动态页面技术” 与服务器进行数据交换，从而实现了网页的异步加载。</font></p>
<p><strong>动态页面技术 &gt;&gt;&gt;</strong></p>
<p>实际上，你可以将 动态页面技术 理解为：页面中除了 HTML 标记语言外的一些具有特定功能的代码。</p>
<p>这些代码，可以使得浏览器和服务器进行交互，服务器端会根据客户端的不同请求，执行可能涉及到数据库连接、访问、查询等一系列的 IO 操作（响应速度略差于静态网页），然后返回请求信息给浏览器，从而实现网页的异步加载。</p>
<p>以查看百度图片为例 &gt;&gt;&gt;</p>
<p>浏览器中打开百度图片（<a target="_blank" rel="noopener" href="https://image.baidu.com/">https://image.baidu.com/</a>）并搜索 Python，向下滚动鼠标滑轮，会查看到越来越多的逐渐加载出来的图片。</p>
<p>也就意味着，当你滚动鼠标滑轮时，网页会从服务器数据库自动加载数据并渲染页面。如下所示：</p>
<p><img src="https://s2.loli.net/2022/12/31/5Cv3r9IoJwZilF2.png"></p>
<p><strong>动态页面抓取 &gt;&gt;&gt;</strong></p>
<p>抓取动态网页的过程较为复杂，需要通过动态抓包来获取客户端与服务器交互的 JSON 数据。</p>
<p>抓包可以使用谷歌浏览器开发者模式（快捷键：F12）Network 选项，然后点击 <code>XHR</code>，找到获取 JSON 数据的 URL，如下所示：</p>
<p><img src="https://s2.loli.net/2022/12/31/34wsG6zg2SJfViY.png"></p>
<p>或者，你也可以使用专业的抓包工具 &gt;&gt;&gt;&gt; <a target="_blank" rel="noopener" href="https://www.telerik.com/fiddler">Fiddler</a>。</p>
<hr>
<h3 id="审查页面元素"><a href="#审查页面元素" class="headerlink" title="审查页面元素"></a>审查页面元素</h3><p>对于一个优秀的爬虫工程师而言，要善于发现 HTML 网页元素的规律，并且能从中提炼出有效的信息。</p>
<p>浏览器都自带审查页面元素（Inspect）的功能，你可以通过打开浏览器的开发者工具（F12），审查页面元素功能在页面的左上角，如图：</p>
<p><img src="https://s2.loli.net/2022/12/31/Mzvnyak6S7Bd1Eg.png"></p>
<p>通过审查页面元素（Inspect）功能，你可以确定网页中某内容所对应的 HTML 代码位置。下图以 <strong>百度首页中搜索框</strong> 为例：</p>
<p><img src="https://s2.loli.net/2022/12/31/4enHmYRyrs8DpM1.png"></p>
<p>点击审查元素按钮 &gt;&gt;&gt; 将鼠标移动至想审查的位置（如：百度的输入框），然后单击 &gt;&gt;&gt; 自动显示该位置内容的代码段（如上图）。</p>
<p>并且，代码段处支持快速复制 &gt;&gt;&gt; 右击代码段 &gt;&gt;&gt; Copy 选项卡 &gt;&gt;&gt; 二级会话框内选择 Copy element。即可复制正在审查的元素代码，如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">&quot;kw&quot;</span> <span class="attr">name</span>=<span class="string">&quot;wd&quot;</span> <span class="attr">class</span>=<span class="string">&quot;s_ipt&quot;</span> <span class="attr">value</span>=<span class="string">&quot;&quot;</span> <span class="attr">maxlength</span>=<span class="string">&quot;255&quot;</span> <span class="attr">autocomplete</span>=<span class="string">&quot;off&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>依照上述方法，你可以检查页面内的所有元素。</p>
<hr>
<p><strong>编辑网页代码 &gt;&gt;&gt;&gt;</strong></p>
<p>通过元素审查，你可以快速定位到页面内容的元素代码段。然后你可以基于定位代码段，更改网页代码。</p>
<p>以 <a target="_blank" rel="noopener" href="https://mail.aliyun.com/">阿里云个人邮箱登录界面</a> 为例：</p>
<p><img src="https://s2.loli.net/2022/12/31/WHNeTp64cmIUv1E.png"></p>
<p>检查密码框的 HTML 代码，代码如下所示：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">&quot;fm-login-password&quot;</span> <span class="attr">class</span>=<span class="string">&quot;fm-text&quot;</span> <span class="attr">type</span>=<span class="string">&quot;password&quot;</span> <span class="attr">name</span>=<span class="string">&quot;password&quot;</span> <span class="attr">tabindex</span>=<span class="string">&quot;2&quot;</span> <span class="attr">placeholder</span>=<span class="string">&quot;Password&quot;</span> <span class="attr">autocorrect</span>=<span class="string">&quot;off&quot;</span> <span class="attr">autocapitalize</span>=<span class="string">&quot;off&quot;</span> <span class="attr">data-spm-anchor-id</span>=<span class="string">&quot;0.0.0.i1.18d9614fk4hZWl&quot;</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>你只需要在代码段上稍微做一下更改（双击 <code>type=&quot;password&quot;</code> 将输入框类型更改为 <code>type=&quot;text&quot;</code>），密码就会变为可见状态。效果如下：</p>
<p><img src="https://s2.loli.net/2022/12/31/moBkhNTPJ4wOfvi.png"></p>
<p>此操作适用于所有网站的登录界面！！！</p>
<blockquote>
<p>需要注意的是，更改网页代码效果仅限本次有效，当关闭（或重新刷新）网页后，会自动恢复为原来的状态。</p>
</blockquote>
<hr>
<h3 id="检查网页结构"><a href="#检查网页结构" class="headerlink" title="检查网页结构"></a>检查网页结构</h3><p>对于网络爬虫而言，检查网页结构是至关重要的一步。</p>
<p>检查网页结构，即 <font color="red">对网页的 HTML 文档结构进行分析，并找出要提取信息所对应元素节点的相似性（规律）。</font></p>
<p>以 <a target="_blank" rel="noopener" href="https://www.maoyan.com/board">猫眼电影网</a> 榜单页面为例（期望提取榜单中电影信息），审查每部影片的 HTML 元素结构：</p>
<p><img src="https://s2.loli.net/2022/12/31/97YUVqSmG4sRaQt.png"></p>
<p>第一部影片的代码段如下所示：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-main&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-content&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-info&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;name&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/films/1205&quot;</span> <span class="attr">title</span>=<span class="string">&quot;放牛班的春天&quot;</span> <span class="attr">data-act</span>=<span class="string">&quot;boarditem-click&quot;</span> <span class="attr">data-val</span>=<span class="string">&quot;&#123;movieId:1205&#125;&quot;</span>&gt;</span>放牛班的春天<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;star&quot;</span>&gt;</span>主演：热拉尔·朱诺,弗朗西斯·贝尔兰德,凯德·麦拉德<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;releasetime&quot;</span>&gt;</span>上映时间：2004-10-16<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-number score-num&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;score&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;integer&quot;</span>&gt;</span>9.<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fraction&quot;</span>&gt;</span>6<span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>接下来，检查第二部影片的代码段，如下所示：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-main&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-content&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-info&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;name&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/films/341219&quot;</span> <span class="attr">title</span>=<span class="string">&quot;穿靴子的猫2&quot;</span> <span class="attr">data-act</span>=<span class="string">&quot;boarditem-click&quot;</span> <span class="attr">data-val</span>=<span class="string">&quot;&#123;movieId:341219&#125;&quot;</span>&gt;</span>穿靴子的猫2<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;star&quot;</span>&gt;</span>主演：安东尼奥·班德拉斯,萨尔玛·海耶克,哈维·吉兰<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;releasetime&quot;</span>&gt;</span>上映时间：2022-12-23<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-number score-num&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;score&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;integer&quot;</span>&gt;</span>9.<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fraction&quot;</span>&gt;</span>3<span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>对比可发现，每部影片的除了信息不同之外，它们的 HTML 结构是相同的。比如，每部影片都使用 <code>&lt;dd&gt;&lt;/dd&gt;</code> 标签包裹起来。</p>
<p>这里我们只检查了两部影片，在实际编写时，你可以多检查几部，从而确定它们的 HTML 结构是相同的。</p>
<hr>
<h3 id="实例：爬虫抓取百度贴吧数据"><a href="#实例：爬虫抓取百度贴吧数据" class="headerlink" title="实例：爬虫抓取百度贴吧数据"></a>实例：爬虫抓取百度贴吧数据</h3><p>有了上面的知识储备，这里来看一个 Python 爬虫实战案例：抓取百度贴吧信息（某贴吧下多个页面信息），并将其保存至本地。</p>
<p>案例说明：抓取 <a target="_blank" rel="noopener" href="https://tieba.baidu.com/">百度贴吧</a> 搜索关键词（Python）后检索到的 <strong>Python 吧</strong> 中前 5 个页面信息 &gt;&gt;&gt;&gt;</p>
<p>以下案例流程为 &gt;&gt;&gt; <strong>编写网络爬虫脚本的通用流程</strong>：</p>
<h4 id="预分析"><a href="#预分析" class="headerlink" title="预分析"></a>预分析</h4><p>在开始编写网络爬虫脚本之前，你需要对待爬取的页面进行如下分析：</p>
<p><strong>[1] &gt;&gt;&gt; 判断页面类型</strong></p>
<p><font color="red">分析方法</font> &gt;&gt;&gt; 如果页面中的所有数据信息都包含在其 HTML 文档中，那么当前待爬取的页面属于静态页面，而网页数据存在异步加载的页面为动态页面。</p>
<p>以案例为例，具体操作为：打开百度贴吧，搜索 <code>Python</code>，在出现的页面中复制任意一段信息，比如 <code>解决一切python问题</code>，然后点击右键选择查看源码（View Page Source），并使用 <code>Ctrl+F</code> 快捷键在源码页面搜索刚刚复制的数据。</p>
<p>故，可以判断出 &gt;&gt;&gt;&gt; 抓取的百度贴吧页面属于静态网页。</p>
<p><strong>[2] &gt;&gt;&gt; 分析 URL 变化规律</strong></p>
<p>接下来，你需要寻找待爬取百度贴吧页面的 URL 规律（用于后续的请求发送以获取带爬取页面的 HTML 信息）：</p>
<p>搜索 “Python” 后，此贴吧第一页的的 URL 如下所示：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://tieba.baidu.com/f?ie=utf-8&amp;kw=python&amp;fr=search</span><br></pre></td></tr></table></figure>

<p>点击<code>第二页</code>，其 URL 信息如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://tieba.baidu.com/f?kw=python&amp;ie=utf-8&amp;pn=50</span><br></pre></td></tr></table></figure>

<p>点击<code>第三页</code>，其 URL 信息如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://tieba.baidu.com/f?kw=python&amp;ie=utf-8&amp;pn=100</span><br></pre></td></tr></table></figure>

<p>重新点击<code>第一页</code>，其 URL 信息如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://tieba.baidu.com/f?kw=python&amp;ie=utf-8&amp;pn=0</span><br></pre></td></tr></table></figure>

<p>如果还不确定，你可以继续多浏览几页。你可以发现 URL 具有两个关键查询参数，分别是 <code>kw</code> 和 <code>pn</code>，并且 pn 参数具有规律性：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">第 N 页：pn = (n-1) * 50</span><br><span class="line"></span><br><span class="line"># 查询参数：</span><br><span class="line">pn = (pageNum - 1) * 50</span><br><span class="line"># 查询参数字典：</span><br><span class="line">params = &#123;</span><br><span class="line">    &quot;kw&quot;: &quot;python&quot;,</span><br><span class="line">    &quot;pn&quot;: &quot;str(pn)&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>故，百度贴吧页面访问 URL 规则可以简写为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://tieba.baidu.com/f?kw=python&amp;pn=(pageNum-1)*50</span><br></pre></td></tr></table></figure>

<p><strong>[3] &gt;&gt;&gt; 审查网页结构以及元素节点信息</strong></p>
<p>一般情况下，为了提取页面中的期望信息，在编写网络爬虫脚本前你还必须审查<font color="red"> 期望信息在 HTML 文档中的结构以及元素节点信息</font> 以确定内容提取的解析表达式。审查方法可参考上文，解析表达式可参见下文。</p>
<p>这里，因为我们抓取的是整个页面，故不需要进一步审查。</p>
<hr>
<h4 id="网络爬虫编写"><a href="#网络爬虫编写" class="headerlink" title="网络爬虫编写"></a>网络爬虫编写</h4><p>这里，以面向对象的编程设计思路，给出案例的爬虫脚本代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> fake_useragent</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义爬虫类：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TiebaSpider</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.url = <span class="string">&quot;https://tieba.baidu.com/f?&#123;&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 请求获取网页 HTML 信息（GET）：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">requestHTML</span>(<span class="params">self, url</span>):</span></span><br><span class="line">        <span class="comment"># 随机获取浏览器伪装 UA：</span></span><br><span class="line">        ua = fake_useragent.UserAgent()</span><br><span class="line">        ua_info = ua.random</span><br><span class="line">        <span class="comment"># print(&quot;User-Agent: &quot; + ua_info)</span></span><br><span class="line"></span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">&quot;User-Agent&quot;</span>: ua_info,</span><br><span class="line">            <span class="comment"># &quot;Accept&quot;: &quot;&quot;,</span></span><br><span class="line">            <span class="comment"># &quot;Cookie&quot;: &quot;&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 重构请求对象：</span></span><br><span class="line">        req = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">        <span class="comment"># 发送请求</span></span><br><span class="line">        response = urllib.request.urlopen(req)</span><br><span class="line">        <span class="comment"># print(&quot;Request Status Code:&quot;, response.status)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 响应的网页信息：</span></span><br><span class="line">        html = response.read().decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        <span class="comment"># html = response.read().decode(&quot;gbk&quot;, &quot;ignore&quot;)</span></span><br><span class="line">        <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 页面解析：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parseHTML</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> 后续介绍解析模块后进行完善</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 数据持久化：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dataSave</span>(<span class="params">self, filename, data</span>):</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write(data)</span><br><span class="line">        <span class="comment"># print(&quot;Data is stored&quot;)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 入口函数：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        name = <span class="built_in">input</span>(<span class="string">&quot;Input Tieba Name: &quot;</span>)</span><br><span class="line">        pnum_start = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;Input Start Page: &quot;</span>))</span><br><span class="line">        pnum_end = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&quot;Input Abort Page: &quot;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历所有待爬取页面：</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(pnum_start, pnum_end+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 构建 URL：</span></span><br><span class="line">            page_num = (page - <span class="number">1</span>) * <span class="number">50</span></span><br><span class="line">            params = &#123;</span><br><span class="line">                <span class="string">&quot;kw&quot;</span>: name,</span><br><span class="line">                <span class="string">&quot;pn&quot;</span>: <span class="built_in">str</span>(page_num)</span><br><span class="line">            &#125;</span><br><span class="line">            url = self.url.<span class="built_in">format</span>(urllib.parse.urlencode(params))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 请求获取网页 HTML 信息：</span></span><br><span class="line">            html = self.requestHTML(url)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 数据持久化：</span></span><br><span class="line">            filename = <span class="string">&quot;&#123;&#125;-&#123;&#125;p.html&quot;</span>.<span class="built_in">format</span>(name, page)</span><br><span class="line">            self.dataSave(filename, html)</span><br><span class="line">            print(<span class="string">&quot;Page %d was successfully crawled&quot;</span> % page)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每爬取一个页面随机休眠 1-2 秒：</span></span><br><span class="line">            time.sleep(random.randint(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 爬虫主程序入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    print(<span class="string">&quot;| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Start Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |&quot;</span>)</span><br><span class="line">    start = time.time()</span><br><span class="line">    spider = TiebaSpider()</span><br><span class="line">    spider.run()</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">&quot;Script Runtime:%.2f s&quot;</span> % (end - start))</span><br><span class="line">    print(<span class="string">&quot;| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Close Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>程序执行结果（数据文件保存在当前工作目录下）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Start Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |</span><br><span class="line">Input Tieba Name: Python</span><br><span class="line">Input Start Page: 1</span><br><span class="line">Input Abort Page: 3</span><br><span class="line">Page 1 was successfully crawled</span><br><span class="line">Page 2 was successfully crawled</span><br><span class="line">Page 3 was successfully crawled</span><br><span class="line">Script Runtime:16.51 s</span><br><span class="line">| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Close Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |</span><br></pre></td></tr></table></figure>

<p><strong>爬虫程序结构解析 &gt;&gt;&gt;</strong></p>
<p>用面向对象的方法编写爬虫程序时，逻辑结构较为固定，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">xxxSpider</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 常量定义</span></span><br><span class="line">       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">requestHTML</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 请求获取网页 HTML 信息（GET）</span></span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parseHTML</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 页面解析：Regular Expression/Xpath/Beautiful Soup 4，以提取期望数据</span></span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dataSave</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 将提取到的数据进行持久化：CSV/MySQL</span></span><br><span class="line">       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 主入口函数，控制脚本整体逻辑</span></span><br><span class="line">       </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    spider = xxxSpider()</span><br><span class="line">    spider.run()</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="爬虫随机休眠"><a href="#爬虫随机休眠" class="headerlink" title="爬虫随机休眠"></a>爬虫随机休眠</h4><p>爬虫程序访问网站会非常快，这与正常人类的点击行为非常不符。</p>
<p>因此，你可以通过设置随机休眠，来使爬虫程序更像是人类在访问网站，从而让网站不易察觉是爬虫访问网站：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每爬取一个页面爬虫随机休眠 1-2 秒：</span></span><br><span class="line">time.sleep(random.randint(<span class="number">1</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<p><font color="red">爬虫随机休眠代价</font> &gt;&gt;&gt;&gt; 影响程序的执行效率。</p>
<hr>
<h2 id="页面内容结构解析以提取有效信息"><a href="#页面内容结构解析以提取有效信息" class="headerlink" title="页面内容结构解析以提取有效信息"></a>页面内容结构解析以提取有效信息</h2><p>这一章节来看 &gt;&gt;&gt; <font color="red">如何根据网页内容结构以及元素节点信息，借助 HTML 对象解析工具（Regular Expression/Xpath/Beautiful Soup 4）以解析页面提取有效数据？！！</font></p>
<p>以 <a target="_blank" rel="noopener" href="https://www.maoyan.com/board">猫眼电影网</a> 榜单页面为例 &gt;&gt;&gt;&gt; 假设我们想要提取：榜单页面中每部电影的名称、主演、上映时间以及评分信息（页面中的特定内容）。</p>
<p>你可以先通过浏览器元素审查工具（Inspect）审查每部影片的 HTML 元素结构：</p>
<p><img src="https://s2.loli.net/2022/12/31/97YUVqSmG4sRaQt.png"></p>
<p>每一部影片的代码段都类似如下（以第一部为例）：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-main&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-content&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-info&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;name&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/films/1205&quot;</span> <span class="attr">title</span>=<span class="string">&quot;放牛班的春天&quot;</span> <span class="attr">data-act</span>=<span class="string">&quot;boarditem-click&quot;</span> <span class="attr">data-val</span>=<span class="string">&quot;&#123;movieId:1205&#125;&quot;</span>&gt;</span>放牛班的春天<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;star&quot;</span>&gt;</span>主演：热拉尔·朱诺,弗朗西斯·贝尔兰德,凯德·麦拉德<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;releasetime&quot;</span>&gt;</span>上映时间：2004-10-16<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-number score-num&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;score&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;integer&quot;</span>&gt;</span>9.<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fraction&quot;</span>&gt;</span>6<span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><font color="red">这就涉及到一个问题</font> &gt;&gt;&gt; 如何从页面的所有 HTML 对象信息中仅提取 &gt;&gt;&gt; 每部电影的名称、主演、上映时间以及评分？？？</p>
<p>你需要编写 &gt;&gt;&gt; <strong>可以匹配目标信息（数据）的 解析表达式</strong>，常见的解析表达式工具有：Regular Expression &amp;&amp; Xpath &amp;&amp; Beautiful Soup 4。</p>
<p>也就是说，<font color="red">解析表达式（Regular Expression/Xpath/BS 4）可以使得爬虫从整个页面的 HTML 对象信息中，抓取我们所关注的有效信息（数据），而不是整个 HTML。</font></p>
<hr>
<p>开始本章节的学习之前，你需要基本掌握正则表达式的基础语法，初学者请参见 [ &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2016/10/01/yi-wen-xue-hui-zheng-ze-biao-da-shi-yu-fa/">一文学会正则表达式语法</a> &lt;&lt;&lt;&lt;]。</p>
<p>以正则表达式（Regular Expression）实现解析表达式为例：</p>
<h3 id="Use-Built-in-Re-Lib"><a href="#Use-Built-in-Re-Lib" class="headerlink" title="Use Built-in Re Lib"></a>Use Built-in Re Lib</h3><p>Python 中内置的 <code>re</code> 模块，用于提供正则表达式支持。</p>
<p>也就是说，通过编写可以匹配 目标信息（或数据） 的正则表达式，然后通过 <code>re</code> 模块提供的文本（字符串）查找方法，可以轻松提取到有效信息（每部电影的名称、主演、上映时间以及评分）。</p>
<h4 id="Re-模块常用爬虫方法"><a href="#Re-模块常用爬虫方法" class="headerlink" title="Re 模块常用爬虫方法"></a>Re 模块常用爬虫方法</h4><p>网络爬虫中常用的 <code>re</code> 模块方法如下：</p>
<p><font color="red">1）re.findall()</font></p>
<p><code>re.findall()</code> 方法会根据正则表达式的文本匹配模式（pattern），来匹配目标字符串中内容。其语法格式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.findall(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>其中，参数 <code>pattern</code> 为正则表达式；<code>string</code> 参数为匹配的目标字符串；而 <code>flags</code> 表示功能标志位，可用来拓展正则表达式功能。</p>
<p>该函数的返回值是 pattern 匹配内容的列表。需要注意的是，如果正则表达式中含有一个分组，则返回分组所匹配内容字符串的列表（每个字符串元素都是一次成功的匹配）；如果含有多个分组，则返回一个元组列表（每个元组元素都是一次成功的匹配，可以包含多个分组内容）。</p>
<p><font color="red">2）re.split()</font></p>
<p><code>re.split()</code> 方法会根据正则匹配内容，来切割目标字符串，返回值是切割后的内容列表。其语法格式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(pattern, string, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p><font color="red">3）re.sub()</font></p>
<p><code>re.sub()</code> 方法会一个替换字符串（replace），来替换正则匹配到的内容，返回值是替换后的字符串。其语法格式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.sub(pattern, replace, string, maxcount, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>其中，参数 <code>replace</code> 为替换字符串；<code>maxcount</code> 参数为最多替换基础，默认为全替换；其它参数同上。</p>
<p><font color="red">4）re.match() &amp;&amp; re.search()</font></p>
<p>re 模块中还支持几乎所有编程语言都支持的 match() 和 search() 方法等等。</p>
<p><strong>关于 flags 功能标志位 &gt;&gt;&gt;</strong></p>
<p>功能标志位的作用是扩展正则表达的匹配功能。常用的 flag 如下所示：</p>
<table>
<thead>
<tr>
<th>缩写元字符</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>A</td>
<td>元字符只能匹配 ASCII 码。</td>
</tr>
<tr>
<td>I</td>
<td>匹配忽略字母大小写。</td>
</tr>
<tr>
<td>S</td>
<td>使得 <code>.</code> 元字符可以匹配换行符。</td>
</tr>
<tr>
<td>M</td>
<td>多行模式，使 <code>^</code> &amp;&amp; <code>$</code> 可以匹配每一行的开头和结尾位置。</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>注意：可以同时使用多个功能标志位（<code>|</code> 连接），比如 flags=re.I|re.S。</p>
<hr>
<h4 id="正则表达式对象方法"><a href="#正则表达式对象方法" class="headerlink" title="正则表达式对象方法"></a>正则表达式对象方法</h4><p>绝大部分重要的应用，总是会先将正则表达式编译为正则表达式对象，之后再进行操作，这可以为正则的使用提供一些其它特性。</p>
<p>你可以通过 <code>re</code> 模块提供的 <code>re.compile()</code> 方法来生成一个正则表达式对象，其语法格式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regex = re.<span class="built_in">compile</span>(pattern, flags=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>生成正则表达式对象之后，就可以调用其方法以及属性了。</p>
<p>正则表达式对象中也提供了上一小节中提到的所有方法（match、search、findall…），注意新的特性。例如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以 findall() 方法为例：</span></span><br><span class="line">regex = re.<span class="built_in">compile</span>(pattern, flags=<span class="number">0</span>)</span><br><span class="line">regex.findall(string, pos, endpos)</span><br></pre></td></tr></table></figure>

<p>其中，参数 <code>string</code> 为匹配的目标字符串；<code>pos</code> 参数为目标字符串的开始匹配位置；<code>endpos</code> 参数为目标字符串的结束匹配位置。</p>
<p>是不是感觉用法上更加灵活了？？？</p>
<hr>
<h4 id="适合-HTML-文档的正则规则"><a href="#适合-HTML-文档的正则规则" class="headerlink" title="适合 HTML 文档的正则规则"></a>适合 HTML 文档的正则规则</h4><p>首先，给出一个使用贪婪以及非贪婪模式来匹配 HTML 元素的实例，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">html = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;div&gt;&lt;p&gt;JavaScript Lesson&lt;/p&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;div&gt;&lt;p&gt;Hello JS.&lt;/p&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则表达式文本模式（贪婪模式）：</span></span><br><span class="line">pattern = <span class="string">&quot;&lt;div&gt;&lt;p&gt;.*&lt;/p&gt;&lt;/div&gt;&quot;</span></span><br><span class="line"><span class="comment"># 构建正则表达式对象（flags=re.S）：</span></span><br><span class="line">regRex = re.<span class="built_in">compile</span>(pattern, flags=re.S)</span><br><span class="line"><span class="comment"># 匹配 HTMLX 元素，提取信息：</span></span><br><span class="line">res_list = regRex.findall(html)</span><br><span class="line">print(res_list)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># [&#x27;&lt;div&gt;&lt;p&gt;JavaScript Lesson&lt;/p&gt;&lt;/div&gt;\n&lt;div&gt;&lt;p&gt;Hello JS.&lt;/p&gt;&lt;/div&gt;&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则表达式文本模式（非贪婪模式 &amp;&amp; flags=re.S）：</span></span><br><span class="line">regRex1 = re.<span class="built_in">compile</span>(<span class="string">&quot;&lt;div&gt;&lt;p&gt;.*?&lt;/p&gt;&lt;/div&gt;&quot;</span>, flags=re.S)</span><br><span class="line">res_list1 = regRex1.findall(html)</span><br><span class="line">print(res_list1)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># [&#x27;&lt;div&gt;&lt;p&gt;JavaScript Lesson&lt;/p&gt;&lt;/div&gt;&#x27;, &#x27;&lt;div&gt;&lt;p&gt;Hello JS.&lt;/p&gt;&lt;/div&gt;&#x27;]</span></span><br></pre></td></tr></table></figure>

<p>从输出结果中可以看出，<strong>非贪婪模式要更加适合提取 HTML 元素节点中信息</strong>。</p>
<p>我们期望的是，仅提取出 <code>JavaScript Lesson</code> &amp;&amp; <code>Hello JS.</code> 信息就可以了，怎么办？！！</p>
<p>肯定有人能想到 <strong><code>findall()</code> 方法中正则表达式包含分组</strong> 的情况，其仅返回分组所匹配内容的列表。上述实例修改为如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">html = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;div&gt;&lt;p&gt;JavaScript Lesson&lt;/p&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;div&gt;&lt;p&gt;Hello JS.&lt;/p&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则表达式文本模式（非贪婪模式 &amp;&amp; flags=re.S &amp;&amp; 分组）：</span></span><br><span class="line">regRex1 = re.<span class="built_in">compile</span>(<span class="string">&quot;&lt;div&gt;&lt;p&gt;(.*?)&lt;/p&gt;&lt;/div&gt;&quot;</span>, flags=re.S)</span><br><span class="line">res_list1 = regRex1.findall(html)</span><br><span class="line">print(res_list1)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># [&#x27;JavaScript Lesson&#x27;, &#x27;Hello JS.&#x27;]</span></span><br></pre></td></tr></table></figure>

<p>OK~~~</p>
<hr>
<h4 id="深入解读-Re-库"><a href="#深入解读-Re-库" class="headerlink" title="深入解读 Re 库"></a>深入解读 Re 库</h4><p>关于 Python <strong>re</strong> 正则表达式模块的详细用法可参见 Python 博文系列 [ &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2018/01/04/python-ji-chu-yu-fa/">Python 教程</a> &lt;&lt;&lt;&lt;]。</p>
<p><strong>| &gt;&gt;&gt; ============================================== Split Line =========================================== &lt;&lt;&lt; |</strong></p>
<p>事实上，正则表达式（Regular Expression）的语法较为 “复杂”，初学者学习成本较高。</p>
<p>后续，我们会引入第三方的、方便快捷的 Beautiful Soup 4（BS 4）&amp;&amp; lxml 库，以更简单、便捷的方式实现解析表达式。</p>
<p>关于 BS 4 &amp;&amp; lxml 库的使用说明可以参见博文系列中 [ &gt;&gt;&gt;&gt; <a href="https://www.orangeshare.cn/2020/01/22/web-crawler-jiao-cheng-zhi-wang-luo-pa-chong-gong-ju-ku/">Web Crawler 教程之网络爬虫工具库</a> &lt;&lt;&lt;&lt;] 中解析库部分的说明。</p>
<hr>
<h3 id="网页有效信息提取"><a href="#网页有效信息提取" class="headerlink" title="网页有效信息提取"></a>网页有效信息提取</h3><p>再回过头来思考本章节开头的问题， 如何使用正则表达式来解析每部影片的代码段（类似如下）以提取 &gt;&gt;&gt; 每部电影的名称、主演、上映时间以及评分？？？</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-main&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-content&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-info&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;name&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/films/1205&quot;</span> <span class="attr">title</span>=<span class="string">&quot;放牛班的春天&quot;</span> <span class="attr">data-act</span>=<span class="string">&quot;boarditem-click&quot;</span> <span class="attr">data-val</span>=<span class="string">&quot;&#123;movieId:1205&#125;&quot;</span>&gt;</span>放牛班的春天<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;star&quot;</span>&gt;</span>主演：热拉尔·朱诺,弗朗西斯·贝尔兰德,凯德·麦拉德<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;releasetime&quot;</span>&gt;</span>上映时间：2004-10-16<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-number score-num&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;score&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;integer&quot;</span>&gt;</span>9.<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fraction&quot;</span>&gt;</span>6<span class="tag">&lt;/<span class="name">i</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>实战演练（提取连续两部影片的名称、主演、上映时间以及评分信息）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line">html = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;div class=&quot;board-item-main&quot;&gt;</span></span><br><span class="line"><span class="string">    &lt;div class=&quot;board-item-content&quot;&gt;</span></span><br><span class="line"><span class="string">        &lt;div class=&quot;movie-item-info&quot;&gt;</span></span><br><span class="line"><span class="string">            &lt;p class=&quot;name&quot;&gt;&lt;a href=&quot;/films/1205&quot; title=&quot;放牛班的春天&quot; data-act=&quot;boarditem-click&quot; data-val=&quot;&#123;movieId:1205&#125;&quot;&gt;放牛班的春天&lt;/a&gt;</span></span><br><span class="line"><span class="string">            &lt;/p&gt;</span></span><br><span class="line"><span class="string">            &lt;p class=&quot;star&quot;&gt;主演：热拉尔·朱诺,弗朗西斯·贝尔兰德,凯德·麦拉德&lt;/p&gt;</span></span><br><span class="line"><span class="string">            &lt;p class=&quot;releasetime&quot;&gt;上映时间：2004-10-16&lt;/p&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &lt;div class=&quot;movie-item-number score-num&quot;&gt;</span></span><br><span class="line"><span class="string">            &lt;p class=&quot;score&quot;&gt;&lt;i class=&quot;integer&quot;&gt;9.&lt;/i&gt;&lt;i class=&quot;fraction&quot;&gt;6&lt;/i&gt;</span></span><br><span class="line"><span class="string">            &lt;/p&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;div class=&quot;board-item-main&quot;&gt;</span></span><br><span class="line"><span class="string">    &lt;div class=&quot;board-item-content&quot;&gt;</span></span><br><span class="line"><span class="string">        &lt;div class=&quot;movie-item-info&quot;&gt;</span></span><br><span class="line"><span class="string">            &lt;p class=&quot;name&quot;&gt;&lt;a href=&quot;/films/243&quot; title=&quot;阿凡达&quot; data-act=&quot;boarditem-click&quot; data-val=&quot;&#123;movieId:243&#125;&quot;&gt;阿凡达&lt;/a&gt;</span></span><br><span class="line"><span class="string">            &lt;/p&gt;</span></span><br><span class="line"><span class="string">            &lt;p class=&quot;star&quot;&gt;主演：萨姆·沃辛顿,佐伊·索尔达娜,米歇尔·罗德里格兹&lt;/p&gt;</span></span><br><span class="line"><span class="string">            &lt;p class=&quot;releasetime&quot;&gt;上映时间：2010-01-04&lt;/p&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">        &lt;div class=&quot;movie-item-number score-num&quot;&gt;</span></span><br><span class="line"><span class="string">            &lt;p class=&quot;score&quot;&gt;&lt;i class=&quot;integer&quot;&gt;9.&lt;/i&gt;&lt;i class=&quot;fraction&quot;&gt;4&lt;/i&gt;</span></span><br><span class="line"><span class="string">            &lt;/p&gt;</span></span><br><span class="line"><span class="string">        &lt;/div&gt;</span></span><br><span class="line"><span class="string">    &lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;/div&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则表达式文本模式（贪婪模式）：</span></span><br><span class="line">pattern = <span class="string">r&#x27;&lt;div.*?title=&quot;(.*?)&quot;.*?star&quot;&gt;主演：(.*?)&lt;/p.*?time&quot;&gt;上映时间：(.*?)&lt;/p.*?integer&quot;&gt;(.*?)&lt;/i.*?fraction&quot;&gt;(.*?)&lt;/i.*?/div&gt;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 正则表达式文本模式（非贪婪模式 &amp;&amp; flags=re.S）：</span></span><br><span class="line">regRex = re.<span class="built_in">compile</span>(pattern, flags=re.S)</span><br><span class="line">res_list = regRex.findall(html)</span><br><span class="line">print(res_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 整理数据格式并输出</span></span><br><span class="line"><span class="keyword">if</span> res_list:</span><br><span class="line">    print(<span class="number">20</span>*<span class="string">&quot;*&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> res_list:</span><br><span class="line">        print(<span class="string">&quot;影片名称：&quot;</span>, item[<span class="number">0</span>])</span><br><span class="line">        print(<span class="string">&quot;影片主演：&quot;</span>, item[<span class="number">1</span>])</span><br><span class="line">        print(<span class="string">&quot;上映时间：&quot;</span>, item[<span class="number">2</span>])</span><br><span class="line">        print(<span class="string">&quot;影片评分：&quot;</span>, item[<span class="number">3</span>] + item[<span class="number">4</span>])</span><br><span class="line">        print(<span class="number">20</span>*<span class="string">&quot;*&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[(&#x27;放牛班的春天&#x27;, &#x27;热拉尔·朱诺,弗朗西斯·贝尔兰德,凯德·麦拉德&#x27;, &#x27;2004-10-16&#x27;, &#x27;9.&#x27;, &#x27;6&#x27;), (&#x27;阿凡达&#x27;, &#x27;萨姆·沃辛顿,佐伊·索尔达娜,米歇尔·罗德里格兹&#x27;, &#x27;2010-01-04&#x27;, &#x27;9.&#x27;, &#x27;4&#x27;)]</span><br><span class="line">********************</span><br><span class="line">影片名称： 放牛班的春天</span><br><span class="line">影片主演： 热拉尔·朱诺,弗朗西斯·贝尔兰德,凯德·麦拉德</span><br><span class="line">上映时间： 2004-10-16</span><br><span class="line">影片评分： 9.6</span><br><span class="line">********************</span><br><span class="line">影片名称： 阿凡达</span><br><span class="line">影片主演： 萨姆·沃辛顿,佐伊·索尔达娜,米歇尔·罗德里格兹</span><br><span class="line">上映时间： 2010-01-04</span><br><span class="line">影片评分： 9.4</span><br><span class="line">********************</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="持久化存储页面有效信息"><a href="#持久化存储页面有效信息" class="headerlink" title="持久化存储页面有效信息"></a>持久化存储页面有效信息</h2><p>获取到页面中的有效信息后，这一章节来看 &gt;&gt;&gt; <font color="red"> 如何将这些提取到的有效的信息（数据）存储下来？！！</font></p>
<p>通过爬虫脚本将有效信息（数据）抓取下来，然后将数据存储在本地文件，或数据库中，这个过程就称为 &gt;&gt;&gt; <strong>数据持久化存储</strong>。</p>
<p>两种常见的数据持久化存储方式：</p>
<ul>
<li>本地文件存储</li>
<li>数据库存储</li>
</ul>
<h3 id="本地文件存储"><a href="#本地文件存储" class="headerlink" title="本地文件存储"></a>本地文件存储</h3><p>CSV 是电子表格（如 Excel）和数据库中最常见的输入、输出文件格式。</p>
<p>CSV 文件，又称为 <font color="red">逗号分隔值文件</font>，适用于存储表格数据（数据或字符）。</p>
<p>Python 中内置的 <code>csv</code> 模块，用来提供 CSV 格式文件的读、写操作。</p>
<h4 id="写入-CSV-文件"><a href="#写入-CSV-文件" class="headerlink" title="写入 CSV 文件"></a>写入 CSV 文件</h4><p>我们可以，通过 Python <code>csv</code> 模块提供的文件读写类中的方法，来向 CSV 文件中写入数据。</p>
<p>CSV 模块中，常用的写文件类如下：</p>
<p><strong>[1] &gt;&gt;&gt; csv.writer</strong></p>
<p>csv 模块中的 writer 类，可用于向 CSV 文件写入序列化的数据。构建 writer 类的语法格式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">csv.writer(csvfile, dialect=<span class="string">&#x27;excel&#x27;</span>, **fmtparams)</span><br></pre></td></tr></table></figure>

<p>其中，参数 <code>csvfile</code> 必须是可迭代（Iterator）对象，例如 文件对象（file）或列表（list）等；参数 <code>dialect</code> 指编码风格（方言），默认为 Excel 的风格，也就是使用都好（<code>,</code>）分隔；<code>**fmtparams</code> 格式化参数，用来覆盖之前 dialect 参数指定的编码风格。</p>
<p>关于 <code>**fmtparams</code> 格式化参数，假如你不想使用 Excel 风格，你可以使用如下格式化参数进行覆盖：</p>
<ul>
<li><strong>delimiter</strong> &gt;&gt;&gt; 用来指定写入行内多个数据项的分隔符；</li>
<li><strong>quotechar</strong> &gt;&gt;&gt; 用来指定引用符，如果数据项内本身包含分隔字符时，为了排除歧义，可以将当前数据项使用引用符引起来表示完整的一个数据项。</li>
</ul>
<p>逐行写入内容的实例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 csv 模块</span></span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 操作文件对象时，需要添加 newline 参数逐行写入，否则会出现空行现象</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;test.csv&quot;</span>, mode=<span class="string">&quot;w&quot;</span>, newline=<span class="string">&quot;&quot;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    <span class="comment"># 构建文件读写对象 writer：</span></span><br><span class="line">    obj_CSVWrite = csv.writer(csvfile, delimiter=<span class="string">&quot; &quot;</span>, quotechar=<span class="string">&quot;/&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 单行写入方法，列表格式传入数据：</span></span><br><span class="line">    obj_CSVWrite.writerow([<span class="string">&quot;Hello&quot;</span>]*<span class="number">5</span> + [<span class="string">&quot;JS&quot;</span>])</span><br><span class="line">    obj_CSVWrite.writerow([<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;JS&quot;</span>, <span class="string">&quot;Welcome to JS World&quot;</span>])</span><br></pre></td></tr></table></figure>

<p>生成文件 <code>test.csv</code> 内容如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hello Hello Hello Hello Hello JS</span><br><span class="line">Hello JS /Welcome to JS World/</span><br></pre></td></tr></table></figure>

<p>可见文件中，使用 <code>writerow()</code> 方法逐行写入，行内多个数据项以空格（<code>delimiter=&quot; &quot;</code>）分隔，对于本身包含分隔符的数据项会使用斜杠符（<code>quotechar=&quot;/&quot;</code>）包围以引用。</p>
<p><strong>如何同时写入多行内容 &gt;&gt;&gt;</strong></p>
<p>如果想同时写入多行数据，需要使用 <code>writerrows()</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 操作文件对象时，需要添加 newline 参数逐行写入，否则会出现空行现象</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;test.csv&quot;</span>, mode=<span class="string">&quot;w&quot;</span>, newline=<span class="string">&quot;&quot;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    <span class="comment"># 构建文件读写对象 writer：</span></span><br><span class="line">    obj_CSVWrite = csv.writer(csvfile)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 同时多行写入方法：元组元素列表格式传入数据：</span></span><br><span class="line">    <span class="comment"># 列表中的每个元组元素为一行数据</span></span><br><span class="line">    obj_CSVWrite.writerows([(<span class="string">&quot;Javascript&quot;</span>, <span class="string">&quot;Course&quot;</span>), (<span class="string">&quot;Spider&quot;</span>, <span class="string">&quot;Course&quot;</span>)])</span><br></pre></td></tr></table></figure>

<p>生成文件 <code>test.csv</code> 内容如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Javascript,Course</span><br><span class="line">Spider,Course</span><br></pre></td></tr></table></figure>

<hr>
<p><strong>[2] &gt;&gt;&gt; csv.DictWriter()</strong></p>
<p>类似于 writer 读写类，可以使用 DictWriter 类向 CSV 文件中以字典（Dict）的形式写入数据。其构建语法格式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">csv.writer(csvfile, fieldname, **fmtparams)</span><br></pre></td></tr></table></figure>

<p>其中，参数 <code>fieldname</code> 可用于指定表头（表格字段名），对应字典的 Key；其它参数类似 writer。</p>
<p>实例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 操作文件对象时，需要添加 newline 参数逐行写入，否则会出现空行现象</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;test1.csv&quot;</span>, mode=<span class="string">&quot;w&quot;</span>, newline=<span class="string">&quot;&quot;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">    <span class="comment"># 构建表头（字段名称）：</span></span><br><span class="line">    fieldname = [<span class="string">&quot;first_name&quot;</span>, <span class="string">&quot;last_name&quot;</span>]</span><br><span class="line">    <span class="comment"># 构建文件读写对象 DictWriter：</span></span><br><span class="line">    obj_CSVWrite = csv.DictWriter(csvfile, fieldnames=fieldname, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 写入字段名称，作为表头：</span></span><br><span class="line">    obj_CSVWrite.writeheader()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 单行写入方法，字典格式传入数据：</span></span><br><span class="line">    obj_CSVWrite.writerow(&#123;<span class="string">&quot;first_name&quot;</span>: <span class="string">&quot;Baked&quot;</span>, <span class="string">&quot;last_name&quot;</span>: <span class="string">&quot;Beans&quot;</span>&#125;)</span><br><span class="line">    obj_CSVWrite.writerow(&#123;<span class="string">&quot;first_name&quot;</span>: <span class="string">&quot;Black&quot;</span>, <span class="string">&quot;last_name&quot;</span>: <span class="string">&quot;John&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 同时多行写入方法：字典元素列表格式传入数据：</span></span><br><span class="line">    <span class="comment"># 列表中的每个字典元素为一行数据</span></span><br><span class="line">    obj_CSVWrite.writerows([&#123;<span class="string">&#x27;first_name&#x27;</span>: <span class="string">&#x27;Baked&#x27;</span>, <span class="string">&#x27;last_name&#x27;</span>: <span class="string">&#x27;Beans&#x27;</span>&#125;,&#123;<span class="string">&#x27;first_name&#x27;</span>: <span class="string">&#x27;Lovely&#x27;</span>, <span class="string">&#x27;last_name&#x27;</span>: <span class="string">&#x27;Spam&#x27;</span>&#125;])</span><br></pre></td></tr></table></figure>

<p>生成文件 <code>test1.csv</code> 内容如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">first_name,last_name</span><br><span class="line">Baked,Beans</span><br><span class="line">Black,John</span><br><span class="line">Baked,Beans</span><br><span class="line">Lovely,Spam</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="读取-CSV-文件"><a href="#读取-CSV-文件" class="headerlink" title="读取 CSV 文件"></a>读取 CSV 文件</h4><p>同理，通过 Python csv 模块提供的文件读写类中的方法，来从 CSV 文件中读取数据。</p>
<p>CSV 模块中，常用的读文件类如下：</p>
<p><strong>[1] &gt;&gt;&gt; csv.reader</strong></p>
<p>csv 模块中的 reader 类，可用于从 CSV 文件中读取数据。构建 reader 类的语法格式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">csv.reader(csvfile, dialect=<span class="string">&#x27;excel&#x27;</span>, **fmtparams)</span><br></pre></td></tr></table></figure>

<p>读取文件 <code>test.csv</code> 内容实例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 操作文件对象时，需要添加 newline 参数逐行写入，否则会出现空行现象</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;test.csv&quot;</span>, mode=<span class="string">&quot;r&quot;</span>, newline=<span class="string">&quot;&quot;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建文件读写对象 reader：</span></span><br><span class="line">    obj_CSVReader = csv.reader(csvfile, delimiter=<span class="string">&quot; &quot;</span>, quotechar=<span class="string">&quot;/&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出读取的每行内容：</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> obj_CSVReader:</span><br><span class="line">        <span class="comment"># 输出行内的数据项列表：</span></span><br><span class="line">        print(row)</span><br></pre></td></tr></table></figure>

<p><strong>[2] &gt;&gt;&gt; csv.DictReader</strong></p>
<p>类似于 csv 模块中的 reader 类，DictReader 类可用于从 CSV 文件中以字典的形式读取数据。构建 reader 类的语法格式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">csv.DictReader(csvfile, filedname, **fmtparams)</span><br></pre></td></tr></table></figure>

<p>读取文件 <code>test1.csv</code> 内容实例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 操作文件对象时，需要添加 newline 参数逐行写入，否则会出现空行现象</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;test1.csv&quot;</span>, mode=<span class="string">&quot;r&quot;</span>, newline=<span class="string">&quot;&quot;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建文件读写对象 reader：</span></span><br><span class="line">    obj_CSVReader = csv.DictReader(csvfile, delimiter=<span class="string">&quot;,&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出读取的每行内容：</span></span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> obj_CSVReader:</span><br><span class="line">        print(row[<span class="string">&quot;first_name&quot;</span>], row[<span class="string">&quot;last_name&quot;</span>])</span><br></pre></td></tr></table></figure>

<p>输出结果如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Baked Beans</span><br><span class="line">Black John</span><br><span class="line">Baked Beans</span><br><span class="line">Lovely Spam</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="实例：抓取猫影电影排行榜"><a href="#实例：抓取猫影电影排行榜" class="headerlink" title="实例：抓取猫影电影排行榜"></a>实例：抓取猫影电影排行榜</h3><p>有了上面的知识储备，我们来演示一个完整的 Python 爬虫实战案例：抓取 <a target="_blank" rel="noopener" href="https://www.maoyan.com/board/4">猫眼电影网 TOP100 排行榜</a> 中的影片信息，包括电影名称、上映时间、主演信息以及电影评分。</p>
<p>以下案例流程为 &gt;&gt;&gt; <strong>编写网络爬虫脚本的通用流程</strong>：</p>
<p><strong>[1] &gt;&gt;&gt; 预分析</strong></p>
<p>在开始编写网络爬虫脚本之前，你需要对待爬取的排行榜页面进行如下分析：</p>
<p><strong>1.1】 &gt;&gt;&gt; 判断页面类型</strong></p>
<p>点击右键查看页面源码，确定要抓取的数据是否存在于页面内。</p>
<p>以排行榜为例，具体操作为：猫眼电影网 TOP100 排行榜，在出现的页面中复制任意一部影片的信息，比如 <font color="red">肖申克的救赎</font>，然后点击右键选择查看源码（View Page Source），并使用 Ctrl+F 快捷键在源码页面搜索刚刚复制的影片名称。</p>
<p>故，可以判断出 &gt;&gt;&gt;&gt; 抓取的百度贴吧页面属于静态网页。</p>
<p><strong>1.2】 &gt;&gt;&gt; 分析 URL 变化规律</strong></p>
<p>接下来，你需要寻找待爬取TOP100 排行榜页面的 URL 规律（用于后续的请求发送以获取带爬取页面的 HTML 信息）：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">第一页 &gt;&gt;&gt; https://www.maoyan.com/board/4?offset=0</span><br><span class="line">第二页 &gt;&gt;&gt; https://www.maoyan.com/board/4?offset=10</span><br><span class="line">第三页 &gt;&gt;&gt; https://www.maoyan.com/board/4?offset=20</span><br><span class="line">...</span><br><span class="line">第 n 页 &gt;&gt;&gt; https://www.maoyan.com/board/4?offset=(n-1)*10</span><br></pre></td></tr></table></figure>

<p><strong>1.3】 &gt;&gt;&gt; 审查网页结构以及元素节点信息以确定解析表达式</strong></p>
<p>审查 期望信息在 HTML 文档中的结构以及元素节点信息 以确定内容提取的解析表达式：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-main&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;board-item-content&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-info&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;name&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/films/1200486&quot;</span> <span class="attr">title</span>=<span class="string">&quot;我不是药神&quot;</span> <span class="attr">data-act</span>=<span class="string">&quot;boarditem-click&quot;</span> <span class="attr">data-val</span>=<span class="string">&quot;&#123;movieId:1200486&#125;&quot;</span>&gt;</span>我不是药神<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;star&quot;</span>&gt;</span>主演：徐峥,周一围,王传君<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;releasetime&quot;</span>&gt;</span>上映时间：2018-07-05<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;movie-item-number score-num&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;score&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;integer&quot;</span>&gt;</span>9.<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;fraction&quot;</span>&gt;</span>6<span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>使用 Chrome 开发者调试工具来精准定位要抓取信息的元素结构。之所以这样做，是因为这能避免正则表达式的冗余，提高编写正则表达式的速度。最终正则表达式如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;div class=&quot;board-item-main&quot;&gt;.*?title=&quot;(.*?)&quot;.*?class=&quot;star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p&gt;.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;</span><br></pre></td></tr></table></figure>

<p>正则表达式中，将需要提取的信息使用 <code>(.*?)</code> 代替，而不需要的内容（包括元素标签）使用 <code>.*?</code> 代替。</p>
<hr>
<p><strong>[2] &gt;&gt;&gt; 网络爬虫编写</strong></p>
<p>这里，以面向对象的编程设计思路，给出案例的爬虫脚本代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> fake_useragent</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义爬虫类：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaoyanSpider</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.url = <span class="string">&quot;https://www.maoyan.com/board/4?&#123;&#125;&quot;</span></span><br><span class="line">        <span class="comment"># 计数器</span></span><br><span class="line">        self.counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 请求获取网页 HTML 信息（GET）：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">requestHTML</span>(<span class="params">self, url</span>):</span></span><br><span class="line">        <span class="comment"># 随机获取浏览器伪装 UA：</span></span><br><span class="line">        ua = fake_useragent.UserAgent()</span><br><span class="line">        <span class="comment"># ua_info = ua.random</span></span><br><span class="line">        ua_info = ua.edge</span><br><span class="line">        <span class="comment"># print(&quot;User-Agent: &quot; + ua_info)</span></span><br><span class="line"></span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">&quot;User-Agent&quot;</span>: ua_info,</span><br><span class="line">            <span class="comment"># &quot;Accept&quot;: &quot;*/*&quot;,</span></span><br><span class="line">            <span class="string">&quot;Cookie&quot;</span>: <span class="string">&quot;你本机浏览器的 Cookie&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># 重构请求对象：</span></span><br><span class="line">        req = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">        <span class="comment"># 发送请求</span></span><br><span class="line">        response = urllib.request.urlopen(req)</span><br><span class="line">        <span class="comment"># print(&quot;Request Status Code:&quot;, response.status)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 响应的网页信息：</span></span><br><span class="line">        html = response.read().decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        <span class="comment"># html = response.read().decode(&quot;gbk&quot;, &quot;ignore&quot;)</span></span><br><span class="line">        <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 页面解析：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parseHTML</span>(<span class="params">self, pattern, html</span>):</span></span><br><span class="line">        <span class="comment"># 基于 Pattern 构建正则表达式对象（非贪婪模式 &amp;&amp; flags=re.S）：</span></span><br><span class="line">        regExp_Obj = re.<span class="built_in">compile</span>(pattern=pattern, flags=re.S)</span><br><span class="line">        <span class="comment"># 查找匹配内容：</span></span><br><span class="line">        find_res = regExp_Obj.findall(html)</span><br><span class="line">        <span class="comment"># 返回一个元组元素列表（每个元组元素都是一次成功的匹配，可以包含多个分组内容）：</span></span><br><span class="line">        <span class="keyword">return</span> find_res</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 数据持久化：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dataSave</span>(<span class="params">self, filename, validData</span>):</span></span><br><span class="line">        <span class="comment"># 操作文件对象时，需要添加 newline 参数逐行写入，否则会出现空行现象</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&quot;a&quot;</span>, newline=<span class="string">&quot;&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">            <span class="comment"># 构建文件读写对象 writer：</span></span><br><span class="line">            csvWriter_Obj = csv.writer(csvfile)</span><br><span class="line">            <span class="comment"># 数据处理：</span></span><br><span class="line">            <span class="keyword">if</span> validData:</span><br><span class="line">                <span class="keyword">for</span> record <span class="keyword">in</span> validData:</span><br><span class="line">                    video_name = record[<span class="number">0</span>].strip()</span><br><span class="line">                    video_actor = record[<span class="number">1</span>].strip()[<span class="number">3</span>:]</span><br><span class="line">                    video_time = record[<span class="number">2</span>].strip()[<span class="number">5</span>:]</span><br><span class="line">                    video_score = record[<span class="number">3</span>] + record[<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">                    lineData = [video_name, video_time, video_actor, video_score]</span><br><span class="line">                    csvWriter_Obj.writerow(lineData)</span><br><span class="line">                    <span class="comment"># 打印第一页影片信息</span></span><br><span class="line">                    <span class="keyword">if</span> (self.counter &lt; <span class="number">10</span>):</span><br><span class="line">                        print(video_name, video_time, video_actor, video_score)</span><br><span class="line">                        self.counter = self.counter + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">&quot;Request Failed&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 入口函数：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        pnum_start = <span class="number">1</span></span><br><span class="line">        pnum_end = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历所有待爬取页面：</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(pnum_start, pnum_end+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 构建 URL：</span></span><br><span class="line">            page_num = (page - <span class="number">1</span>) * <span class="number">10</span></span><br><span class="line">            params = &#123;</span><br><span class="line">                <span class="string">&quot;offset&quot;</span>: page_num</span><br><span class="line">            &#125;</span><br><span class="line">            url = self.url.<span class="built_in">format</span>(urllib.parse.urlencode(params))</span><br><span class="line">            <span class="comment"># print(&quot;Request URL: &quot;, url)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 请求获取网页 HTML 信息：</span></span><br><span class="line">            html = self.requestHTML(url)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 页面解析以提取有效数：</span></span><br><span class="line">            <span class="comment"># 正则表达式文本模式：</span></span><br><span class="line">            pattern = <span class="string">&#x27;&lt;div class=&quot;board-item-main&quot;&gt;.*?title=&quot;(.*?)&quot;.*?class=&quot;star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p&gt;.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;&#x27;</span></span><br><span class="line">            validData_list = self.parseHTML(pattern, html)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 数据持久化存储：</span></span><br><span class="line">            filename = <span class="string">&quot;maoyanTop100.csv&quot;</span></span><br><span class="line">            self.dataSave(filename, validData_list)</span><br><span class="line">            print(<span class="string">&quot;Page %d was successfully crawled&quot;</span> % page)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每爬取一个页面随机休眠 1-2 秒：</span></span><br><span class="line">            time.sleep(random.randint(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 爬虫主程序入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    print(<span class="string">&quot;| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Start Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |&quot;</span>)</span><br><span class="line">    start = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 捕捉异常：</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spider = MaoyanSpider()</span><br><span class="line">        spider.run()</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">&quot;Error: &quot;</span>, e)</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">&quot;Script Runtime:%.2f s&quot;</span> % (end - start))</span><br><span class="line">    print(<span class="string">&quot;| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Close Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>输出如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Start Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |</span><br><span class="line">我不是药神 2018-07-05 徐峥,周一围,王传君 9.6</span><br><span class="line">肖申克的救赎 1994-09-10(加拿大) 蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿 9.5</span><br><span class="line">海上钢琴师 2019-11-15 蒂姆·罗斯,比尔·努恩 ,克兰伦斯·威廉姆斯三世 9.3</span><br><span class="line">绿皮书 2019-03-01 维果·莫腾森,马赫沙拉·阿里,琳达·卡德里尼 9.5</span><br><span class="line">霸王别姬 1993-07-26 张国荣,张丰毅,巩俐 9.4</span><br><span class="line">美丽人生 2020-01-03 罗伯托·贝尼尼,朱斯蒂诺·杜拉诺,赛尔乔·比尼·布斯特里克 9.3</span><br><span class="line">小偷家族 2018-08-03 中川雅也,安藤樱,松冈茉优 8.1</span><br><span class="line">这个杀手不太冷 1994-09-14(法国) 让·雷诺,加里·奥德曼,娜塔莉·波特曼 9.4</span><br><span class="line">哪吒之魔童降世 2019-07-26 吕艳婷,囧森瑟夫,瀚墨 9.6</span><br><span class="line">怦然心动 2010-07-26(美国) 玛德琳·卡罗尔,卡兰·麦克奥利菲,艾丹·奎因 8.9</span><br><span class="line">Page 1 was successfully crawled</span><br><span class="line">Page 2 was successfully crawled</span><br><span class="line">Page 3 was successfully crawled</span><br><span class="line">Page 4 was successfully crawled</span><br><span class="line">Page 5 was successfully crawled</span><br><span class="line">Page 6 was successfully crawled</span><br><span class="line">Page 7 was successfully crawled</span><br><span class="line">Page 8 was successfully crawled</span><br><span class="line">Page 9 was successfully crawled</span><br><span class="line">Page 10 was successfully crawled</span><br><span class="line">Script Runtime:19.87 s</span><br><span class="line">| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Close Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |</span><br></pre></td></tr></table></figure>

<p>同时，查看当前工作目录下的生成的数据存储文件 <code>maoyanTop100.csv</code>，可以看到抓取到的 100 条影片数据。</p>
<hr>
<p>开始之前，相信你已经掌握了 SQL 语言的基本语法 &gt;&gt;&gt;</p>
<h3 id="数据库存储"><a href="#数据库存储" class="headerlink" title="数据库存储"></a>数据库存储</h3><p>上面我们将提取到的网页有效数据存储到了本地 CSV 格式的文件中，这里来看如何将有效数据存储至 MySQL 数据库？！！</p>
<p>Python 第三方的 <code>pymysql</code> 模块，用于提供 Python 连接，以及操作 MySQL 数据库。安装方法如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="库以及存储数据表准备"><a href="#库以及存储数据表准备" class="headerlink" title="库以及存储数据表准备"></a>库以及存储数据表准备</h4><p>首先，你应该确保你安装有可供测试、使用的 MySQL 数据库。然后，在 DOS 下进行如下操作以构建用于存储有效数据的库以及数据表：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># 1. 开启 MySQL 服务（未开启时）：</span><br><span class="line">&gt; net start mysql</span><br><span class="line">MySQL 服务正在启动 .</span><br><span class="line">MySQL 服务已经启动成功。</span><br><span class="line"></span><br><span class="line"># 2. 连接 MySQL 数据库：</span><br><span class="line">&gt; mysql -h 127.0.0.1 -u root -padmin</span><br><span class="line">mysql&gt;</span><br><span class="line"></span><br><span class="line"># 3. 构建库：</span><br><span class="line">mysql&gt; CREATE DATABASE maoyandb CHARSET UTF8;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"># 4. 切换库：</span><br><span class="line">mysql&gt; USE maoyandb</span><br><span class="line">Database changed</span><br><span class="line"></span><br><span class="line"># 5. 构建数据表</span><br><span class="line">mysql&gt; CREATE TABLE filmtb (</span><br><span class="line">    -&gt; name varchar(50),</span><br><span class="line">    -&gt; time varchar(30),</span><br><span class="line">    -&gt; actor varchar(100),</span><br><span class="line">    -&gt; score varchar(10)</span><br><span class="line">    -&gt; );</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="Use-Third-Party-Pymysql"><a href="#Use-Third-Party-Pymysql" class="headerlink" title="Use Third-Party Pymysql"></a>Use Third-Party Pymysql</h4><p>网络爬虫相关的 MySQL 操作：</p>
<p><strong>[1] &gt;&gt;&gt; 连接数据库</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db = pymysql.connect(host=<span class="string">&quot;localhost/ip&quot;</span>, user, password, database)</span><br></pre></td></tr></table></figure>

<p>其中，参数 <code>host</code> 用来指定 MySQL 数据库地址，可以是本地服务端地址，也可以是远程数据库 IP 地址；参数 <code>user</code> 用来指定用于连接数据库的用户名；<code>password</code> 参数用于指定连接数据库的用户密码；<code>database</code> 用来指定想要连接的数据库名称。</p>
<p><strong>[2] &gt;&gt;&gt; 构建游标对象</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cursorObject = db.cursor()</span><br></pre></td></tr></table></figure>

<p><strong>[3] &gt;&gt;&gt; 执行 sql 语句</strong></p>
<p>cursor 对象提供的 <code>execute()</code> 语句用于执行 sql 语句，以实现数据库表的增、删、改、查等操作。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一：通过占位符传入数据</span></span><br><span class="line">sql = <span class="string">&quot;insert into filmtb values(&#x27;%s&#x27;, &#x27;%s&#x27;, &#x27;%s&#x27;, &#x27;%s&#x27;)&quot;</span> % (<span class="string">&#x27;我不是药神&#x27;</span>, <span class="string">&#x27;2018-07-05&#x27;</span>, <span class="string">&#x27;徐峥,周一围,王传君&#x27;</span>, <span class="string">&#x27;9.6&#x27;</span>)</span><br><span class="line">cursorObject.execute(sql)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二：通过列表传参方式</span></span><br><span class="line">sql = <span class="string">&quot;insert into filmtb values(%s, %s, %s, %s)&quot;</span></span><br><span class="line">cursorObject.execute(sql, [<span class="string">&#x27;我不是药神&#x27;</span>, <span class="string">&#x27;2018-07-05&#x27;</span>, <span class="string">&#x27;徐峥,周一围,王传君&#x27;</span>, <span class="string">&#x27;9.6&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p><strong>[4] &gt;&gt;&gt; 提交数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.commit()</span><br></pre></td></tr></table></figure>

<p><strong>[5] &gt;&gt;&gt; 资源释放</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭游标</span></span><br><span class="line">cursorObject.close()</span><br><span class="line"><span class="comment"># 断开连接</span></span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="存储实例"><a href="#存储实例" class="headerlink" title="存储实例"></a>存储实例</h4><p>向数据库表中插入一条数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建数据库连接：</span></span><br><span class="line">db = pymysql.connect(host=<span class="string">&quot;localhost&quot;</span>, user=<span class="string">&quot;root&quot;</span>, password=<span class="string">&quot;admin&quot;</span>, database=<span class="string">&quot;maoyandb&quot;</span>)</span><br><span class="line"><span class="comment"># 构建游标 Cursor 对象：</span></span><br><span class="line">cursorObj = db.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 SQL 语句，进行单行数据插入：</span></span><br><span class="line">data_list = [<span class="string">&#x27;我不是药神&#x27;</span>, <span class="string">&#x27;2018-07-05&#x27;</span>, <span class="string">&#x27;徐峥,周一围,王传君&#x27;</span>, <span class="string">&#x27;9.6&#x27;</span>]</span><br><span class="line">sql = <span class="string">&quot;insert into filmtb values(%s, %s, %s, %s)&quot;</span></span><br><span class="line">cursorObj.execute(sql, data_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交数据至数据库：</span></span><br><span class="line">db.commit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 释放资源：</span></span><br><span class="line">cursorObj.close()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure>

<p>DOS 下连接数据库查询数据结果，如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> SELECT * FROM FILMTB;</span></span><br><span class="line">+-----------------+------------+----------------------------+-------+</span><br><span class="line">| name            | time       | actor                      | score |</span><br><span class="line">+-----------------+------------+----------------------------+-------+</span><br><span class="line">| 我不是药神       | 2018-07-05 | 徐峥,周一围,王传君          | 9.6   |</span><br><span class="line">+-----------------+------------+----------------------------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p><strong>多条数据同时插入 &gt;&gt;&gt;</strong></p>
<p>cursor 对象还提供了一种更效率的插入方法 <code>executemany()</code>，支持同时向表中插入多条数据：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建数据库连接：</span></span><br><span class="line">db = pymysql.connect(host=<span class="string">&quot;localhost&quot;</span>, user=<span class="string">&quot;root&quot;</span>, password=<span class="string">&quot;admin&quot;</span>, database=<span class="string">&quot;maoyandb&quot;</span>)</span><br><span class="line"><span class="comment"># 构建游标 Cursor 对象：</span></span><br><span class="line">cursorObj = db.cursor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 SQL 语句，同时插入多行数据：</span></span><br><span class="line">dataTuple_list = [(<span class="string">&quot;肖申克的救赎&quot;</span>, <span class="string">&quot;1994-09-10&quot;</span>, <span class="string">&quot;蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿&quot;</span>, <span class="string">&quot;9.5&quot;</span>), (<span class="string">&quot;海上钢琴师&quot;</span>, <span class="string">&quot;2019-11-15&quot;</span>, <span class="string">&quot;蒂姆·罗斯,比尔·努恩 ,克兰伦斯·威廉姆斯三世&quot;</span>, <span class="string">&quot;9.3&quot;</span>)]</span><br><span class="line">sql = <span class="string">&quot;insert into filmtb values(%s, %s, %s, %s)&quot;</span></span><br><span class="line">cursorObj.executemany(sql, dataTuple_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提交数据至数据库：</span></span><br><span class="line">db.commit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 释放资源：</span></span><br><span class="line">cursorObj.close()</span><br><span class="line">db.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>DOS 下连接数据库查询数据结果，如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> SELECT * FROM FILMTB;</span></span><br><span class="line">+--------------------+------------+-----------------------------------------------------------------+-------+</span><br><span class="line">| name               | time       | actor                                                           | score |</span><br><span class="line">+--------------------+------------+-----------------------------------------------------------------+-------+</span><br><span class="line">| 我不是药神         | 2018-07-05 | 徐峥,周一围,王传君                                              | 9.6   |</span><br><span class="line">| 肖申克的救赎       | 1994-09-10 | 蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿                               | 9.5   |</span><br><span class="line">| 海上钢琴师         | 2019-11-15 | 蒂姆·罗斯,比尔·努恩 ,克兰伦斯·威廉姆斯三世                      | 9.3   |</span><br><span class="line">+--------------------+------------+-----------------------------------------------------------------+-------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="实例：抓取猫影电影排行榜-1"><a href="#实例：抓取猫影电影排行榜-1" class="headerlink" title="实例：抓取猫影电影排行榜"></a>实例：抓取猫影电影排行榜</h3><p>预分析过程同【4.2】小节，这里我们使用数据库的存储方法重写【4.2】小节中的网络爬虫脚本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> fake_useragent</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># import csv</span></span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义爬虫类：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaoyanSpider</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># Base URL：</span></span><br><span class="line">        self.url = <span class="string">&quot;https://www.maoyan.com/board/4?&#123;&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构建数据库连接对象：</span></span><br><span class="line">        self.db = pymysql.connect(host=<span class="string">&quot;localhost&quot;</span>, user=<span class="string">&quot;root&quot;</span>, password=<span class="string">&quot;admin&quot;</span>, database=<span class="string">&quot;maoyandb&quot;</span>, charset=<span class="string">&quot;utf8&quot;</span>)</span><br><span class="line">        <span class="comment"># 构建游标对象：</span></span><br><span class="line">        self.cursorObject = self.db.cursor()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计数器</span></span><br><span class="line">        self.counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1. 请求获取网页 HTML 信息（GET）：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">requestHTML</span>(<span class="params">self, url</span>):</span></span><br><span class="line">        <span class="comment"># 随机获取浏览器伪装 UA：</span></span><br><span class="line">        ua = fake_useragent.UserAgent()</span><br><span class="line">        <span class="comment"># ua_info = ua.random</span></span><br><span class="line">        ua_info = ua.edge</span><br><span class="line">        <span class="comment"># print(&quot;User-Agent: &quot; + ua_info)</span></span><br><span class="line"></span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">&quot;User-Agent&quot;</span>: ua_info,</span><br><span class="line">            <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;*/*&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Cookie&quot;</span>: <span class="string">&quot;__mta=146102795.1672499681444.1673112786612.1673166343246.9; uuid_n_v=v1; uuid=D815AEF0891D11ED8B7C3F15132DB86FB3A3D5406DE64D05805D9B914B5EC73D; _csrf=c0b537815f7f65bdd82c6a2df76280c3a4cc2ba75dbac1113793e4c716926538; _lxsdk_cuid=18568bfd59ec8-063f8bb1638243-7a575473-144000-18568bfd59ec8; Hm_lvt_703e94591e87be68cc8da0da7cbd0be2=1672499681; _lxsdk=D815AEF0891D11ED8B7C3F15132DB86FB3A3D5406DE64D05805D9B914B5EC73D; __mta=146102795.1672499681444.1673111960576.1673112438542.42; Hm_lpvt_703e94591e87be68cc8da0da7cbd0be2=1673166343; _lxsdk_s=185907c46cf-0b7-a45-7b0%7C%7C4&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># 重构请求对象：</span></span><br><span class="line">        req = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">        <span class="comment"># 发送请求</span></span><br><span class="line">        response = urllib.request.urlopen(req)</span><br><span class="line">        <span class="comment"># print(&quot;Request Status Code:&quot;, response.status)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 响应的网页信息：</span></span><br><span class="line">        html = response.read().decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">        <span class="comment"># html = response.read().decode(&quot;gbk&quot;, &quot;ignore&quot;)</span></span><br><span class="line">        <span class="keyword">return</span> html</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 页面解析：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parseHTML</span>(<span class="params">self, pattern, html</span>):</span></span><br><span class="line">        <span class="comment"># 基于 Pattern 构建正则表达式对象（非贪婪模式 &amp;&amp; flags=re.S）：</span></span><br><span class="line">        regExp_Obj = re.<span class="built_in">compile</span>(pattern=pattern, flags=re.S)</span><br><span class="line">        <span class="comment"># 查找匹配内容：</span></span><br><span class="line">        find_res = regExp_Obj.findall(html)</span><br><span class="line">        <span class="comment"># 返回一个元组元素列表（每个元组元素都是一次成功的匹配，可以包含多个分组内容）：</span></span><br><span class="line">        <span class="keyword">return</span> find_res</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 数据持久化：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dataSave</span>(<span class="params">self, validData</span>):</span></span><br><span class="line">        <span class="comment"># 定义数据仓库</span></span><br><span class="line">        dataRepo = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数据处理：</span></span><br><span class="line">        <span class="keyword">if</span> validData:</span><br><span class="line">            <span class="keyword">for</span> record <span class="keyword">in</span> validData:</span><br><span class="line">                video_name = record[<span class="number">0</span>].strip()</span><br><span class="line">                video_actor = record[<span class="number">1</span>].strip()[<span class="number">3</span>:]</span><br><span class="line">                video_time = record[<span class="number">2</span>].strip()[<span class="number">5</span>:<span class="number">15</span>]</span><br><span class="line">                video_score = record[<span class="number">3</span>] + record[<span class="number">4</span>]</span><br><span class="line"></span><br><span class="line">                dataItem = (video_name, video_time, video_actor, video_score)</span><br><span class="line">                dataRepo.append(dataItem)</span><br><span class="line">                <span class="comment"># 打印第一页影片信息</span></span><br><span class="line">                <span class="keyword">if</span> (self.counter &lt; <span class="number">10</span>):</span><br><span class="line">                    print(<span class="built_in">str</span>(dataItem))</span><br><span class="line">                    self.counter = self.counter + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 存入数据库：</span></span><br><span class="line">            sql = <span class="string">&quot;insert into filmtb values (%s, %s, %s, %s)&quot;</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                self.cursorObject.executemany(sql, dataRepo)</span><br><span class="line">                self.db.commit()</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                print(<span class="string">&quot;Error: &quot;</span>, e)</span><br><span class="line">                <span class="comment"># 发生错误后回滚数据库</span></span><br><span class="line">                self.db.rollback()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">&quot;Request Failed&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 入口函数：</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self</span>):</span></span><br><span class="line">        pnum_start = <span class="number">1</span></span><br><span class="line">        pnum_end = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历所有待爬取页面：</span></span><br><span class="line">        <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(pnum_start, pnum_end+<span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 构建 URL：</span></span><br><span class="line">            page_num = (page - <span class="number">1</span>) * <span class="number">10</span></span><br><span class="line">            params = &#123;</span><br><span class="line">                <span class="string">&quot;offset&quot;</span>: page_num</span><br><span class="line">            &#125;</span><br><span class="line">            url = self.url.<span class="built_in">format</span>(urllib.parse.urlencode(params))</span><br><span class="line">            <span class="comment"># print(&quot;Request URL: &quot;, url)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 请求获取网页 HTML 信息：</span></span><br><span class="line">            html = self.requestHTML(url)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 页面解析以提取有效数：</span></span><br><span class="line">            <span class="comment"># 正则表达式文本模式：</span></span><br><span class="line">            pattern = <span class="string">&#x27;&lt;div class=&quot;board-item-main&quot;&gt;.*?title=&quot;(.*?)&quot;.*?class=&quot;star&quot;&gt;(.*?)&lt;/p&gt;.*?releasetime&quot;&gt;(.*?)&lt;/p&gt;.*?integer&quot;&gt;(.*?)&lt;/i&gt;.*?fraction&quot;&gt;(.*?)&lt;/i&gt;&#x27;</span></span><br><span class="line">            validData_list = self.parseHTML(pattern, html)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 数据持久化存储：</span></span><br><span class="line">            self.dataSave(validData_list)</span><br><span class="line">            print(<span class="string">&quot;Page %d was successfully crawled&quot;</span> % page)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每爬取一个页面随机休眠 1-2 秒：</span></span><br><span class="line">            time.sleep(random.randint(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 释放资源：</span></span><br><span class="line">        self.cursorObject.close()</span><br><span class="line">        self.db.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 爬虫主程序入口</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    print(<span class="string">&quot;| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Start Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |&quot;</span>)</span><br><span class="line">    start = time.time()</span><br><span class="line">    spider = MaoyanSpider()</span><br><span class="line">    spider.run()</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(<span class="string">&quot;Script Runtime:%.2f s&quot;</span> % (end - start))</span><br><span class="line">    print(<span class="string">&quot;| &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Close Spider &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; |&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>数据库查询存储结果，如下所示（篇幅原因，只截取一部分）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> SELECT * FROM FILMTB;</span></span><br><span class="line">+---------------------------------+--------------------+-----------------------------------------------------------------------------------------+-------+</span><br><span class="line">| name                            | time               | actor                                                                                   | score |</span><br><span class="line">+---------------------------------+--------------------+-----------------------------------------------------------------------------------------+-------+</span><br><span class="line">| 我不是药神                      | 2018-07-05         | 徐峥,周一围,王传君                                                                      | 9.6   |</span><br><span class="line">| 肖申克的救赎                    | 1994-09-10         | 蒂姆·罗宾斯,摩根·弗里曼,鲍勃·冈顿                                                       | 9.5   |</span><br><span class="line">| 海上钢琴师                      | 2019-11-15         | 蒂姆·罗斯,比尔·努恩 ,克兰伦斯·威廉姆斯三世                                              | 9.3   |</span><br><span class="line">| 绿皮书                          | 2019-03-01         | 维果·莫腾森,马赫沙拉·阿里,琳达·卡德里尼                                                 | 9.5   |</span><br><span class="line">| 霸王别姬                        | 1993-07-26         | 张国荣,张丰毅,巩俐                                                                      | 9.4   |</span><br><span class="line">| 美丽人生                        | 2020-01-03         | 罗伯托·贝尼尼,朱斯蒂诺·杜拉诺,赛尔乔·比尼·布斯特里克                                    | 9.3   |</span><br><span class="line">| 小偷家族                        | 2018-08-03         | 中川雅也,安藤樱,松冈茉优                                                                | 8.1   |</span><br><span class="line">| 这个杀手不太冷                  | 1994-09-14         | 让·雷诺,加里·奥德曼,娜塔莉·波特曼                                                       | 9.4   |</span><br><span class="line">....</span><br><span class="line">....</span><br><span class="line">....</span><br><span class="line">| 波西米亚狂想曲                  | 2019-03-22         | 拉米·马雷克,本·哈迪,约瑟夫•梅泽罗                                                       | 9.4   |</span><br><span class="line">| 真爱至上                        | 2003-11-21         | 休·格兰特,比尔·奈伊,连姆·尼森                                                           | 8.6   |</span><br><span class="line">| 大鱼                            | 2003-12-04         | 伊万·麦克格雷格,阿尔伯特·芬尼,杰西卡·兰格                                               | 8.6   |</span><br><span class="line">| 模仿游戏                        | 2015-07-21         | 本尼迪克特·康伯巴奇,凯拉·奈特莉,马修·古迪                                               | 9.3   |</span><br><span class="line">| 血战钢锯岭                      | 2016-12-08         | 安德鲁·加菲尔德,雨果·维文,卢克·布雷西                                                   | 9.3   |</span><br><span class="line">| 傲慢与偏见                      | 2008-02-10         | 马修·麦克费登,吉娜·马隆,妲露拉·莱莉                                                     | 8.4   |</span><br><span class="line">| 致命魔术                        | 2006-10-17         | 休·杰克曼,克里斯蒂安·贝尔,迈克尔·凯恩                                                   | 8.8   |</span><br><span class="line">| 奇迹男孩                        | 2018-01-19         | 雅各布·特瑞布雷,朱莉娅·罗伯茨,欧文·威尔逊                                               | 9.2   |</span><br><span class="line">| 禁闭岛                          | 2010-02-13         | 莱昂纳多·迪卡普里奥,马克·鲁法洛,本·金斯利                                               | 8.7   |</span><br><span class="line">| 鬼子来了                        | 2000-05-13         | 姜文,姜宏波,陈强                                                                        | 8.9   |</span><br><span class="line">+---------------------------------+--------------------+-----------------------------------------------------------------------------------------+-------+</span><br><span class="line">100 rows in set (0.01 sec)</span><br></pre></td></tr></table></figure>

<p>可见，爬虫脚本已将猫眼电影 TOP100 排行榜中的所有影片信息全部抓取存储到了数据库表中。</p>
<hr>
<h2 id="网络爬虫常见问题"><a href="#网络爬虫常见问题" class="headerlink" title="网络爬虫常见问题"></a>网络爬虫常见问题</h2><p>这一小节来看网络爬虫常见问题以及其解决方法：</p>
<h3 id="百度安全验证问题"><a href="#百度安全验证问题" class="headerlink" title="百度安全验证问题"></a>百度安全验证问题</h3><p>爬虫抓取百度搜索页面信息时，你可能会发现抓取到的内容不是网页信息，响应内容显示：<strong>百度安全验证 &amp; 网络不给力，请稍后重试 &amp; 返回首页 &amp; 问题反馈</strong>。如下显示：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="meta-keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;zh-CN&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;utf-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>百度安全验证<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;Content-Type&quot;</span> <span class="attr">content</span>=<span class="string">&quot;text/html; charset=utf-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;apple-mobile-web-app-capable&quot;</span> <span class="attr">content</span>=<span class="string">&quot;yes&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;apple-mobile-web-app-status-bar-style&quot;</span> <span class="attr">content</span>=<span class="string">&quot;black&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;viewport&quot;</span> <span class="attr">content</span>=<span class="string">&quot;width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">name</span>=<span class="string">&quot;format-detection&quot;</span> <span class="attr">content</span>=<span class="string">&quot;telephone=no, email=no&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;shortcut icon&quot;</span> <span class="attr">href</span>=<span class="string">&quot;https://www.baidu.com/favicon.ico&quot;</span> <span class="attr">type</span>=<span class="string">&quot;image/x-icon&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;icon&quot;</span> <span class="attr">sizes</span>=<span class="string">&quot;any&quot;</span> <span class="attr">mask</span> <span class="attr">href</span>=<span class="string">&quot;https://www.baidu.com/img/baidu.svg&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;X-UA-Compatible&quot;</span> <span class="attr">content</span>=<span class="string">&quot;IE=Edge&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">http-equiv</span>=<span class="string">&quot;Content-Security-Policy&quot;</span> <span class="attr">content</span>=<span class="string">&quot;upgrade-insecure-requests&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">&quot;stylesheet&quot;</span> <span class="attr">href</span>=<span class="string">&quot;https://ppui-static-wap.cdn.bcebos.com/static/touch/css/api/mkdjump_aac6df1.css&quot;</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;timeout hide&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;timeout-img&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;timeout-title&quot;</span>&gt;</span>网络不给力，请稍后重试<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">button</span> <span class="attr">type</span>=<span class="string">&quot;button&quot;</span> <span class="attr">class</span>=<span class="string">&quot;timeout-button&quot;</span>&gt;</span>返回首页<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;timeout-feedback hide&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;timeout-feedback-icon&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">p</span> <span class="attr">class</span>=<span class="string">&quot;timeout-feedback-title&quot;</span>&gt;</span>问题反馈<span class="tag">&lt;/<span class="name">p</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://wappass.baidu.com/static/machine/js/api/mkd.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">&quot;https://ppui-static-wap.cdn.bcebos.com/static/touch/js/mkdjump_db105ab.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>通过查阅资料了解到，出现此问题可能是请求头（Request Headers）定义不完善被百度反爬（大多数是因为请求头缺少 <code>Accept</code>），还有可能是因为未登录时获取无效 Cookie 来定义请求头等。</p>
<p><font color="red">解决思路 &gt;&gt;&gt;</font> 需要进一步重构请求头信息，以实现更真实的浏览器请求伪装。</p>
<p><strong>解决办法 &gt;&gt;&gt;&gt;</strong></p>
<p>收集浏览器中百度搜索页面的请求头（Request Headers）信息，如下图：</p>
<p><img src="https://s2.loli.net/2022/12/31/wcJx5Aus8bk3QHr.png"></p>
<p>你可以将 <code>Accept</code> &amp;&amp; <code>Cookie</code> 的键值对 Copy 下来，用于重构爬虫的请求头：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">header&#123;</span><br><span class="line">    <span class="string">&quot;Cookie&quot;</span>: <span class="string">&#x27;填写你浏览器的 Cookie 值&#x27;</span>,</span><br><span class="line">    <span class="string">&quot;Accept&quot;</span>: <span class="string">&quot;text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9&quot;</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="HTTP-Error-302"><a href="#HTTP-Error-302" class="headerlink" title="HTTP Error 302"></a>HTTP Error 302</h3><p>有时爬虫在抓取某些网站时，可能出现如下错误：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HTTPError: HTTP Error <span class="number">302</span>: The HTTP server returned a redirect error that would lead to an infinite loop.</span><br><span class="line">The last 30x error message was: .....</span><br></pre></td></tr></table></figure>

<p>查询资料，发现是请求资源发生重定向导致无限循环的错误，这也是网站反爬机制的一种。</p>
<p><strong>解决方法一 &gt;&gt;&gt;</strong></p>
<p>一种原因可能是请求时没有 <code>Cookie</code>，被网站反爬。你可以通过重构请求头，为请求添加 <code>Cookie</code>。</p>
<p><strong>解决方法二 &gt;&gt;&gt;</strong></p>
<p>看到有一篇博文说，可以使用 <code>Requests</code> 库来发送请求，可以作为一种尝试方法。</p>
<hr>
</div><div class="article-licensing box"><div class="licensing-title"><p>Web Crawler 教程之网络爬虫通用流程解读</p><p><a href="https://www.orangeshare.cn/2020/01/21/web-crawler-jiao-cheng-zhi-wang-luo-pa-chong-tong-yong-liu-cheng-jie-du/">https://www.orangeshare.cn/2020/01/21/web-crawler-jiao-cheng-zhi-wang-luo-pa-chong-tong-yong-liu-cheng-jie-du/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Waldeinsamkeit</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2020-01-21</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2023-01-12</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Spider/">Spider</a><a class="link-muted mr-2" rel="tag" href="/tags/Web-Crawler/">Web Crawler</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/01/22/web-crawler-jiao-cheng-zhi-wang-luo-pa-chong-gong-ju-ku/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Web Crawler 教程之网络爬虫工具库</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/01/20/web-crawler-jiao-cheng-zhi-chu-shi-wang-luo-pa-chong/"><span class="level-item">Web Crawler 教程之初识网络爬虫</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen  order-3 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Waldeinsamkeit"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Waldeinsamkeit</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">113</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">13</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">43</p></a></div></div></nav></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#请求获取网页-HTML-信息"><span class="level-left"><span class="level-item">1</span><span class="level-item">请求获取网页 HTML 信息</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Use-Built-in-UrlLib-Lib"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">Use Built-in UrlLib Lib</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#模拟发送请求"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item">模拟发送请求</span></span></a></li><li><a class="level is-mobile" href="#获取-HTML-信息"><span class="level-left"><span class="level-item">1.1.2</span><span class="level-item">获取 HTML 信息</span></span></a></li><li><a class="level is-mobile" href="#URL-的编码和解码"><span class="level-left"><span class="level-item">1.1.3</span><span class="level-item">URL 的编码和解码</span></span></a></li><li><a class="level is-mobile" href="#深入解读-UrlLib-库"><span class="level-left"><span class="level-item">1.1.4</span><span class="level-item">深入解读 UrlLib 库</span></span></a></li></ul></li><li><a class="level is-mobile" href="#User-Agent"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">User-Agent</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#爬虫程序-UA-信息"><span class="level-left"><span class="level-item">1.2.1</span><span class="level-item">爬虫程序 UA 信息</span></span></a></li><li><a class="level is-mobile" href="#重构爬虫-UA-信息"><span class="level-left"><span class="level-item">1.2.2</span><span class="level-item">重构爬虫 UA 信息</span></span></a></li><li><a class="level is-mobile" href="#构建-UA-代理池"><span class="level-left"><span class="level-item">1.2.3</span><span class="level-item">构建 UA 代理池</span></span></a></li></ul></li><li><a class="level is-mobile" href="#实例：爬虫抓取网页信息"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">实例：爬虫抓取网页信息</span></span></a></li></ul></li><li><a class="level is-mobile" href="#审查网页结构以及元素节点信息"><span class="level-left"><span class="level-item">2</span><span class="level-item">审查网页结构以及元素节点信息</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#网页的构成"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">网页的构成</span></span></a></li><li><a class="level is-mobile" href="#动态网页-VS-静态网页"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">动态网页 VS 静态网页</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#静态网页"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">静态网页</span></span></a></li><li><a class="level is-mobile" href="#动态网页"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">动态网页</span></span></a></li></ul></li><li><a class="level is-mobile" href="#审查页面元素"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">审查页面元素</span></span></a></li><li><a class="level is-mobile" href="#检查网页结构"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">检查网页结构</span></span></a></li><li><a class="level is-mobile" href="#实例：爬虫抓取百度贴吧数据"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">实例：爬虫抓取百度贴吧数据</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#预分析"><span class="level-left"><span class="level-item">2.5.1</span><span class="level-item">预分析</span></span></a></li><li><a class="level is-mobile" href="#网络爬虫编写"><span class="level-left"><span class="level-item">2.5.2</span><span class="level-item">网络爬虫编写</span></span></a></li><li><a class="level is-mobile" href="#爬虫随机休眠"><span class="level-left"><span class="level-item">2.5.3</span><span class="level-item">爬虫随机休眠</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#页面内容结构解析以提取有效信息"><span class="level-left"><span class="level-item">3</span><span class="level-item">页面内容结构解析以提取有效信息</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Use-Built-in-Re-Lib"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Use Built-in Re Lib</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Re-模块常用爬虫方法"><span class="level-left"><span class="level-item">3.1.1</span><span class="level-item">Re 模块常用爬虫方法</span></span></a></li><li><a class="level is-mobile" href="#正则表达式对象方法"><span class="level-left"><span class="level-item">3.1.2</span><span class="level-item">正则表达式对象方法</span></span></a></li><li><a class="level is-mobile" href="#适合-HTML-文档的正则规则"><span class="level-left"><span class="level-item">3.1.3</span><span class="level-item">适合 HTML 文档的正则规则</span></span></a></li><li><a class="level is-mobile" href="#深入解读-Re-库"><span class="level-left"><span class="level-item">3.1.4</span><span class="level-item">深入解读 Re 库</span></span></a></li></ul></li><li><a class="level is-mobile" href="#网页有效信息提取"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">网页有效信息提取</span></span></a></li></ul></li><li><a class="level is-mobile" href="#持久化存储页面有效信息"><span class="level-left"><span class="level-item">4</span><span class="level-item">持久化存储页面有效信息</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#本地文件存储"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">本地文件存储</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#写入-CSV-文件"><span class="level-left"><span class="level-item">4.1.1</span><span class="level-item">写入 CSV 文件</span></span></a></li><li><a class="level is-mobile" href="#读取-CSV-文件"><span class="level-left"><span class="level-item">4.1.2</span><span class="level-item">读取 CSV 文件</span></span></a></li></ul></li><li><a class="level is-mobile" href="#实例：抓取猫影电影排行榜"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">实例：抓取猫影电影排行榜</span></span></a></li><li><a class="level is-mobile" href="#数据库存储"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">数据库存储</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#库以及存储数据表准备"><span class="level-left"><span class="level-item">4.3.1</span><span class="level-item">库以及存储数据表准备</span></span></a></li><li><a class="level is-mobile" href="#Use-Third-Party-Pymysql"><span class="level-left"><span class="level-item">4.3.2</span><span class="level-item">Use Third-Party Pymysql</span></span></a></li><li><a class="level is-mobile" href="#存储实例"><span class="level-left"><span class="level-item">4.3.3</span><span class="level-item">存储实例</span></span></a></li></ul></li><li><a class="level is-mobile" href="#实例：抓取猫影电影排行榜-1"><span class="level-left"><span class="level-item">4.4</span><span class="level-item">实例：抓取猫影电影排行榜</span></span></a></li></ul></li><li><a class="level is-mobile" href="#网络爬虫常见问题"><span class="level-left"><span class="level-item">5</span><span class="level-item">网络爬虫常见问题</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#百度安全验证问题"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">百度安全验证问题</span></span></a></li><li><a class="level is-mobile" href="#HTTP-Error-302"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">HTTP Error 302</span></span></a></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="When Art Meets Tech" height="28"></a><p class="is-size-7"><span>&copy; 2024 Waldeinsamkeit</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>